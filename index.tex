% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod,
  oneside]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
  \setmainfont[]{ETbb}
  \setsansfont[]{Source Sans Pro}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[left=1in,marginparwidth=2.0666666666667in,textwidth=4.1333333333333in,marginparsep=0.3in]{geometry}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslentryspacingunit} % times entry-spacing
\setlength{\cslentryspacingunit}{\parskip}
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
  \let\oldpar\par
  \def\par{\hangindent=\cslhangindent\oldpar}
  \fi
  % set entry spacing
  \setlength{\parskip}{#2\cslentryspacingunit}
 }%
 {}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}\break}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\input{resources/tex/boxes.tex}

% page layout
\geometry{
  dvips=false, pdftex=false, vtex=false, % drivers can have unexpected behaviors
  papersize={8in,10in},                  % size specified by MIT Press
  centering,                             % split margins equally
  margin=.6in,                           % margins (must all be at least .5in)
  includemp, includehead,                % include sidenotes & header in body
  showframe                              % show page structure (for debugging)
}

% set font specifications
\setkomafont{disposition}{\rmfamily\itshape}
\addtokomafont{part}{\sffamily\scshape}
\addtokomafont{partnumber}{\sffamily\scshape}
\addtokomafont{chapter}{\sffamily\scshape}
\setkomafont{partentry}{\sffamily\scshape}
\setkomafont{chapterentry}{\sffamily\scshape}
\addtokomafont{title}{\sffamily\scshape}
\addtokomafont{subtitle}{\sffamily\scshape}
\addtokomafont{author}{\sffamily\scshape}
\addtokomafont{pagehead}{\sffamily\scshape}
\addtokomafont{pagenumber}{\sffamily\scshape}

% adjust spacing around section headers
\RedeclareSectionCommand[
  runin=false,
  afterskip=0pt % remove extra space after for section
]{section}
\RedeclareSectionCommand[
  runin=false,
  afterskip=0pt % remove extra space after for subsection
]{subsection}

% headers/footers
\usepackage{scrlayer-scrpage}
\KOMAoptions{headwidth=textwithmarginpar} % make header full width
\automark{chapter}
\clearpairofpagestyles
\renewcommand{\chaptermark}[1]{\markboth{#1}{}} % prevent chaptermark from uppercasing
\ihead{%
  \ifnum\value{chapter}>0 \thechapter\hspace{3pt} \fi % include chapter number if not 0
  \textsc{\leftmark} % then chapter name
}
\ohead{\pagemark}
\pagestyle{scrheadings}

% table of contents
\usepackage[titles]{tocloft}
\renewcommand{\cftpartfont}{\sffamily\scshape\Large}     % part title
\renewcommand{\cftpartpagefont}{\sffamily\scshape\Large} % part page number
\setlength{\cftbeforepartskip}{1em}                      % part vspace before
\renewcommand{\cftchapfont}{\sffamily\scshape\Large}     % chapter title
\renewcommand{\cftchappagefont}{\sffamily\scshape\Large} % chapter page number
\setlength{\cftbeforechapskip}{.5em}                     % chapter vspace before

% set chapter numbers flushright
\newcommand{\numlen}{.5em}
\renewcommand{\cftchappresnum}{\hfill}
\renewcommand{\cftchapaftersnum}{\hspace*{\numlen}}
\addtolength{\cftchapnumwidth}{\numlen}

% lists
\usepackage{enumitem}
\setlist[itemize]{
  label={--} % en-dash as bullet symbol
}

\usepackage{threeparttable} % for papaja apa tables
\setlength{\tabcolsep}{4pt} % horizontal space between table columns

% styling for captions
\usepackage[format=plain]{caption}
\usepackage{marginfix} % load before sidenotes to improve sidenote positioning
\usepackage{sidenotes}
\usepackage{marginnote}
% \setlength{\belowcaptionskip}{0pt} 
\definecolor{captioncolor}{cmyk}{.20,.10,0,.56} % grey
\DeclareCaptionFont{caps}{\footnotesize\color{captioncolor}} % smaller and grey
\captionsetup{labelfont=caps,textfont=caps} %,belowskip=-12pt}
\DeclareCaptionStyle{sidecaption}{labelfont=caps,textfont=caps,skip=6pt}
\DeclareCaptionStyle{marginfigure}{labelfont=caps,textfont=caps,skip=6pt}
\DeclareCaptionStyle{margintable}{labelfont=caps,textfont=caps,skip=6pt}
\DeclareCaptionStyle{longtable}{labelfont=caps,textfont=caps,skip=6pt}

% reset sidenote counter at start of each chapter
\let\oldchapter\chapter
\def\chapter{%
  \setcounter{sidenote}{1}%
  \oldchapter
}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{sidenotes}{}{\usepackage{sidenotes}}
\@ifpackageloaded{marginnote}{}{\usepackage{marginnote}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Experimentology},
  pdfauthor={Michael C. Frank; Mika Braginsky; Julie Cachia; Nicholas Coles; Tom E. Hardwicke; Robert Hawkins; Maya Mathur; Rondeline Williams; Adapted for undergraduates by Russ Poldrack},
  colorlinks=true,
  linkcolor={DarkBlue},
  filecolor={Maroon},
  citecolor={DarkGreen},
  urlcolor={DarkGreen},
  pdfcreator={LaTeX via pandoc}}

\title{Experimentology}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{An Open Science Approach to Experimental Psychology Methods
for Undergraduates}
\author{Michael C. Frank \and Mika Braginsky \and Julie
Cachia \and Nicholas Coles \and Tom E. Hardwicke \and Robert
Hawkins \and Maya Mathur \and Rondeline Williams \and Adapted for
undergraduates by Russ Poldrack}
\date{2023-11-25}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, interior hidden, borderline west={3pt}{0pt}{shadecolor}, enhanced, breakable, sharp corners, frame hidden]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{0}
\tableofcontents
}
\listoftables
\hypertarget{preface}{}
\bookmarksetup{startatroot}

\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

As scientists and practitioners, we often want to create generalizable,
causal theories of human behavior. As it turns out, experiments -- in
which we use random assignment to measure a causal effect -- are an
unreasonably effective tool to help with this task. But how should we go
about doing good experiments?

This book provides an introduction to the workflow of the experimental
researcher working in psychology or the behavioral sciences more
broadly. The organization of the book is sequential, from the planning
stages of the research process through design, data gathering, analysis,
and reporting. We introduce these concepts via narrative examples from a
range of sub-disciplines, including cognitive, developmental, and social
psychology. Throughout, we also illustrate the pitfalls that led to the
``replication crisis'' in psychology.

To help researchers avoid these pitfalls, we advocate for an
open-science based approach in which transparency is integral to the
entire experimental workflow. We provide readers with guidance for
preregistration, project management, data sharing, and reproducible
report writing.

\hypertarget{the-story-of-this-book}{%
\section*{The story of this book}\label{the-story-of-this-book}}
\addcontentsline{toc}{section}{The story of this book}

\markright{The story of this book}

Experimental Methods (Psych 251) is the foundational course for incoming
graduate students in the Stanford psychology department. For the last
twelve years, one of us (Frank) has taught this course and most of us
(Cachia, Coles, Hardwicke, Hawkins, Mathur, Williams) have TA'ed, taken,
or otherwise contributed to the course. The course goal is to orient
students to the nuts and bolts of doing behavioral experiments,
including how to plan and design a solid experiment and how to avoid
common pitfalls regarding design, measurement, and sampling.

Almost all student coursework both before and in graduate school deals
with the content of their research, including theories and results in
their areas of focus. In contrast, the Experimental Methods course is
sometimes the only one that deals with the \emph{process} of research,
from big questions about why we do experiments and what it means to make
a causal inference, all the way to the tiny details of project
organization, like what to name your directories and how to make sure
you don't lose your data in a computer crash.

This observation leads to our book's title. ``Experimentology'' is the
set of practices, findings, and approaches that enable the construction
of robust, precise, and generalizable experiments.

The centerpiece of the Experimental Methods course is a replication
project, reflecting a teaching model first described in Frank and Saxe
(\protect\hyperlink{ref-frank2012}{2012}) and later expanded on in
Hawkins et al. (\protect\hyperlink{ref-hawkins2018}{2018}). Each student
chooses a published experiment in the literature and collects new data
on a pre-registered version of the same experimental paradigm, comparing
their result to the original publication. Over the course of the
quarter, we walk through how to set up a replication experiment, how to
pre-register confirmatory analyses, and how to write a reproducible
report on the findings. The project teaches concepts like reliability
and validity, which allow students to analyze choices that the original
experimenters made -- often choices that could have been made
differently in hindsight!

At the end of the course, we reap the harvest of these projects. The
project presentations are a wonderful demonstration of both how much the
students can accomplish in a quarter and also how tricky it can be to
reproduce (redo calculations in the original data) and replicate
(recover similar results in new data) the published literature. Often
our replication success rate for the course hovers just above 50\%, an
outcome that can be disturbing or distressing for students who assume
that the published literature reports the absolute truth.

\begin{marginfigure}

{\centering \includegraphics{images/dog.jpeg}

}

\caption{\label{fig-dog}This book has fun stuff going on in the
margins!}

\end{marginfigure}

This book is an attempt to distill some of the lessons of the course
(and students' course projects) into a textbook. We'll tell the story of
the major shifts in psychology that have come about in the last ten
years, including both the ``replication crisis''
(\protect\hyperlink{ref-osc2015}{Open Science Collaboration 2015} et
seq.) and the positive methodological reforms that have resulted from
it. Using this story as motivation, we will highlight the importance of
transparency during all aspects of the experimental process from
planning to dissemination of materials, data, and code.

\hypertarget{what-this-book-is-and-isnt-about}{%
\section*{What this book is and isn't
about}\label{what-this-book-is-and-isnt-about}}
\addcontentsline{toc}{section}{What this book is and isn't about}

\markright{What this book is and isn't about}

This book is about psychology experiments. These will be typically be
short studies conducted online or in a single visit to a lab, often with
a convenience population. When we say ``experiments'' here we mean
\textbf{randomized experiments} where some aspect of the participants'
experience is \textbf{manipulated} by the experimenter and then some
outcome variable is \textbf{measured}.\sidenote{\footnotesize We use \textbf{bold} to
  indicate the introduction of new technical terms.}

The central thesis of the book is that:

\begin{quote}
Experiments are intended to make maximally unbiased, generalizable, and
precise estimates of specific causal effects.
\end{quote}

\noindent We'll follow the implications of this thesis for a host of
topics, including causal inference, experimental design, measurement,
sampling, preregistration, data analysis, and many others.

Because our focus is on experiments, we won't be talking much about
observational designs, survey methods, or qualitative research; these
are important tools and appropriate for a whole host of questions, but
they aren't our focus here. We also won't go into depth about the many
fascinating methodological and statistical issues brought up by
single-participant case studies, longitudinal research, field studies,
or other methodological variants. Many of the concerns we raise are
still important for these types of studies, but some of our advice won't
transfer to these less common designs.

Even for students who are working on non-experimental research, we
expect that a substantial part of the book content will still be useful,
including chapters on replication (\textbf{?@sec-replication}), ethics
(\textbf{?@sec-ethics}), sampling (\textbf{?@sec-sampling}), project
management (\textbf{?@sec-management}), and all of the material on
reporting (\textbf{?@sec-writing}, \textbf{?@sec-viz},
\textbf{?@sec-meta}).

In our writing, we presuppose that readers have some background in
psychology, at least at an introductory level. In addition, although we
discuss some statistical topics, readers might find these sections more
accessible with an undergraduate statistics course under their belt.
Finally, our examples are written in the R statistical programming
language, and for chapters on statistics and visualization especially
(Chapters \textbf{?@sec-estimation}, \textbf{?@sec-inference},
\textbf{?@sec-models}, \textbf{?@sec-viz}, \textbf{?@sec-meta}), some
familiarity with R will be helpful for understanding the code.

\hypertarget{how-to-use-this-book}{%
\section*{How to use this book}\label{how-to-use-this-book}}
\addcontentsline{toc}{section}{How to use this book}

\markright{How to use this book}

The book is organized into five main sections, mirroring the timeline of
an experiment: 1) Foundations, 2) Statistics, 3) Design, 4) Execution,
and 5) Reporting. We hope that this organization makes it well-suited
for teaching or for use as a reference book.\sidenote{\footnotesize If you are an
  instructor who is planning to adopt the book for a course, you might
  be interested in our resources for instructors, including sample
  course schedules, in \textbf{?@sec-instructors}.}

The book is designed for a course for graduate students or advanced
undergraduates, but the material is also suitable for self-study by
anyone interested in experimental methods, whether in academic
psychology or any other context -- in our out of academia -- in which
behavioral experimentation is relevant. We also hope that some readers
will come to particular chapters of the book because of an interest in
specific topics like measurement (\textbf{?@sec-measurement}) or
sampling (\textbf{?@sec-sampling}) and will be able to use those
chapters as standalone references. And finally, for those interested in
the ``replication crisis'' and subsequent reforms, Chapters
\textbf{?@sec-theories}, \textbf{?@sec-replication},
\textbf{?@sec-prereg}, \textbf{?@sec-management} will be especially
interesting.

Ultimately, we want to give you what you need to plan and execute your
own study! Instead of enumerating different approaches, we try to
provide a single coherent -- and often quite opinionated -- perspective,
using marginal notes and references to give pointers to more advanced
materials or alternative approaches. Throughout, we offer:

\begin{itemize}
\tightlist
\item
  \textbf{Case studies} that illustrate the central concepts of a
  chapter,
\item
  \textbf{Accident reports} describing examples where poor research
  practices led to issues in the literature, and
\item
  \textbf{Depth boxes} providing simulations, linkages to advanced
  techniques, or more nuanced discussion.
\end{itemize}

While case studies are often integral to the chapters, the other boxes
can typically be skipped without issue.

\hypertarget{themes}{%
\section*{Themes}\label{themes}}
\addcontentsline{toc}{section}{Themes}

\markright{Themes}

We highlight four major cross-cutting themes for the book:
\textbf{transparency}, \textbf{precision}, \textbf{bias reduction}, and
\textbf{generalizability}.

\begin{itemize}
\tightlist
\item
  \textbf{Transparency}: For experiments to be reproducible, other
  researchers need to be able to determine exactly what you did. Thus,
  every stage of the research process should be guided by a primary
  concern for transparency. For example, preregistration creates
  transparency into the researcher's evolving expectations and thought
  processes; releasing open materials and analysis scripts creates
  transparency into the details of the procedure.
\item
  \textbf{Precision}: We want researchers to start planning an
  experiment by thinking ``what causal effect do I want to measure'' and
  to make planning, sampling, design, and analytic choices that maximize
  the precision of this measurement. A downstream consequence of this
  mindset is that we move away from a focus on dichotomized inferences
  about statistical significance and towards analytic and meta-analytic
  models that focus on continuous effect sizes and confidence intervals
  (\protect\hyperlink{ref-cumming2014}{Cumming 2014}).
\item
  \textbf{Bias reduction}: While precision refers to random error in a
  measurement, measurements also have systematic sources of error that
  bias them away from the true quantity. In our samples, analyses,
  experimental designs, and in the literature, we need to think
  carefully about sources of bias in the quantity being estimated.
\item
  \textbf{Generalizability}: Complex behaviors are rarely universal
  across all settings and populations, and any given experiment can only
  hope to cover a small slice of the possible conditions where a
  behavior of interest takes place
  (\protect\hyperlink{ref-yarkoni2020}{Yarkoni 2020}). Psychologists
  must therefore consider the generalizability of their findings at
  every stage of the process, from stimulus selection and sampling
  procedures, to analytic methods and reporting.
\end{itemize}

Throughout the book, we will return to the important relationships
between these four concepts, and how the decisions made by the
experimenter at every stage of design, data gathering, and analysis bear
on the inferences that can be made about the results.

\hypertarget{the-software-toolkit-for-this-book}{%
\section*{The software toolkit for this
book}\label{the-software-toolkit-for-this-book}}
\addcontentsline{toc}{section}{The software toolkit for this book}

\markright{The software toolkit for this book}

We introduce and advocate for an approach to reproducible study
planning, analysis, and writing. This approach depends on an ecosystem
of open-source software tools, which we introduce in the book's
appendices.\sidenote{\footnotesize These appendices are available online at
  \url{https://experimentology.io} but not in the print version of the
  book, since their content is best viewed in the web format.}

\begin{itemize}
\tightlist
\item
  The R statistical programming language and the
  \href{https://posit.co/download/rstudio-desktop/}{RStudio} integrated
  development environment,
\item
  Version control using \texttt{git} and
  \href{https://github.com/}{GitHub}, allowing collaboration on text
  documents like code, prose, and data, storing and integrating
  contributions over time (\textbf{?@sec-git}),
\item
  The \texttt{RMarkdown} and \texttt{Quarto} tools for creating
  reproducible reports that can be rendered to a variety of formats
  (\textbf{?@sec-rmarkdown}),
\item
  The \texttt{tidyverse} family of R packages, which extend the basic
  functionality of R with simple tools for data wrangling, analysis, and
  visualization (\textbf{?@sec-tidyverse}), and
\item
  The \texttt{ggplot2} plotting package, which makes it easy to create
  flexible data visualizations for both confirmatory and exploratory
  data analyses (\textbf{?@sec-ggplot}).
\end{itemize}

Where appropriate, we provide \textbf{code boxes} that show the specific
\texttt{R} code used to create our examples.

\hypertarget{onward}{%
\section*{Onward!}\label{onward}}
\addcontentsline{toc}{section}{Onward!}

\markright{Onward!}

Thanks for joining us for Experimentology! Whether you are casually
browsing, doing readings for a course, or using the book as a reference
in your own experimental work, we hope you find it useful. Throughout,
we have tried to practice what we preach in terms of reproducibility,
and so the full source code for the book is available at
\url{https://github.com/langcog/experimentology}. We encourage you to
browse, comment, and log issues or suggestions.\sidenote{\footnotesize If you want to
  log a specific issue, please feel free to create an issue on our
  github page at
  \url{https://github.com/langcog/experimentology/issues}.}

\leavevmode\vadjust pre{\hypertarget{foundations}{}}%
\part{Foundations}

\hypertarget{bibliography-999}{%
\section*{References}\label{bibliography-999}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-999}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-cumming2014}{}}%
Cumming, Geoff. 2014. {``The New Statistics: Why and How.''}
\emph{Psychol. Sci.} 25 (1): 7--29.

\leavevmode\vadjust pre{\hypertarget{ref-frank2012}{}}%
Frank, Michael C, and Rebecca Saxe. 2012. {``Teaching Replication.''}
\emph{Perspectives on Psychological Science} 7: 595--99.

\leavevmode\vadjust pre{\hypertarget{ref-hawkins2018}{}}%
Hawkins, Robert D, Eric N Smith, Carolyn Au, Juan Miguel Arias, Rhia
Catapano, Eric Hermann, Martin Keil, et al. 2018. {``Improving the
Replicability of Psychological Science Through Pedagogy.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (1):
7--18.

\leavevmode\vadjust pre{\hypertarget{ref-osc2015}{}}%
Open Science Collaboration. 2015. {``Estimating the Reproducibility of
Psychological Science.''} \emph{Science} 349 (6251).

\leavevmode\vadjust pre{\hypertarget{ref-yarkoni2020}{}}%
Yarkoni, Tal. 2020. {``The Generalizability Crisis.''} \emph{Behav.
Brain Sci.} 45: 1--37.

\end{CSLReferences}

\hypertarget{sec-experiments}{%
\chapter{Experiments}\label{sec-experiments}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Define what an experiment is
\item
  Contrast observational and experimental studies using causal graphs
\item
  Understand the role of randomization in experiments
\item
  Consider constraints on the generalizability of experiments
\end{itemize}

\end{tcolorbox}

Welcome to Experimentology! This is a book all about the art of running
experiments in psychology. Throughout, we will be guided by a simple
idea:

\begin{quote}
The purpose of experiments is to estimate the magnitude of causal
effects.\sidenote{\footnotesize Perhaps you're already saying, ``that's not what I
  thought experiments were for! I thought they were for testing
  hypotheses.'' Bear with us and we hope we'll convince you that our
  definition is a bit more general, and that testing a hypothesis is one
  thing you can do with a measurement.}
\end{quote}

Starting from our core idea, we'll provide advice about how to navigate
things like experimental design, measurement, sampling, and more. Our
decisions about each of these will determine how precise our estimate
is, and whether it is subject to bias. But before we get to those
topics, let's start by thinking about \emph{why} we might do an
experiment.

\hypertarget{observational-studies-dont-reveal-causality}{%
\section{Observational studies don't reveal
causality}\label{observational-studies-dont-reveal-causality}}

If you're reading this book, there's probably something about psychology
you want to understand. How is language learned? How is it that we
experience emotions like happiness and sadness? Why do humans sometimes
work together and other times destroy one another? When psychologists
study these centuries-old questions, they often transform them into
questions about \textbf{causality}.\sidenote{\footnotesize Defining causality is one
  of the trickiest and oldest problems in philosophy, and we won't
  attempt to solve it here! But from a psychological perspective, we're
  fond of Lewis (\protect\hyperlink{ref-lewis1973}{1973})'s
  ``counterfactual'' analysis of causality. On this view, we can
  understand the claim that \emph{money causes happiness} by considering
  a scenario where if people \emph{hadn't} been given more money, they
  \emph{wouldn't} have experienced an increase in happiness.}

\hypertarget{describing-causal-relationships}{%
\subsection{Describing causal
relationships}\label{describing-causal-relationships}}

Consider the age-old question: does money make people happy? This
question is -- at its heart -- a question about what interventions on
the world we can make. Can I get more money and make myself happier? Can
I \emph{cause} happiness with money?

How could we test our hypothesized effect of money on happiness?
Intuitively, many people think of running an \textbf{observational
study}. We might survey people about how much money they make and how
happy they are. The result of this study would be a pair of measurements
for each participant: {[}money, happiness{]}.

Now, imagine your observational study found that money and happiness
were related -- statistically \textbf{correlated} with one another:
people with more money tended to be happier. Can we conclude that money
causes happiness? Not necessarily. The presence of a correlation does
not mean that there is a causal relationship!

\begin{marginfigure}

{\centering \includegraphics{images/experiments/money1-drawing.png}

}

\caption{\label{fig-experiments-money1}The hypothesized causal effect of
money on happiness.}

\end{marginfigure}

Let's get a bit more precise about our causal hypothesis. To illustrate
causal relationships, we can use a tool called \textbf{directed acyclic
graphs} (DAGs; Pearl (\protect\hyperlink{ref-pearl1998}{1998})).
Figure~\ref{fig-experiments-money1} shows an example of a DAG for money
and happiness: the arrow represents our idea about the potential causal
link between two variables: money and happiness.\sidenote{\footnotesize In this
  chapter, we're going to use the term ``variables'' without discussing
  why we study some variables and not others. In the next chapter, we'll
  introduce the term ``construct,'' which indicates a psychological
  entity that we want to theorize about.} The direction of the arrow
tells us which way we hypothesize that the causal relationship goes.

The correlation between money and happiness we saw in our observational
study is consistent with the causal model in
Figure~\ref{fig-experiments-money1}; however, it is also consistent with
several alternative causal models, which we will illustrate with DAGs
below.

\hypertarget{the-problems-of-directionality-and-confounding}{%
\subsection{The problems of directionality and
confounding}\label{the-problems-of-directionality-and-confounding}}

\begin{marginfigure}

{\centering \includegraphics{images/experiments/money2-drawing.png}

}

\caption{\label{fig-experiments-money2}Three reasons why money and
happiness can be correlated.}

\end{marginfigure}

Figure~\ref{fig-experiments-money2} uses DAGs to illustrate several
causal models that are consistent with the observed correlation between
money and happiness. DAG \#1 represents our hypothesized relationship --
money causes people to be happy. But DAG \#2 shows an effect in
completely the opposite direction! In this DAG, being happy causes
people to make more money.

Even more puzzling, there could be a correlation, but no causal
relationship between money and happiness in either direction. Instead, a
third variable -- often referred to as a \textbf{confound} -- may be
causing increases in both money and happiness. For example, maybe having
more friends causes people to both be happier and make more money (DAG
\#3). In this scenario, happiness and money would be correlated even
though one does not cause the other.

A confound (or several) may entirely explain the relationship between
two variables (as in DAG \#3); but it can also just \emph{partly}
explain the relationship. For example, it could be that money does
increase happiness, but the causal effect is rather small, and only
accounts for a small portion of the observed correlation between them,
with the friendship confound (and perhaps others) accounting for the
remainder.

In this case, because of the confounds, we say that the observed
correlation between money and happiness is a \textbf{biased} estimate of
the causal effect of money on happiness. The amount of bias introduced
by the confounds can vary in different scenarios -- it may only be
small, or it may be so strong that we conclude there's a causal
relationship between two variables when there isn't one at all.

The state of affairs summarized in Figure~\ref{fig-experiments-money2}
is why we say ``correlation doesn't imply causation.'' A correlation
between two variables \emph{is consistent with} a causal relationship
between them, but it's also consistent with other relationships as
well.\sidenote{\footnotesize People sometimes ask whether \emph{causation implies
  correlation} (the opposite direction). The short answer is ``also
  no.'' A causal relationship between two variables often means that
  they will be correlated in the data, but not always. For example,
  imagine you measured the speed of a car and the pressure on the gas
  pedal / accelerator. In general, pressure and speed will be
  correlated, consistent with the causal relationship between the two.
  But now imagine you only measured these two variables when someone was
  driving the car up a hill -- now the speed would be constant, but the
  pressure might be increasing, reflecting the driver's attempts to keep
  their speed up. So there would be no correlation between the two
  variables in that dataset, despite the continued causal relationship.}

You can still learn about causal relationships from observational
studies, but you have to take a more sophisticated approach. You can't
just measure correlations and leap to causal conclusions. The ``causal
revolution'' in the social sciences has been fueled by the development
of statistical methods for reasoning about causal relationships from
observational datasets.\sidenote{\footnotesize In fact, DAGs are one of the key tools
  that social scientists use to reason about causal relationships. DAGs
  guide the creation of statistical models to estimate particular causal
  effects from observational data. We won't talk about these methods
  here, but if you're interested, check out the suggested readings at
  the end of this chapter.} As interesting as these methods are,
however, they are only applicable in certain specific circumstances. In
contrast, the experimental method \emph{always} works (though of course
there are certain experiments that we can't do for ethical or practical
reasons).

\hypertarget{experiments-help-us-answer-causal-questions}{%
\section{Experiments help us answer causal
questions}\label{experiments-help-us-answer-causal-questions}}

Imagine that you (a) created an exact replica of our world, (b) gave
\$1,000 to everybody in the replica world, and then (c) found a few
years later that everyone in the replica world was happier than their
matched self in the original world. This experiment would provide strong
evidence that money makes people happier. Let's think through why.

Consider a particular person -- if they are happier in the replica
vs.~original world, what could explain that difference? Since we have
replicated the world exactly, but made only one change -- money -- then
that change is the only factor that could explain the difference in
happiness. We can say that we \textbf{held all variables constant}
except for money, which we \textbf{manipulated} experimentally,
observing its effect on some \textbf{measure} -- happiness. This idea --
holding all variables constant except for the specific experimental
manipulation -- is the basic logic that underpins the experimental
method (as articulated by \protect\hyperlink{ref-mill1859}{Mill
1859}).\sidenote{\footnotesize Another way to reason about why we can infer causality
  here follows the counterfactual logic we described in an earlier
  footnote. If the definition of causality is counterfactual (``what
  would have happened if the cause had been different''), then this
  experiment fulfills that definition. In our impossible experiment, we
  can literally \emph{see} the counterfactual: if the person had \$1,000
  more, here's how much happier they would be!} Let's think back to our
observational study of money and happiness. One big causal inference
problem was the presence of ``third variable'' confounds like having
more friends. More friends could cause you to have more money and also
cause you to be happier. The idea of an experiment is to hold everything
else constant -- including the number of friends that people have -- so
we can measure the effect of money on happiness. By holding number of
friends constant, we would be severing the causal links between friends
and both money and happiness. This move is graphically conveyed in the
DAG in Figure~\ref{fig-experiments-money3}, where we ``snip away'' the
friend confound.

\begin{marginfigure}

{\centering \includegraphics{images/experiments/money3-drawing.png}

}

\caption{\label{fig-experiments-money3}In principle, experiments allows
us to ``snip away'' the friend confound by holding it constant (though
in practice, it can be tough to figure out how to hold something
constant when you are talking about people as your unit of study).}

\end{marginfigure}

\hypertarget{we-cant-hold-people-constant}{%
\subsection{We can't hold people
constant}\label{we-cant-hold-people-constant}}

This all sounds great in theory, you might be thinking, but we can't
actually create replica worlds where everything is held constant, so how
do we run experiments in the real world? If we were talking about
experiments on baking cakes, it's easy to see how we could hold all of
the ingredients constant and just vary one thing, like baking
temperature. Doing so would allow us to conduct an experimental test of
the effect of baking temperature. But how we can ``hold something
constant'' when we're talking about people? People aren't cakes. No two
people are alike and, as every parent with multiple children knows, even
if you try to ``hold the ingredients constant'' they don't come out the
same!

If we take two people and give one of them money, we are often comparing
two \emph{different} people, not two instances of the same person with
everything held constant. It wouldn't work to \emph{make} the first
person have more or fewer friends so they match the second person --
that's not holding anything constant, instead it's another (big,
difficult, and potentially unethical) intervention that might itself
cause lots of effects on happiness.

You may be wondering: why don't we just ask people how many friends they
have and use this information to split them into equal groups? You could
do that, but this kind of strategy only allows you to control for the
confounds you know of. For example, you may split people equally based
on their number of friends, but not their education attainment. If
educational attainment also impacts both money and happiness, you still
have a confound. You may then try to split people by both their number
of friends and their education. But perhaps there's another confound
you've missed: sleep quality! Similarly, it also doesn't work to select
people who have the same number of friends -- that only holds the
friends variable constant and not everything \emph{else} that's
different between the two people. So what do we do instead?\sidenote{\footnotesize Many
  researchers who have seen regression models used in the social
  sciences assume that ``controlling for lots of stuff'' is a good way
  to improve causal inference. Not so! In fact, inappropriately
  controlling for a variable in the absence of a clear causal
  justification can actually make your effect estimate \emph{more}
  biased (\protect\hyperlink{ref-wysocki2022}{Wysocki, Lawson, and
  Rhemtulla 2022}).}

\hypertarget{randomization-saves-the-day}{%
\subsection{Randomization saves the
day}\label{randomization-saves-the-day}}

The answer is \textbf{randomization}. If you randomly split a large
roomful of people into two groups, the groups will, on average, have a
similar number of friends. Similarly, if you randomly pick who in your
experiment gets to receive money, you will find that the money and
no-money groups, on average, have a similar number of friends. In other
words, through randomization, the confounding role of friends is
controlled. But the most important thing is that it's not \emph{just}
the role of friends that's controlled; educational attainment, sleep
quality, and all the other confounds are controlled as well. If you
randomly split a large group of people into groups, the groups will, on
average, be equal in every way (Figure~\ref{fig-experiments-money4}).

\begin{marginfigure}

{\centering \includegraphics{images/experiments/money4-drawing.png}

}

\caption{\label{fig-experiments-money4}If you randomly split a large
group of people into groups, the groups will, on average, be equal in
every way.}

\end{marginfigure}

So, here's our simple experimental design: we randomly assign some
people to a money group and some people to a no-money control group!
Then we measure happiness. The basic logic of randomization is that, if
money causes happiness, we should see more happiness -- on average -- in
the money group.\sidenote{\footnotesize You may already be protesting that this
  experiment could be done better. Maybe we could measure happiness
  before and after randomization, to increase precision. Maybe we need
  to give a small amount of money to participants in the control
  condition to make sure that participants in both conditions interact
  with an experimenter and hence that the conditions are as similar as
  possible. We agree! These are important parts of experimental design,
  and we'll touch on them in subsequent chapters.}

Randomization is a powerful tool, but there is a caveat: it doesn't work
every time. \emph{On average}, randomization will ensure that your money
and no-money groups will be equal with respect to confounds like number
of friends, education attainment, and sleep quality. But just as you can
flip a coin and sometimes get heads 9 out of 10 times, sometimes you use
randomization and still get more highly-educated people in one condition
than the other. When you randomize, you guarantee that, on average, all
confounds are controlled. Hence, there is no systematic bias in your
estimate from these confounds. But there will stil be some noise from
random variation.

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{unhappy-randomization}{%
\section*{Unhappy randomization?}\label{unhappy-randomization}}
\addcontentsline{toc}{section}{Unhappy randomization?}

\markright{Unhappy randomization?}

As we've been discussing, random assignment removes confounding by
ensuring that -- on average -- groups are equivalent with respect to all
of their characteristics. Equivalence for any \emph{particular} random
assignment is more likely the larger your sample is, however. Any
individual experiment may be affected by \textbf{unhappy randomization},
when a particular confound is unbalanced between groups by chance.

Unhappy randomization is much more common in small experiments than
larger ones. To see why, we use a technique called \textbf{simulation}.
In simulations, we invent data randomly following a set of assumptions:
we make up a group of participants and generate their characteristics
and their condition assignments. By varying the assumptions we use, we
can investigate how particular choices might change the structure of the
data.

To look at unhappy randomization, we created many simulated versions of
our money-happiness experiment, in which an experimental group receives
money and the control group receives none, and then happiness is
measured for both groups. We assume that each participant has a set
number of friends, and that the more friends they have, the happier they
are. So when we randomly assign them to experimental and control groups,
we run the risk of unhappy randomization -- sometimes one group will
have substantially more friends than the other.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{001-experiments_files/figure-html/experiments-randomization-1.png}

}

\caption{\label{fig-experiments-randomization}Simulated data from our
money-happiness experiment. Each dot represents the measured happiness
effect (vertical position) for an experiment with a set number of
participants in each group (horizontal position). Dot color shows how
uneven friendship is between the groups. The dashed line shows the true
effect.}

\end{figure}

Figure~\ref{fig-experiments-randomization} shows the results of this
simulation. Each dot is an experiment, representing one estimate of the
happiness effect (how much happiness is gained for the amount of money
given to the experimental group). For very small experiments (e.g., with
1 or 3 participants per group), dots are very far from the dashed line
showing the true effect -- meaning these estimates are extremely noisy!
And the reason is unhappy randomization. The upper and lower points are
those in which one group had far more friends than the other.

There are three things to notice about this simulation, however. First,
the noise overall goes down as the sample sizes get bigger: larger
experiments yield estimates closer to the true effect. Second, the
unhappy randomization decreases dramatically as well with larger
samples. Although individuals still differ just as much in large
experiments, the \emph{group} average number of friends is virtually
identical for each condition in the largest groups.

Finally, although the small experiments are individually very noisy, the
\emph{average effect} across all of the small experiments is still very
close to the true effect. This last point illustrates what we mean when
we say that randomized experiments remove confounds. Even though
friendship is still an important factor determining happiness in our
simulation, the average effect across experiments is correct and each
individual estimate is unbiased.

\end{tcolorbox}

In sum, randomization is a remarkably simple and effective way of
holding everything constant besides a manipulated variable. In doing so,
randomization allows experimental psychologists to make unbiased
estimates of causal relationships. Importantly, randomization works both
when you do have control of every aspect of the experiment -- like when
you are baking a cake -- and even when you don't -- like when you are
doing experiments with people.

\hypertarget{generalizability}{%
\section{Generalizability}\label{generalizability}}

When we are asking questions about psychology, it's important to think
about who we are trying to study. Do we want to know if money increases
happiness in \emph{all people}? In people who live in materialistic
societies? In people whose basic needs are not being met? We call the
group we are trying to study our \textbf{population of interest}, and
the people who actually participate in our experiment our
\textbf{sample}.

Sometimes researchers take a sample from one population, but make a
claim about another, usually broader, population. For example, they may
run their experiment with a particular sample of U.S. college students,
but then generalize to all people (their intended population of
interest).\sidenote{\footnotesize Unfortunately, psychologists pervasively assume that
  research on U.S. and European samples generalizes to the rest of the
  world, and it often does not. To highlight this issue, Henrich, Heine,
  and Norenzayan (\protect\hyperlink{ref-henrich2010}{2010}) coined the
  acronym WEIRD. This catchy name describes the oddness of making
  generalizations about all of humanity from experiments on a sample
  that is quite unusual because it is Western, Educated, Industrialized,
  Rich, and Democratic. Henrich and colleagues argue that seemingly
  ``fundamental'' psychological functions like visual perception,
  spatial cognition, and social reasoning all differ pervasively across
  populations -- hence, any generalization from an effect estimated with
  a WEIRD sub-population may be unwarranted.} The mismatch of sample and
population is not always a problem, but quite often causal relationships
are different for different populations.

In the early 2000's, researchers found that gratitude interventions --
like writing a brief essay about something nice that somebody did for
you -- increased happiness in studies conducted in Western countries.
Based on these findings, some psychologists believed that gratitude
interventions could increase happiness in all people. But it seems they
were wrong. A few years later, Layous et al.
(\protect\hyperlink{ref-layous2013culture}{2013}) ran a gratitude
experiment in two locations: the U.S. and South Korea. Surprisingly, the
gratitude intervention decreased happiness in the South Korean sample.
The researchers attributed this negative effect to feelings of
indebtedness that people in South Korea more prominently experienced
when reflecting on gratitude. In this example, we would say that the
findings obtained with the U.S. sample may not \textbf{generalize} to
people in South Korea.

Issues of generalizability extend to all aspects of an experiment, not
just its sample. For example, even if our hypothetical cash intervention
experiment resulted in gains in happiness, we might not be warranted in
generalizing to different ways of providing money. Perhaps there was
something special about the amount of money we gave or the way we
provided it that led to the effect we observed. Without testing multiple
different intervention types, we can't make a broad claim. As we'll see
in \textbf{?@sec-models} and \textbf{?@sec-design}, this issue has
consequences for both our statistical analyses and our experimental
designs (\protect\hyperlink{ref-yarkoni2020}{Yarkoni 2020}).

Questions of generalizability are pervasive, but the first step is to
simply acknowledge and reason about them. Perhaps all papers should have
a Constraints on Generality statement, where researchers discuss whether
they expect their findings to generalize across different samples,
experimental stimuli, procedures, and historical and temporal features
(\protect\hyperlink{ref-simons2017}{Simons, Shoda, and Lindsay 2017}).
This kind of statement would at least remind researchers to be humble:
experiments are a powerful tool for understanding how the world works,
but there are limits to what any individual experiment can teach us.

\hypertarget{anatomy-of-a-randomized-experiment}{%
\section{Anatomy of a randomized
experiment}\label{anatomy-of-a-randomized-experiment}}

Now is a good time for us to go back and consolidate the anatomy of an
experiment, since this anatomy is used throughout the book.
Figure~\ref{fig-experiments-anatomy} shows a simple two-group experiment
like our possible money-happiness intervention. A sample is taken from a
larger population, and then participants in the sample are randomly
assigned to one of two conditions (the manipulation) -- either the
experimental condition, in which money is provided, or the control
condition, in which none is given. Then an outcome measure -- happiness
-- is recorded for each participant.

\begin{figure}

\sidecaption{\label{fig-experiments-anatomy}Anatomy of a randomized
experiment.}

{\centering \includegraphics{images/experiments/anatomy.png}

}

\end{figure}

We'll have a lot more to say about all of these components in subsequent
chapters. We'll discuss measures in \textbf{?@sec-measurement}, because
good measurement is the foundation of a good experiment. Then in
\textbf{?@sec-design} we'll discuss the different kinds of experimental
designs that are possible and their pros and cons. Finally, we'll cover
the process of sampling in \textbf{?@sec-sampling}.

\hypertarget{chapter-summary-experiments}{%
\section{Chapter summary:
Experiments}\label{chapter-summary-experiments}}

In this chapter, we defined an experiment as a combination of a
manipulation and a measure. When combined with randomization,
experiments allow us to make strong causal inferences, even when we are
studying people (who are hard to hold constant). Nonetheless, there are
limits to the power of experiments: there are always constraints on the
sample, experimental stimuli, and procedure that limit how broadly we
can generalize.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Imagine that you run a survey and find that people who spend more time
  playing violent video games tend to be more aggressive (i.e., that
  there is a positive correlation between violent video games and
  aggression). Following Figure~\ref{fig-experiments-money2}, list three
  reasons why these variables may be correlated.
\item
  Suppose you wanted to run an experiment testing whether playing
  violent video games causes increases in aggression. What would be your
  manipulation and what would be your measure? How would you deal with
  potential confounding by variables like age?
\item
  Consider an experiment designed to test people's food preferences. The
  experimenter randomly assigns 30 U.S. preschoolers to be served either
  asparagus or chicken tenders and then asks them how much they enjoyed
  their meal. Overall, children enjoyed the meat more; the experimenter
  writes a paper claiming that humans prefer meat over vegetables. List
  some constraints on the generalizability of this study. In light of
  these constraints, is this study (or some modification) worth doing at
  all?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\item
  A basic introduction to causal inference from a social science
  perspective: Huntington-Klein, N. (2022). \emph{The Effect: An
  Introduction to Research Design and Causality.} Chapman \& Hall.
  Available free online at \url{https://theeffectbook.net}.
\item
  A slightly more advanced treatment, focusing primarily on
  econometrics: Cunningham, S. (2021). \emph{Causal Inference: The
  Mixtape.} Yale Press. Available free online at
  \url{https://mixtape.scunning.com}.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-2}{%
\section*{References}\label{bibliography-2}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-2}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-henrich2010}{}}%
Henrich, Joseph, Steven J Heine, and Ara Norenzayan. 2010. {``The
Weirdest People in the World?''} \emph{Behavioral and Brain Sciences} 33
(2-3): 61--83.

\leavevmode\vadjust pre{\hypertarget{ref-layous2013culture}{}}%
Layous, Kristin, Hyunjung Lee, Incheol Choi, and Sonja Lyubomirsky.
2013. {``Culture Matters When Designing a Successful
Happiness-Increasing Activity: A Comparison of the United States and
South Korea.''} \emph{Journal of Cross-Cultural Psychology} 44 (8):
1294--1303.

\leavevmode\vadjust pre{\hypertarget{ref-lewis1973}{}}%
Lewis, David. 1973. \emph{Counterfactuals}. John Wiley \& Sons.

\leavevmode\vadjust pre{\hypertarget{ref-mill1859}{}}%
Mill, John Stuart. 1859. {``Utilitarianism (1863).''}
\emph{Utilitarianism, Liberty, Representative Government}, 7--9.

\leavevmode\vadjust pre{\hypertarget{ref-pearl1998}{}}%
Pearl, Judea. 1998. {``Graphical Models for Probabilistic and Causal
Reasoning.''} \emph{Quantified Representation of Uncertainty and
Imprecision}, 367--89.

\leavevmode\vadjust pre{\hypertarget{ref-simons2017}{}}%
Simons, Daniel J, Yuichi Shoda, and D Stephen Lindsay. 2017.
{``Constraints on Generality (COG): A Proposed Addition to All Empirical
Papers.''} \emph{Perspectives on Psychological Science} 12 (6):
1123--28.

\leavevmode\vadjust pre{\hypertarget{ref-wysocki2022}{}}%
Wysocki, Anna C, Katherine M Lawson, and Mijke Rhemtulla. 2022.
{``Statistical Control Requires Causal Justification.''} \emph{Advances
in Methods and Practices in Psychological Science} 5 (2):
25152459221095823.

\leavevmode\vadjust pre{\hypertarget{ref-yarkoni2020}{}}%
Yarkoni, Tal. 2020. {``The Generalizability Crisis.''} \emph{Behav.
Brain Sci.} 45: 1--37.

\end{CSLReferences}

\hypertarget{sec-theories}{%
\chapter{Theories}\label{sec-theories}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Define theories and their components
\item
  Contrast different philosophical views on scientific theories
\item
  Analyze features of an experiment that can lead to strong tests of
  theory
\item
  Discuss the role of formalization in theory development
\end{itemize}

\end{tcolorbox}

When you do an experiment, sometimes you just want to see what happens,
like a kid knocking down a tower made of blocks. And sometimes you want
to know the answer to a specific applied question, like ``will giving a
midterm vs.~weekly quizzes lead students in a class to perform better on
the final?'' But more often, our goal is to create \textbf{theories}
that help us explain and predict new observations.

What is a theory? We'll argue here that we should think of psychological
theories as sets of proposed relationships among \textbf{constructs},
which are variables that we think play causal roles in determining
behavior. In this conception of theories, the role of causality is
central: theories are guesses about the causal structure of the mind and
about the causal relationships between the mind and the world. This
definition doesn't include everything that gets called a ``theory'' in
psychology. We describe the continuum between theories and
\textbf{frameworks} -- broad sets of ideas that guide research but don't
make specific contact with particular empirical observations.

We begin this chapter by talking about the specific enterprise of
constructing psychological theories. We'll then discuss how theories
make contact with data, reviewing a bit of the philosophy of science,
and give some guidance on how to construct experiments that test
theories. We end by discussing the relationship between theories and
quantitative models.

\hypertarget{what-is-a-psychological-theory}{%
\section{What is a psychological
theory?}\label{what-is-a-psychological-theory}}

The definition we just gave for a psychological theory is that it is a
proposed set of causal relationships among constructs that helps us
explain behavior. Let's look at the ingredients of a theory: the
constructs and the relationships between them. Then we can ask about how
this definition relates to other things that get called ``theories'' in
psychology.

\hypertarget{psychological-constructs}{%
\subsection{Psychological constructs}\label{psychological-constructs}}

Constructs are the psychological variables that we want our theory to
describe, like ``money'' and ``happiness'' in the example from last
chapter. At first glance, it might seem odd that we need a specific name
for these variables. But in probing the relationship between money and
happiness, we will have to figure out a way to measure happiness. Let's
say we just ask people to answer the question ``how happy are you?'' by
giving ratings on a 1 (miserable) to 10 (elated) scale.

Now say someone in the study reports they are an 8 on this scale. Is
this \emph{really} how happy they are? What if they weren't
concentrating very hard on the rating, or if they thought the researcher
wanted them to be happy? What if they act much less happy in their
interactions with family and friends?

We resolve this dilemma by saying that the self-report ratings we
collect are only a \textbf{measure} of a \textbf{latent} construct,
happiness. The construct is latent because we can never see it directly,
but we think it has a causal influence on the measure: happier people
should, on average, provide higher ratings. But many other factors can
lead to noise or bias in the measurement, so we shouldn't mistake those
ratings as actually \emph{being} the construct.

The particular question ``how happy are you?'' is one way of going from
the general construct to a specific measure. The general process of
going from construct to a specific instantiation that can be measured or
manipulated is called \textbf{operationalization}. Happiness can be
operationalized by self-report, but it can also be operationalized many
other ways, for example through a measure like the use of positive
language in a personal essay, or by ratings by friends, family, or a
clinician. These decisions about how to operationalize a construct with
a particular measure are tricky and consequential, and we discuss them
extensively in \textbf{?@sec-measurement}. Each different
operationalization might be appropriate for a specific study, yet it
would require some justification and argument to connect each one to the
others.

Proposing a particular construct is a very important part of making a
theory. For example, a researcher might worry that self-reported
happiness is very different than someone's well-being as observed by the
people around them, and assert that happiness is not a single construct
but rather a group of distinct constructs. This researcher would then be
surprised to know that self-reports of happiness relate very highly to
others' perceptions of a person's well-being
(\protect\hyperlink{ref-sandvik1993}{Sandvik, Diener, and Seidlitz
1993}).\sidenote{\footnotesize Sometimes positing the construct \emph{is} the key part
  of a theory. \emph{g} (general intelligence) is the classic
  psychological example of a single-construct theory. The idea behind
  \emph{g} theory is that the best measure of general intelligence is
  the shared variance between a wide variety of different tests. The
  decision to theorize about and measure a single unified construct for
  intelligence -- rather than say, many different separate kinds of
  intelligence -- is itself a controversial move.}

Even external, apparently non-psychological variables like money don't
have direct effects on people, but rather operate through psychological
constructs. People studying money seriously as a part of psychological
theories think about perceptions of money in different ways depending on
the context. For example, researchers have written about the importance
of how much money you have on hand based on when in the month your
paycheck arrives (\protect\hyperlink{ref-ellwood-lowe2022}{Ellwood-Lowe,
Foushee, and Srinivasan 2022}), but have also considered perceptions of
long-term accumulation of wealth as a way of conceptualizing people's
understanding of the different resources available to White and Black
families (\protect\hyperlink{ref-kraus2019}{Kraus et al. 2019}).

Finally, a construct can be operationalized through a manipulation: in
our money-happiness example, we operationalized ``more money'' in our
theory with a gift of a specific amount of cash. We hope you see through
these examples that operationalization is a huge part of the craft of
being a psychology researcher -- taking a set of abstract constructs
that you're interested in and turning them into a specific experiment
with a manipulation and a measure that tests your causal theory. We'll
have a lot more to say about how this is done in \textbf{?@sec-design}.

\hypertarget{the-relationships-between-constructs}{}
\subsection{The relationships between constructs}

Constructs gain their meaning in part via their own definitions and
operationalizations, but also in part through their causal relationships
to other constructs. Figure~\ref{fig-theory-nomological-net} shows a
schematic of what this kind of theory might look like -- as you can see,
it looks a lot like the DAGs that we introduced in the last chapter!
That's no accident. The arrows here also describe hypothesized causal
links.\sidenote{\footnotesize Sometimes these kind of diagrams are used in the context
  of a statistical method called Structural Equation Modeling, where
  circles represent constructs and lines represent their relationships
  with one another. Confusingly, structural equation models are also
  used by many researchers to describe psychological theories. The
  important point for now is that they are one particular statistical
  formalism, not a general tool for theory building -- the points we are
  trying to make here are more general.}

\begin{figure}

\sidecaption{\label{fig-theory-nomological-net}A schematic of what a
theory might look like.}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{images/theory/nomological-net.png}

}

\end{figure}

This web of constructs and assumptions is what Cronbach and Meehl
(\protect\hyperlink{ref-cronbach1955}{1955}) referred to as a
``nomological network'' -- a set of proposals about how different
entities are connected to one another. The tricky part is that the key
constructs are never observed directly. They are in people's
heads.\sidenote{\footnotesize We're not saying these should correspond to specific
  brain structures. They could, but most likely they won't. The idea
  that psychological constructs are not the same as any particular brain
  state (and especially not any particular brain region) is called
  ``multiple realizability'' by philosophers, who mostly agree that
  psychological states can't be reduced to brain states, as much as
  philosophers agree on anything
  (\protect\hyperlink{ref-block1972}{Block and Fodor 1972} et seq.).} So
researchers only get to probe them by measuring them through specific
operationalizations.

One poetic way of thinking about this idea is that the theoretical
system of constructs ``floats\ldots{} above the plane of observation and
is anchored to it by the rules of
\protect\hyperlink{sec-measurement}{measurement}.''
(\protect\hyperlink{ref-hempel1952}{Hempel 1952}). So, even if your
theory posits that two constructs (say, money and happiness) are
directly related, the best you can do is manipulate one
operationalization and measure another operationalization. If this
manipulation doesn't produce any effect, it's possible that you are
wrong and money does not cause happiness -- but it is also possible that
your operationalizations are poor.

\begin{tcolorbox}[colframe=.violet, title=\faFastForward \enspace Advanced topic]

\hypertarget{theories-as-compression}{%
\section{Theories as compression}\label{theories-as-compression}}

Here's a slightly different way of thinking about a theory. A theory
provides a \textbf{compression} of potentially complex data into much a
smaller set of general factors. If you have a long sequence of numbers,
say {[}2 4 8 16 32 64 128 256 \ldots{]}, then the expression \(2^n\)
serves as a compression of this sequence -- it's a short expression that
tells you what numbers are in vs.~out of the sequence. In the same way,
a theory can compress a large set of observations (maybe data from many
experiments) into a small set of relationships between constructs. Now,
if your data are noisy, say {[}2.2 3.9 8.1 16.1 31.7 \ldots{} {]}, then
the theory will not be a perfect representation of the data. But it will
still be useful.

\end{tcolorbox}

In particular, having a theory allows you to \textbf{explain} observed
data and \textbf{predict} new data. Both of these are good things for a
theory to do. For example, if it turned out that the money causes
happiness theory was true, we could use it to explain observations such
as greater levels of happiness among wealthy people. We could also make
predictions about the effects of policies like giving out a universal
basic income on overall happiness.\sidenote{\footnotesize The relationship between
  money and happiness is actually much more complicated than what we're
  assuming here. For example, Killingsworth, Kahneman, and Mellers
  (\protect\hyperlink{ref-killingsworth2023}{2023}) describes a
  collaboration between two sets of researchers that had different
  viewpoints on the connection between money and happiness.}

Explanation is an important feature of good theories, but it's also easy
to trick yourself by using a vague theory to explain a finding
\textbf{post-hoc} (after the fact). Thus, the best test of a theory is
typically a new prediction, as we discuss below.

\hypertarget{specific-theories-vs.-general-frameworks}{%
\subsection{Specific theories vs.~general
frameworks}\label{specific-theories-vs.-general-frameworks}}

You may be thinking, ``psychology is full of theories but they don't
look that much like the ones you're talking about!'' Very few of the
theories that bear that label in psychology describe causal
relationships linking clearly defined and operationalized constructs.
You also don't see that many DAGs, though these are getting (slightly)
more common lately (\protect\hyperlink{ref-rohrer2018}{Rohrer 2018}).

Here's an example of something that gets called a theory yet doesn't
share the components described above. Bronfenbrenner
(\protect\hyperlink{ref-bronfenbrenner1992}{1992})'s Ecological Systems
Theory (EST) is pictured in Figure~\ref{fig-theory-bronfenbrenner}. The
key thesis of this theory is that children's development occurs in a set
of nested contexts that each affect one another and in turn affect the
child. This theory has been immensely influential. Yet if it's read as a
causal theory, it's almost meaningless: nearly everything connects to
everything in both directions and the constructs are not operationalized
-- it's very hard to figure out what kind of predictions it makes!

\begin{marginfigure}

{\centering \includegraphics{images/theory/bronfenbrenner2.png}

}

\caption{\label{fig-theory-bronfenbrenner}The diagram often used to
represent Bronfenbrenner's ecological systems theory. Note that circles
no longer denote discrete constructs; arrows can be interpreted as
causal relationships, but all constructs are assumed to be fully
connected.}

\end{marginfigure}

EST is not really a theory in the sense that we are advocating for in
this chapter (and the same goes for many other very interesting ideas in
psychology). It's not a set of causal relationships between constructs
that allow specific predictions about future observations. EST is
instead a broad set of ideas about what sorts of theories are more
likely to explain specific phenomena. For example, it helps remind us
that a child's behavior is likely to be influenced by a huge range of
factors, such that any individual theory cannot just focus on an
individual factor and hope to provide a full explanation. In this sense,
EST is a \textbf{framework}: it guides and inspires specific theories --
in the sense we've discussed here, namely a set of causal relationships
between constructs -- without being a theory itself.

Frameworks are often incredibly important. Ideas like EST have inspired
huge amounts of interesting research. They can also make a big
difference to practice. For example, EST supports a model in social work
in which children's needs are considered not only as the expression of
specific internal developmental issues but also as stemming from a set
of overlapping contextual factors
(\protect\hyperlink{ref-ungar2002}{Ungar 2002}). Concretely, a therapist
might be more likely to examine family, peer, and school environments
when analyzing a child's situation through the lens of EST.

There's a continuum between precisely specified theories and broad
frameworks. Some theories propose interconnected constructs but don't
specify the relationships between them, or don't specify how those
constructs should be operationalized. So when you read a paper that says
it proposes a ``theory,'' it's a good idea to to ask whether it
describes specific relations between operationalized constructs. If it
doesn't that, it may be more of a framework than a theory.

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{a-theory-of-whom}{%
\section*{A theory of whom?}\label{a-theory-of-whom}}
\addcontentsline{toc}{section}{A theory of whom?}

\markright{A theory of whom?}

Theory development isn't just about knowledge for knowledge's sake -- it
has implications for the technologies and policies built off the
theories.

One case study comes from Edward Clarke's infamous theory regarding the
deleterious effects of education for women
(\protect\hyperlink{ref-clarke1884}{Clarke 1884}). Clarke posited that
(1) cognitive and reproductive processes relied on the same fixed pool
of energy, (2) relative to men, women's reproductive processes required
more energy, and that (3) expending too much energy on cognitive tasks
like education depleted women of the energy needed to maintain a healthy
reproductive system. Based on case studies, Clarke suggested that
education was causing women to become ill, experience fertility issues,
and birth weaker offspring. He thus concluded that ``boys must study and
work in a boy's way, and girls in a girl's way'' (p.~18).

Clarke's work is a chilling example of the implication of a
poorly-developed theory. In this scenario, Clarke had neither
instruments that allowed him to measure his constructs or experiments to
measure the causal connections between them. Instead, he merely
highlighted case studies that were consistent with his idea (while
simultaneously dismissing cases that were inconsistent). His ideas
eventually lost favor -- especially as they were subjected to more
rigorous tests. But Clarke's arguments were used to attempt to dissuade
women from pursuing higher education and hindered educational policy
reform.

\end{tcolorbox}

\hypertarget{how-do-we-test-theories}{%
\section{How do we test theories?}\label{how-do-we-test-theories}}

Our view of psychological theories is that they describe a set of
relationships between different constructs. How can we test theories and
decide which one is best? We'll first describe
\textbf{falsificationism}, a historical viewpoint on this issue that has
been very influential in the past and that connects to ideas about
statistical inference presented in \textbf{?@sec-inference}. We'll then
turn to a more modern viewpoint, \textbf{holism}, that recognizes the
interconnections between theory and measurement.

\hypertarget{falsificationism}{%
\subsection{Falsificationism}\label{falsificationism}}

One historical view that resonates with many scientists is the
philosopher Karl Popper's \textbf{falsificationism}. In particular,
there is a simplistic version of falsificationism that is often repeated
by working scientists, even though it's much less nuanced than what
Popper actually said! On this view, a scientific theory is a set of
hypotheses about the world that instantiate claims like the connection
between money and happiness.\sidenote{\footnotesize Earlier we treated the claim that
  money caused happiness as a theory. It is one! It's just a very simple
  theory that has only one hypothesized connection in it.} What makes a
statement a \emph{scientific} hypothesis is that it can be disproved
(i.e., it is \textbf{falsifiable}) by an observation that contradicts
it. For example, observing a lottery winner who immediately becomes
depressed would falsify the hypothesis that receiving money makes you
happier.

For the simplistic falsificationist, theories are never
\textbf{confirmed}. The hypotheses that form parts of theories are
universal statements. You can never prove them right; you can only fail
to find falsifying evidence. Seeing hundreds of people get happier when
they received money would not prove that the money-happiness hypothesis
was universally true. There could always be a counter-example around the
corner.

This theory doesn't really describe how scientists work. For example,
scientists like to say that their evidence ``supports'' or ``confirms''
their theory, and falsificationism rejects this kind of talk. A
falsificationist says that confirmation is an illusion; that the theory
is simply surviving to be tested another day. This strict
falsificationist perspective is unpalatable to many scientists. After
all, if we observe that hundreds of people get happier when they receive
money, it seems like this should at least slightly increase our
confidence that money causes happiness!\sidenote{\footnotesize An alternative
  perspective comes from the Bayesian tradition that we'll learn more
  about in Chapters \textbf{?@sec-estimation} and
  \textbf{?@sec-inference}. In a nutshell, Bayesians propose that our
  subjective belief in a particular hypothesis can be captured by a
  probability, and that our scientific reasoning can then be described
  by a process of normative probabilistic reasoning
  (\protect\hyperlink{ref-strevens2006}{Strevens 2006}). The Bayesian
  scientist distributes probability across a wide range of alternative
  hypotheses; observations that are more consistent with a hypothesis
  increase the hypothesis's probability
  (\protect\hyperlink{ref-sprenger2019}{Sprenger and Hartmann 2019}).}

\hypertarget{a-holistic-viewpoint-on-theory-testing}{%
\subsection{A holistic viewpoint on theory
testing}\label{a-holistic-viewpoint-on-theory-testing}}

The key issue that leads us to reject strict falsificationism is the
observation that no individual hypothesis (a part of a theory) can be
falsified independently. Instead, a large series of what are called
\textbf{auxiliary assumptions} (or auxilliary hypotheses) are usually
necessary to link an observation to a theory
(\protect\hyperlink{ref-lakatos1976}{Lakatos 1976}). For example, if
giving some individual person money didn't change their happiness, we
wouldn't immediately throw out our theory that money causes happiness.
Instead, the fault might be in any one of our auxiliary assumptions,
like our measurement of happiness, or our choice of how much money to
give or when to give it. The idea that individual parts of a theory
can't be falsified independently is sometimes called \textbf{holism}.

One consequence of holism is that the relationship between data and
theory isn't always straightforward. An unexpected observation may not
cause us to give up on a main hypothesis in our theory -- but it will
often cause us to question our auxiliary assumptions instead (e.g., how
we operationalize our constructs). Thus, before abandoning our theory of
money causing happiness, we might want to try several happiness
questionnaires!

The broader idea of holism is supported by historical and sociological
studies of how science progresses, especially in the work of Kuhn
(\protect\hyperlink{ref-kuhn1962}{1962}). Examining historical evidence,
Kuhn found that scientific revolutions didn't seem to be caused by the
falsification of a theoretical statement via an incontrovertible
observation. Instead, Kuhn described scientists as mostly working within
\textbf{paradigms}: sets of questions, assumptions, methods, phenomena,
and explanatory hypotheses.

Paradigms allow for activities Kuhn described as \textbf{normal science}
-- that is, testing questions within the paradigm, explaining new
observations or modifying theory to fit these paradigms. But normal
science is punctuated by periods of \textbf{crisis} when scientists
begin to question their theory and their methods. Crises don't happen
just because a single observation is inconsistent with the current
theory. Rather, there will often be a holistic transition to a new
paradigm, typically because of a striking explanatory or predictive
success -- often one that's outside the scope of the current working
theory entirely.

In sum, the lesson of holism is that we can't just put our theories in
direct contact with evidence and think that they will be supported or
overturned. Instead, we need to think about the scope of our theory (in
terms of the phenomena and measures it is meant explain), as well as the
auxiliary hypotheses -- operationalizations -- that link it to specific
observations.

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{what-makes-something-science-versus-pseudoscience}{%
\section*{What makes something ``science'' versus
``pseudoscience''?}\label{what-makes-something-science-versus-pseudoscience}}
\addcontentsline{toc}{section}{What makes something ``science'' versus
``pseudoscience''?}

\markright{What makes something ``science'' versus ``pseudoscience''?}

We often want to distinguish between real science (such as astronomy)
and ``pseudoscience'', which refers to nonscientific ideas that are
presented as being scientific. One well known example is ``scientific
creationism,'' which claims to provide scientific evidence for the
divine creation of the earth and all living species roughly 6000 years
ago. When we say that creationism is a pseudoscience, what do we mean?

Philosophers of science have long argued about what divides science from
pseudoscience, which they call the ``demarcation problem.'' One of the
most influential ideas about the demarcation problem came from Karl
Popper, who claimed that what defined a hypothesis as scientific is that
it is falsifiable. But others have pointed out that many of the things
we call pseudoscience do indeed make falsifiable claims that have been
falsified; for example, researchers have tested and found no empirical
evidence for the relationship between astrological signs and personality
(REF). In fact, there doesn't turn out to be any simple rule that
distinguishes what we call science and what we call pseudoscience, in
part because science encompasses such a broad and diverse set of
practices.

CONCLUSION TBD

\end{tcolorbox}

\hypertarget{designing-experiments-to-test-theory}{%
\section{Designing experiments to test
theory}\label{designing-experiments-to-test-theory}}

\begin{marginfigure}

{\centering \includegraphics{images/theory/roulette2.png}

}

\caption{\label{fig-theory-roulette}A roulette wheel. Betting on red is
not that risky, but betting all your chips on a particular value (*) is
much riskier.}

\end{marginfigure}

One way of looking at theories is that they let us make \emph{bets}. If
we bet on a spin of the roulette wheel in
Figure~\ref{fig-theory-roulette} that it will show us red as opposed to
black, we have almost a 50\% chance of winning the bet. Winning such a
bet is not impressive. But if we call a particular number, the bet is
riskier because we have a much smaller chance of being right. Cases
where a theory has many chances to be wrong are called \textbf{risky
tests} (\protect\hyperlink{ref-meehl1978}{Meehl 1978}).\sidenote{\footnotesize Even if
  you're not a \emph{falsificationist} like Popper, you can still think
  it's useful to try and falsify theories! Although a single observation
  is not always enough to overturn a theory, it's still a great research
  strategy to look for those observations that are most inconsistent
  with the theory.}

Much psychology consists of verbal theories. Verbal theories make only
qualitative predictions, so it is hard convincingly show them to be
wrong (\protect\hyperlink{ref-meehl1990}{Meehl 1990}). In our discussion
of money and happiness, we just expected happiness to go up as money
increased. We would have accepted \emph{any} increase in happiness (even
if very small) as evidence confirming our hypothesis. Predicting that it
does is a bit like betting on red with the roulette wheel -- it's not
surprising or impressive when you win. And in psychology, verbal
theories often predict that multiple factors interact with one another.
With these theories, it's easy to say that one or the other was
``dominant'' in a particular situation, meaning you can predict almost
any direction of effect.

To test theories, we should design experiments to test conditions where
our theories make ``risky'' predictions. A stronger version of the
money-happiness theory might suggest that happiness increases linearly
in the logarithm of income
(\protect\hyperlink{ref-killingsworth2023}{Killingsworth, Kahneman, and
Mellers 2023}). This specific mathematical form for the relationship --
as well as the more specific operationalization of money as income --
creates opportunities for making much riskier bets about new
experiments. This kind of case is more akin to betting on a specific
number on the roulette wheel: when you win this bet, it is quite
surprising!\sidenote{\footnotesize Theories are often developed iteratively. It's
  common to start with a theory that is less precise and hence, that has
  fewer opportunities for risky tests. But by collecting data and
  testing different alternatives, it's often possible to refine the
  theory so that it is more specific and allows riskier tests. As we
  discuss below, formalizing theories using mathematical or
  computational models is one important route to making more specific
  predictions and creating riskier tests.}

Testing theoretical predictions also requires precise experimental
measurements. As we start to measure the precision of our experimental
estimates in \textbf{?@sec-inference}, we'll see that the more precise
our estimate is, the more values are inconsistent with it. In this
sense, a risky test of a theory requires both a very specific prediction
and a precise measurement. (Imagine spinning the roulette wheel but
seeing such a blurry image of the result that you can't really tell
where the ball is. Not very useful.)

Even when theories make precise predictions, they can still be too
flexible to be tested. When a theory has many \textbf{free parameters}
-- numerical values that can be fit to a particular dataset, changing
the theories predictions on a case-by-case basis -- then it can often
predict a wide range of possible results. This kind of flexibility
reduces the value of any particular experimental test, because the
theorist can always say after the fact that the parameters were wrong
but not the theory itself (\protect\hyperlink{ref-roberts2000}{Roberts
and Pashler 2000}).

One important way to remove this kind of flexibility is to make
predictions in advance, holding all parameters constant. A
preregistration is a great way to do this -- the experimenter derives
predictions and specifies in advance how they will be compared to the
results of the experiment. We'll talk much more about the process of
preregistration in \textbf{?@sec-prereg}.

Finally, we've been focusing mostly on testing a single theory. But the
best state of affairs is if a theory can make a very specific prediction
that other theories don't make. If competing theories both predict that
money increases happiness to the same extent, then data consistent with
that predicted relationship don't differentiate between the theories, no
matter how specific the prediction might be. The experiment that teaches
us the most is going to be the one where a very specific pattern of data
is predicted according to one theory and another.\sidenote{\footnotesize We can use
  this idea, which comes from Bayesian statistics, to try to figure out
  what the \emph{right} experiment is by considering which specific
  experimental conditions derive differences between theories. In fact,
  the idea of choosing experiments based on the predictions that
  different theories make has a long history in statistics
  (\protect\hyperlink{ref-lindley1956}{Lindley 1956}); it's now called
  \textbf{optimal experiment design}
  (\protect\hyperlink{ref-myung2013}{Myung, Cavagnaro, and Pitt 2013};
  \protect\hyperlink{ref-ouyang2018}{Ouyang et al. 2018}). The idea is,
  if you have two or more theories spelled out mathematically or
  computationally, you can simulate their predictions across a lot of
  conditions and pick the most informative conditions to run as an
  actual experiment.}

\hypertarget{formalizing-theories}{%
\section{Formalizing theories}\label{formalizing-theories}}

Say we have a set of constructs we want to theorize about. How do we
describe our ideas about the relationships between them so that we can
make precise predictions that can be compared with other theories? As
one writer noted, mathematics is ``unreasonably effective'' as a
vocabulary for the sciences (\protect\hyperlink{ref-wigner1990}{Wigner
1990}). Indeed, there have been calls for greater formalization of
theory in psychology for at least the last 50 years
(\protect\hyperlink{ref-harris1976}{Harris 1976}).

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{a-universal-law-of-generalization}{%
\section*{A universal law of
generalization?}\label{a-universal-law-of-generalization}}
\addcontentsline{toc}{section}{A universal law of generalization?}

\markright{A universal law of generalization?}

How do you take what you know and apply it to a new situation? One
answer is that you use the same answer that has worked in similar
situations. To do this kind of extrapolation, however, you need a notion
of similarity. Early learning theorists tried to measure similarity by
creating an association between a stimulus -- say a projected circle of
light of a particular size -- and a reward by repeatedly presenting them
together. After this association was learned, they would test
generalization by showing circles of different sizes and measuring the
strength of the expectation for a reward. These experiments yielded
generalization curves: the more similar the stimulus, the more people
and other animals would give the same response, signaling
generalization.

Shepard (\protect\hyperlink{ref-shepard1987}{1987}) was interested in
unifying the results of these different experiments. The first step in
this process was establishing a \textbf{stimulus space}. He used a
procedure called ``multidimensional scaling'' to infer how close stimuli
were to each other on the basis of how strong the generalization between
them was. When he plotted the strength of the generalization by the
distance between stimuli within this space (their similarity), he found
an incredibly consistent pattern: generalization decreased exponentially
as similarity decreased.

He argued that this described a ``universal law'' that governed the
relationship between similarity and generalization for almost any
stimulus, whether it was the size of circles, the color of patches of
light, or the similarity between speech sounds. Later work has even
extended this same framework to highly abstract dimensions such as the
relationships between numbers of different types {[}e.g., being even,
being powers of 2, etc.; Tenenbaum
(\protect\hyperlink{ref-tenenbaum2000}{2000}){]}.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{images/theory/shepard_network3.png}

}

\caption{\label{fig-theory-shepard}The causal theory of similarity and
generalization posited by Shepard
(\protect\hyperlink{ref-shepard1987}{1987}).}

\end{figure}

The pattern shown in Shepard's work is an example of \textbf{inductive
theory building}. In the vocabulary we're developing, Shepard ran (or
obtained the data from) randomized experiments in which the manipulation
was stimulus dimension (e.g., circle size) and the measure was
generalization strength. Then the theory that Shepard proposed was that
manipulations of stimulus dimension acted to change the perceived
similarity between the stimuli. His theory thus linked two constructs:
stimulus similarity and generalization strength
(Figure~\ref{fig-theory-shepard}). Critically the causal relationship he
described was not just a qualitative relationship but instead a specific
mathematical form.

Shepard wrote in the conclusion of his 1987 paper, ``Possibly, behind
the diverse behaviors of humans and animals, as behind the various
motions of planets and stars, we may discern the operation of universal
laws.'' While Shepard's dream is an ambitious one, it defines an ideal
for psychological theorizing.

\end{tcolorbox}

There is no one approach that will be right for theorizing across all
areas of psychology (\protect\hyperlink{ref-oberauer2019}{Oberauer and
Lewandowsky 2019}; \protect\hyperlink{ref-smaldino2020}{Smaldino 2020}).
Mathematical theories like Shepard
(\protect\hyperlink{ref-shepard1987}{1987}) (see Depth box) have long
been one tool that allows for precise statements of particular
relationships.

Computational or formal artifacts are not themselves psychological
theories, but they can be used to create psychological theories via the
mapping of constructs onto entities in the model and the use of the
principles of the formalism to instantiate psychological hypotheses or
assumptions (\protect\hyperlink{ref-guest2021}{Guest and Martin
2021}).\sidenote{\footnotesize This book won't go into more details about routes to
  building computational theories, but if you are interested, we
  encourage you to explore these frameworks as a way to deepen your
  theoretical contributions and to sharpen your experimental choices.}
Yet stating such clear and general laws feels out of reach in many
cases. If we had more Shepard-style theorists or theories, perhaps we'd
be in a better place. Or perhaps such ``universal laws'' are simply out
of reach for most of human behavior.

An alternative approach creates statistical models of data that
incorporate substantive assumptions about the structure of the data. We
use such models all the time for data analysis. The trouble is, we often
don't interpret them as having substantive assumptions about the
structure of the data, even when they do
(\protect\hyperlink{ref-fried2020}{Fried 2020})! But if we examine these
assumptions explicitly, even the simplest statistical models can be
productive tools for building theories.

For example, if we set up a simple linear regression model to estimate
the relationship between money and happiness, we'd be positing a linear
relationship between the two variables -- that an increase in one would
always lead to a proportional increase in the other.\sidenote{\footnotesize Linear
  models are ubiquitous in the social sciences because they are
  convenient to fit, but as theoretical models they are deeply
  impoverished. There is a lot you can do with a linear regression, but
  in the end, most interesting processes are not linear combinations of
  factors!} If we fit the model to a particular dataset, we could then
look at the weights of the model. Our theory might then then be
something like ``giving people \$100 causes .2 points of increase in
happiness on a self-report scale.''

Obviously, this regression model is not a very good theory of the
broader relationship between money and happiness, since it posits that
everyone's happiness would be at the maximum on the 10 point scale if
you gave them (at most) \$4500. It also doesn't tell us how this theory
would generalize to other people, other measures of happiness, or other
aspects of the psychological representation of money such as income or
wealth.

From our viewpoint, these sorts of questions are not distractions --
they are the critical work of moving from experiment to theory
(\protect\hyperlink{ref-smaldino2020}{Smaldino 2020})! In
\textbf{?@sec-models}, we try to draw out this idea further,
reconstruing common statistical tests as models that can be repurposed
to express contentful scientific hypotheses while recognizing the
limitations of their assumptions.

One of the strengths of modern cognitive science is that it provides a
very rich set of tools for expressing more complex statistical models
and linking them to data. For example, the modern Bayesian cognitive
modeling tradition grew out of work like Shepard's; in these models, a
system of equations defines a probability distribution that can be used
to estimate parameters, predict new data, or make other inferences
(\protect\hyperlink{ref-probmods2}{Goodman, Tenenbaum, and Contributors
2016}). And neural network models -- which are now fueling innovations
in artificial intelligence -- have a long history of being used as
substantive models of human psychology
(\protect\hyperlink{ref-elman1996}{Elman, Bates, and Johnson 1996}).

In our discussion, we've presented theories as static entities that are
presented, tested, confirmed, and falsified. That's a simplification
that doesn't take into account the ways that theories -- especially when
instantiated as formal models -- can be flexibly adjusted to accommodate
new data (\protect\hyperlink{ref-navarro2019}{Navarro 2019}). Most
modern computational theories are more like a combination of core
principles, auxiliary assumptions, and supporting empirical assumptions.
The best theories are always being enlarged and refined in response to
new data.\sidenote{\footnotesize In the thinking of the philosopher Imre Lakatos, a
  ``productive'' research program is one where the core principles are
  gradually supplemented with a limited set of additional assumptions to
  explain a growing base of observations. In contrast, a ``degenerate''
  research program is one in which you are constantly making ad-hoc
  tweaks to the theory to explain each new datapoint
  (\protect\hyperlink{ref-lakatos1976}{Lakatos 1976}).}

\hypertarget{chapter-summary-theories}{%
\section{Chapter summary: Theories}\label{chapter-summary-theories}}

In this chapter, we characterized psychological theories as a set of
causal relationships between latent constructs. The role of experiments
is to measure these causal relationships and to adjudicate between
theories by identifying cases where different theories make different
predictions about particular relationships.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Identify an influential theory in your field or sub-field. Can you
  draw the ``nomological network'' for it? What are the key constructs
  and how are they measured? Are the links between constructs just
  directional links or is there additional information about what type
  of relationship exists? Or does our description of a theory in this
  chapter not fit your example?
\item
  Can you think of an experiment that falsified a theory in your area of
  psychology? To what extent is falsification possible for the kinds of
  theories that you are interested in studying?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\item
  A fabulous introduction to issues in the philosophy of science can be
  found in: Godfrey-Smith, P. (2009). Theory and reality. University of
  Chicago Press.
\item
  Bayesian modeling has been very influential in cognitive science and
  neuroscience. A good introduction in cognitive science comes from:
  Lee, M. D. \& Wagenmakers, E. J. (2013). \emph{Bayesian Cognitive
  Modeling: A Practical Course}. Cambridge University Press. Much of the
  book is available free online at
  \url{https://faculty.sites.uci.edu/mdlee/bgm/}.
\item
  A recent introduction to Bayesian modeling with a neuroscience focus:
  Ma, W. J., Kording, K. P., \& Goldreich, D. (2022). \emph{Bayesian
  models of perception and action: An introduction}. MIT Press. Free
  online at \url{https://www.cns.nyu.edu/malab/bayesianbook.html}.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-3}{%
\section*{References}\label{bibliography-3}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-3}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-block1972}{}}%
Block, Ned J, and Jerry A Fodor. 1972. {``What Psychological States Are
Not.''} \emph{The Philosophical Review} 81 (2): 159--81.

\leavevmode\vadjust pre{\hypertarget{ref-bronfenbrenner1992}{}}%
Bronfenbrenner, Urie. 1992. \emph{Ecological Systems Theory.} Jessica
Kingsley Publishers.

\leavevmode\vadjust pre{\hypertarget{ref-clarke1884}{}}%
Clarke, Edward H. 1884. \emph{Sex in Education: Or, a Fair Chance for
the Girl}. Boston: Houghton, Mifflin; Company.

\leavevmode\vadjust pre{\hypertarget{ref-cronbach1955}{}}%
Cronbach, L J, and P E Meehl. 1955. {``Construct Validity in
Psychological Tests.''} \emph{Psychol. Bull.} 52 (4): 281--302.

\leavevmode\vadjust pre{\hypertarget{ref-ellwood-lowe2022}{}}%
Ellwood-Lowe, Monica E, Ruthe Foushee, and Mahesh Srinivasan. 2022.
{``What Causes the Word Gap? Financial Concerns May Systematically
Suppress Child-Directed Speech.''} \emph{Developmental Science} 25 (1):
e13151.

\leavevmode\vadjust pre{\hypertarget{ref-elman1996}{}}%
Elman, Jeffrey L, Elizabeth A Bates, and Mark H Johnson. 1996.
\emph{Rethinking Innateness: A Connectionist Perspective on
Development}. Vol. 10. MIT press.

\leavevmode\vadjust pre{\hypertarget{ref-fried2020}{}}%
Fried, Eiko I. 2020. {``Lack of Theory Building and Testing Impedes
Progress in the Factor and Network Literature.''} \emph{Psychological
Inquiry} 31 (4): 271--88.

\leavevmode\vadjust pre{\hypertarget{ref-probmods2}{}}%
Goodman, Noah D, Joshua B. Tenenbaum, and The ProbMods Contributors.
2016. {``{Probabilistic Models of Cognition}.''}
\url{https://probmods.org/}.

\leavevmode\vadjust pre{\hypertarget{ref-guest2021}{}}%
Guest, Olivia, and Andrea E Martin. 2021. {``How Computational Modeling
Can Force Theory Building in Psychological Science.''}
\emph{Perspectives on Psychological Science} 16 (4): 789--802.

\leavevmode\vadjust pre{\hypertarget{ref-harris1976}{}}%
Harris, Richard J. 1976. {``The Uncertain Connection Between Verbal
Theories and Research Hypotheses in Social Psychology.''} \emph{Journal
of Experimental Social Psychology} 12 (2): 210--19.

\leavevmode\vadjust pre{\hypertarget{ref-hempel1952}{}}%
Hempel, Carl G. 1952. {``Fundamentals of Concept Formation in Empirical
Science, Vol. Ii. No. 7.''}

\leavevmode\vadjust pre{\hypertarget{ref-killingsworth2023}{}}%
Killingsworth, Matthew A, Daniel Kahneman, and Barbara Mellers. 2023.
{``Income and Emotional Well-Being: A Conflict Resolved.''}
\emph{Proceedings of the National Academy of Sciences} 120 (10):
e2208661120.

\leavevmode\vadjust pre{\hypertarget{ref-kraus2019}{}}%
Kraus, Michael W, Ivuoma N Onyeador, Natalie M Daumeyer, Julian M
Rucker, and Jennifer A Richeson. 2019. {``The Misperception of Racial
Economic Inequality.''} \emph{Perspectives on Psychological Science} 14
(6): 899--921.

\leavevmode\vadjust pre{\hypertarget{ref-kuhn1962}{}}%
Kuhn, Thomas. 1962. \emph{The Structure of Scientific Revolutions}.
Princeton University Press.

\leavevmode\vadjust pre{\hypertarget{ref-lakatos1976}{}}%
Lakatos, Imre. 1976. {``Falsification and the Methodology of Scientific
Research Programmes.''} In \emph{Can Theories Be Refuted?}, 205--59.
Springer.

\leavevmode\vadjust pre{\hypertarget{ref-lindley1956}{}}%
Lindley, Dennis V. 1956. {``On a Measure of the Information Provided by
an Experiment.''} \emph{The Annals of Mathematical Statistics},
986--1005.

\leavevmode\vadjust pre{\hypertarget{ref-meehl1978}{}}%
Meehl, Paul E. 1978. {``Theoretical Risks and Tabular Asterisks: Sir
Karl, Sir Ronald, and the Slow Progress of Soft Psychology.''} \emph{J.
Consult. Clin. Psychol.} 46 (4): 806--34.

\leavevmode\vadjust pre{\hypertarget{ref-meehl1990}{}}%
---------. 1990. {``Why Summaries of Research on Psychological Theories
Are Often Uninterpretable.''} \emph{Psychological Reports} 66 (1):
195--244.

\leavevmode\vadjust pre{\hypertarget{ref-myung2013}{}}%
Myung, Jay I, Daniel R Cavagnaro, and Mark A Pitt. 2013. {``A Tutorial
on Adaptive Design Optimization.''} \emph{Journal of Mathematical
Psychology} 57 (3-4): 53--67.

\leavevmode\vadjust pre{\hypertarget{ref-navarro2019}{}}%
Navarro, Danielle J. 2019. {``Between the Devil and the Deep Blue Sea:
Tensions Between Scientific Judgement and Statistical Model
Selection.''} \emph{Computational Brain \& Behavior} 2 (1): 28--34.

\leavevmode\vadjust pre{\hypertarget{ref-oberauer2019}{}}%
Oberauer, Klaus, and Stephan Lewandowsky. 2019. {``Addressing the Theory
Crisis in Psychology.''} \emph{Psychonomic Bulletin \& Review} 26 (5):
1596--1618.

\leavevmode\vadjust pre{\hypertarget{ref-ouyang2018}{}}%
Ouyang, Long, Michael Henry Tessler, Daniel Ly, and Noah D Goodman.
2018. {``Webppl-Oed: A Practical Optimal Experiment Design System.''} In
\emph{CogSci}.

\leavevmode\vadjust pre{\hypertarget{ref-roberts2000}{}}%
Roberts, Seth, and Harold Pashler. 2000. {``How Persuasive Is a Good
Fit? A Comment on Theory Testing.''} \emph{Psychological Review} 107
(2): 358.

\leavevmode\vadjust pre{\hypertarget{ref-rohrer2018}{}}%
Rohrer, Julia M. 2018. {``Thinking Clearly about Correlations and
Causation: Graphical Causal Models for Observational Data.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (1):
27--42.

\leavevmode\vadjust pre{\hypertarget{ref-sandvik1993}{}}%
Sandvik, Ed, Ed Diener, and Larry Seidlitz. 1993. {``Subjective
Well-Being: The Convergence and Stability of Self-Report and
Non-Self-Report Measures.''} \emph{Journal of Personality} 61 (3):
317--42.

\leavevmode\vadjust pre{\hypertarget{ref-shepard1987}{}}%
Shepard, Roger N. 1987. {``Toward a Universal Law of Generalization for
Psychological Science.''} \emph{Science} 237 (4820): 1317--23.

\leavevmode\vadjust pre{\hypertarget{ref-smaldino2020}{}}%
Smaldino, Paul E. 2020. {``How to Translate a Verbal Theory into a
Formal Model.''} \emph{Social Psychology}.

\leavevmode\vadjust pre{\hypertarget{ref-sprenger2019}{}}%
Sprenger, Jan, and Stephan Hartmann. 2019. \emph{Bayesian Philosophy of
Science}. Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-strevens2006}{}}%
Strevens, Michael. 2006. {``The Bayesian Approach to the Philosophy of
Science.''}

\leavevmode\vadjust pre{\hypertarget{ref-tenenbaum2000}{}}%
Tenenbaum, Joshua B. 2000. {``Rules and Similarity in Concept
Learning.''} \emph{Advances in Neural Information Processing Systems}
12: 59--65.

\leavevmode\vadjust pre{\hypertarget{ref-ungar2002}{}}%
Ungar, Michael. 2002. {``A Deeper, More Social Ecological Social Work
Practice.''} \emph{Social Service Review} 76 (3): 480--97.

\leavevmode\vadjust pre{\hypertarget{ref-wigner1990}{}}%
Wigner, Eugene P. 1990. {``The Unreasonable Effectiveness of Mathematics
in the Natural Sciences.''} In \emph{Mathematics and Science}, 291--306.
World Scientific.

\end{CSLReferences}

\hypertarget{sec-replication}{%
\chapter{Reproducibility}\label{sec-replication}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Define and distinguish reproducibility and replicability
\item
  Synthesize the meta-scientific literature on replication and the
  causes of replication failures
\item
  Reason about the relation of replication to theory building
\end{itemize}

\end{tcolorbox}

In the previous chapters, we introduced experiments, their connection
with causal inference, and their role in building psychological theory.
In principle, repeated experimental work combined with theory building
should yield strong research programs that explain and predict phenomena
with increasing scope.

Yet in recent years there has been an increasing recognition that this
idealized view of science might not be a good description of what we
actually see when we look at the psychology literature. Many classic
findings may be wrong, or at least overstated. Their statistical tests
might not be trustworthy. The actual numbers are even wrong in many
papers! And even when experimental findings are ``real'', they may not
generalise broadly to different people and different situations.

How do we know about these problems? A burgeoning field called
\textbf{meta-science} is providing the evidence. Meta-science is
research \emph{about research}, for example investigating how often
findings in a literature can be successfully built on, or trying to
figure out how widespread some negative practice is. Meta-science allows
us to go beyond one-off anecdotes about a particular set of flawed
results or rumors about bad practices. Perhaps the most obvious sign
that something is wrong is that when independent scientists team up in
meta-science projects and try to repeat previous studies, they often do
not get the same results.

\begin{marginfigure}

{\centering \includegraphics{images/replication/terms2.png}

}

\caption{\label{fig-replication-terms}A framework for understanding
different terms related to the repeatabilty of scientific findings.}

\end{marginfigure}

Before we begin reviewing this evidence, let's discuss the different
ways in which a scientific finding can be repeated.
Figure~\ref{fig-replication-terms} gives us a basic starting point for
our definitions (based on
\href{https://figshare.com/articles/Publishing_a_reproducible_paper/5440621}{"Publishing
a reproducible paper" by Kirstie Whitaker}). For a particular finding in
a paper, if we take the same data, do the same analysis, and get the
same result, we call that finding \textbf{reproducible} (sometimes,
\textbf{analytically} or \textbf{computationally reproducible}). If we
collect \emph{new} data using the same methods, do the same analysis,
and get the same result, we call that a \textbf{replication} and say
that the finding is \textbf{replicable}. If we do a different analysis
with the same data, we call this a \textbf{robustness check} and if we
get a similar result, we say that the finding is
\textbf{robust}.\sidenote{\footnotesize You might have observed that a lot of work is
  being done here by the word ``same.'' How do we operationalize
  same-ness for experimental procedures, statistical analyses, samples,
  or results? These are difficult questions that we'll touch on below.
  Keep in mind that there's no single answer and so these terms are
  always going to helpful guides rather than exact labels.} We leave the
last quadrant empty because there's no specific term for it in the
literature -- the eventual goal is to draw \textbf{generalizable}
conclusions but this term means more than just having a finding that is
reproducible and replicable.

In this chapter, we'll primarily discuss reproducibility and
replicability; discussions of robustness and generalizability will be
taken up in Chapters \textbf{?@sec-prereg} and \textbf{?@sec-sampling}
respectively. We'll start out by reviewing key concepts around
reproducibility and replicability as well as some important meta-science
findings. This literature suggests that when you read an average
psychology paper, your default expectation should be that it might not
replicate!

We'll then discuss some of the main reasons \emph{why} findings might
not replicate -- especially \textbf{analytic flexibility} and
\textbf{publication bias}. We end by taking up the issue of how
reproducibility and replicability relate to theory building in
psychology. To summarize, our view is that reproducibility and
replicability are critical foundations for theory building -- they are
\emph{necessary} but not \emph{sufficient} for good theories.

\begin{tcolorbox}[colframe=.blue, title=\faMicroscope \enspace Case study]

\hypertarget{the-open-science-collaboration}{%
\section*{The Open Science
Collaboration}\label{the-open-science-collaboration}}
\addcontentsline{toc}{section}{The Open Science Collaboration}

\markright{The Open Science Collaboration}

Around 2011, we were teaching our Experimental Methods course for the
first time, based on a course model that we had worked on with Rebecca
Saxe (\protect\hyperlink{ref-frank2012}{Frank and Saxe 2012}). The idea
was to introduce students to the nuts and bolts of research by having
them run replications. A guy named Brian Nosek was on sabbatical nearby,
and over coffee we learned that he was starting up an ambitious project
to replicate a large sample of studies published in top psychology
journals in 2008.

In the course that year we chose replication projects from the sample
that Nosek had told us about. Four of these projects were executed very
well and were nominated by the course TAs for inclusion in the broader
project. A few years later, when the final group of 100 replication
studies was completed, we got a look at the results, shown in
Figure~\ref{fig-replication-osc-2015}.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{images/replication/osc-2015.png}

}

\caption{\label{fig-replication-osc-2015}Results from the Open Science
Collaboration (\protect\hyperlink{ref-osc2015}{2015}). Each point
represents one of the studies in the sample, with the horizontal
position giving the original effect size and the vertical position
giving the replication effect size. Dot size represents estimated
statistical power. The dotted line represents a perfect replication.}

\end{figure}

The resulting meta-science paper, which we and others refer to as the
``replication project in psychology'' (RPP), made a substantial
impression on both psychologists and the broader research community,
defining both a field of psychology meta-science studies and providing a
template for many-author collaborative projects
(\protect\hyperlink{ref-osc2015}{Open Science Collaboration 2015}). But
the most striking thing was the result: disappointingly, only around a
third of the replications had similar findings to the original studies.
The others yielded smaller effects that were not statistically
significant in the replication sample (almost all of the original
studies were significant). RPP provided the first large-scale evidence
that there were systematic issues with replicability in the psychology
literature.

RPP's results -- and their interpretation -- were controversial,
however, and much ink was spilled on what these data showed. In
particular, critics pointed to different degrees of fidelity between the
original studies and the replications; insufficient levels of
statistical power and low evidential value in the replications;
non-representative sampling of the literature; and difficulties
identifying specific statistical outcomes for replication success
(\protect\hyperlink{ref-gilbert2016}{Gilbert et al. 2016};
\protect\hyperlink{ref-anderson2016}{Anderson et al. 2016};
\protect\hyperlink{ref-etz2016}{Etz and Vandekerckhove 2016}). In our
view, many of these critiques have merit, and you can't simply interpret
the results of RPP as an unbiased estimate of the replicability of
results in the literature, contra the title.

And yet, RPP's results are still important and compelling, and they
undeniably changed the direction of the field of psychology. Many good
studies are like this -- they have flaws but they inspire follow up
studies that can address those problems. For several of us personally,
working on this project was also transformative in that it showed us the
power of collaborative work. Together we could do a study that no one of
us had any hope of completing on our own, and potentially make a
difference in our field.

\end{tcolorbox}

\hypertarget{reproducibility}{%
\section{Reproducibility}\label{reproducibility}}

Scientific papers are full of numbers: sample sizes, measurements,
statistical results, and visualizations. For those numbers to have
meaning, and for other scientists to be able to verify them, we need to
know where they came from (their \textbf{provenance}). The chain of
actions that scientists perform on the raw data, all the way through to
reporting numbers in their papers, is sometimes called the
\emph{analysis pipeline}. For much of history, scientific papers have
only provided a verbal, description of the analysis pipeline, usually
with little detail.\sidenote{\footnotesize The situation is nicely summed up by a
  prescient quote from Buckheit and Donoho
  (\protect\hyperlink{ref-buckheit1995}{1995}): ``\ldots{} a scientific
  publication is not the scholarship itself, it is merely advertising of
  the scholarship. The actual scholarship is the complete software
  development environment and the complete set of instructions which
  generated the figures.''}

Moreover, researchers typically do not share key research objects from
this pipeline, such as the analysis scripts or the raw data
(\protect\hyperlink{ref-hardwicke2021c}{Hardwicke, Thibault, et al.
2021}).\sidenote{\footnotesize For many years, professional societies, like the
  American Psychological Association, have mandated data sharing
  (\url{https://www.apa.org/ethics/code}), but only for purposes of
  verification, and only ``on request'' -- in other words, scientists
  could keep data hidden by default and it was their responsibility to
  share if another scientist requested access. In practice, this kind of
  policy does not work; data are rarely made available on request
  (\protect\hyperlink{ref-wicherts2006}{Wicherts et al. 2006}). We
  believe this situation is untenable. We provide a longer argument
  justifying data sharing in \textbf{?@sec-ethics} and discuss some of
  the practicalities of sharing in \textbf{?@sec-management}.} Without
code and data, the numbers reported in scientific papers are often not
reproducible -- an independent scientist cannot repeat all of the steps
in the analysis pipeline and get the same results as the original
scientists.

Reproducibility is desirable for a number of reasons. Without it:

\begin{itemize}
\tightlist
\item
  Errors in calculation or reporting could lead to disparities between
  the reported result and the actual result,
\item
  Vague verbal descriptions of analytic computations could keep readers
  from understanding the computations that were actually performed,
\item
  The robustness of data analyses to alternative model specifications
  cannot be checked, and
\item
  Synthesizing evidence across studies, a key part of building a
  cumulative body of scientific knowledge (\textbf{?@sec-meta}), is much
  more difficult.
\end{itemize}

From this list, error detection and correction is probably the most
pressing issue. But are errors common? There are plenty of individual
instances of errors that are corrected in the published literature
(e.g., some of us found an error in
\protect\hyperlink{ref-cesana-arlotti2018}{Cesana-Arlotti et al. 2018}),
and we ourselves have made significant analytic errors (e.g.,
\protect\hyperlink{ref-frank2013}{Frank et al. 2013}). But these kinds
of experiences don't tell us about the frequency of errors more
generally (or the consequences of error for the conclusions that
researchers draw).\sidenote{\footnotesize There is a very interesting discussion of
  the pernicious role of scientific error on theory building in Gould,
  Gold, et al. (\protect\hyperlink{ref-gould1996}{1996})'s ``The
  Mismeasure of Man.'' Gould examines research on racial differences in
  intelligence and documents how scientific errors that supported racial
  differences were often overlooked. Errors are often caught
  asymmetrically; we are more motivated to double-check a result that
  contradicts our biases.}

Estimating the frequency of errors is a meta-scientific issue that
researchers have attempted to answer over the years. If errors are
frequent, that would suggest a need for changes in our policies and
practices to reduce their frequency! Unfortunately, the lack of data
availability creates a problem: it's hard to figure out if calculations
are wrong if you can't check them in the first place. Here's one clever
approach to this issue. In standard American Psychological Association
(APA) reporting format, inferential statistics must be reported with
three pieces of information: the test statistic, the degrees of freedom
for the test, and the \(p\)-value (e.g., \(t(18) = -0.74\),
\(p = 0.47\)). Yet these pieces of information are redundant with one
another. Thus, reported statistics can be checked for consistency simply
by evaluating whether they line up with one another -- that is, whether
the \(p\)-value recomputed from the \(t\) and degrees of freedom matches
the reported value.

Bakker and Wicherts (\protect\hyperlink{ref-bakker2011}{2011}) performed
this kind of statistical consistency analysis on a sample of 281 papers,
and found that around 18\% of statistical results were incorrectly
reported. Even more worrisome, around 15\% of articles contained at
least one decision error -- that is, a case where the error changed the
direction of the inference that was made (e.g., from significant to
insignificant).\sidenote{\footnotesize Confirming Gould's speculation (see note
  above), most of the reporting errors that led to decision errors were
  in line with the researchers' own hypotheses.} Nuijten et al.
(\protect\hyperlink{ref-nuijten2016}{2016}) used an automated method
called ``statcheck''\sidenote{\footnotesize Statcheck is now available as a web app
  (\url{http://statcheck.io}) and an R package so that you can check
  your own manuscripts!} to confirm and extend this analysis. They
checked \(p\)-values for more than 250,000 psychology papers in the
period 1985--2013 and found that around half of all papers contained at
least one incorrect \(p\)-value!

These findings provide a lower bound on the number of errors in the
literature and suggest that reproducibility of analyses is likely very
important. However, they only address the consistency of statistical
reporting. What would happen if we tried to repeat the entire analysis
pipeline from start to finish? It's rather difficult to answer this
question at a large scale: firstly, it takes a long time to run a
reproducibility check; and secondly, the lack of access to raw data
means that for most scientific papers, checking reproducibility is
impossible.

Nevertheless, a few years ago a group of us spotted an opportunity to
check reproducibility by examining studies published in two journals
that either required or encouraged data sharing. Hardwicke et al.
(\protect\hyperlink{ref-hardwicke2018b}{2018}) and Hardwicke, Bohn, et
al. (\protect\hyperlink{ref-hardwicke2021a}{2021}) first identified
studies that shared data, then narrowed those down to studies that
shared \emph{reusable} data (the data were accessible, complete, and
comprehensible). For 60 of these articles, we then attempted to
reproduce numerical values related to a particular statistical result in
the paper. The process was incredibly labor-intensive, with articles
typically requiring 5--10 hours of work each. And the results were
concerning: the targeted values in only about a third of articles were
completely reproducible without help from the original authors! In many
cases, after -- sometimes extensive -- correspondence with the original
authors, they provided additional information that was not reported in
the original paper. After author contact, the reproducibility success
rate improved to 62\% (Figure~\ref{fig-replication-hardwicke}). The
remaining papers appeared to have some values that neither we, nor the
original authors, could reproduce. Importantly, we didn't identify any
patterns of non-reproducibility that seriously undermined the
conclusions drawn in the original articles; however, other
reproducibility studies have found a distressingly high number of
decision errors (\protect\hyperlink{ref-artner2020}{Artner et al.
2020}), albeit with a slightly higher success rate overall.

\begin{figure}

\sidecaption{\label{fig-replication-hardwicke}Analytic reproducibility
of results from open-data articles in \emph{Cognition} and
\emph{Psychological Science}. From Hardwicke, Bohn, et al.
(\protect\hyperlink{ref-hardwicke2021a}{2021}).}

{\centering \includegraphics{images/replication/hardwicke2021.png}

}

\end{figure}

In sum: transparency is a critical imperative for decreasing the
frequency of errors in the published literature. Reporting and
computation errors are frequent in the published literature, and the
identification of these errors depends on the findings being
reproducible. If data are not available, then errors usually cannot be
found.

\hypertarget{replication}{%
\section{Replication}\label{replication}}

Beyond verifying a paper's original analysis pipeline, we are often
interested in understanding whether the study can be replicated -- if we
repeat the study methods and obtain new data, do we get similar results?
To quote from Popper (\protect\hyperlink{ref-popper2005}{2005}), ``the
scientifically significant\ldots{} effect may be defined as that which
can be regularly {[}replicated{]} by anyone who carries out the
appropriate experiment in the way prescribed.''

Replications can be conducted for many reasons
(\protect\hyperlink{ref-schmidt2009}{Schmidt 2009}). One goal can be to
verify that the results of an existing study can be obtained again if
the study is conducted again in exactly the same way, to the best of our
abilities. A second goal can be to gain a more precise estimate of the
effect of interest by conducting a larger replication study, or
combining the results of a replication study with the existing study. A
third goal can be to investigate whether an effect will persist when,
for example, the experimental manipulation is done in a different, but
still theory-consistent, manner. Alternatively, we might want to
investigate whether the effect persists in a different population. Such
replications are often efforts to ``replicate and extend,'' and are
common both when the same research team wants to conduct a sequence of
experiments that each build on one another or when a new team wants to
build on a result from a paper they have read
(\protect\hyperlink{ref-rosenthal1990}{Rosenthal 1990}).

Much of the meta-science literature (and attendant debate and
discussion) has focused on the first goal -- simple verification. This
focus has been so intense that the term ``replication'' has become
associated with skepticism or even attacks on the foundations of the
field. This dynamic is at odds with the role that replication is given
in a lot of philosophy of science, where it is assumed to be a typical
part of ``normal science.''

\hypertarget{conceptual-frameworks-for-replication}{%
\subsection{Conceptual frameworks for
replication}\label{conceptual-frameworks-for-replication}}

The key challenge of replication is \textbf{invariance} -- Popper's
stipulation that a replication be conducted ``in the way prescribed'' in
the quote above. That is, what are the features of the world over which
a particular observation should be relatively constant, and what are
those that are specified as the key ingredients for the effect?
Replication is relatively straightforward in the physical and biological
sciences, in part because of presupposed theoretical background that
allows us to make strong inferences about invariance. If a biologist
reports an observation about a particular cell type from an organism,
the color of the microscope is presumed not to matter to the
observation.

These invariances are far harder to state in psychology, for both the
procedure of an experiment and its sample. Procedurally, should the
color of the experimental stimulus matter to the measured effect? In
some cases yes, in some cases no.\sidenote{\footnotesize A fascinating study by
  Baribault et al. (\protect\hyperlink{ref-baribault2018}{2018})
  proposes a method for empirically understanding psychological
  invariances. Treating a subliminal priming effect as their model
  system, they sampled thousands of ``micro-experiments'' in which small
  parameters of their experimental procedure were randomly sampled.
  These parameters allowed for measurement of their effect of interest,
  averaging across this irrelevant variation. In their case, it turned
  out that color did not matter.} Yet the task of postulating how a
scientific effect should be invariant to lab procedures pales in
comparison to the task of postulating how the effect should be invariant
across different human populations!\sidenote{\footnotesize In some sense, the research
  program of some branches of the social sciences amounts to an
  understanding of invariances across human cognition.}

A lot is at stake in this discussion. If Dr.~Frog publishes a finding
with US undergraduates and Dr.~Toad then ``replicates'' the procedure in
Germany, to what extent should we be perturbed if the effect is
different in magnitude or absent?\sidenote{\footnotesize Presumably not very much if
  Dr.~Toad gave the original instructions in English instead of in
  German -- that's another one of these pesky invariances that we are
  always worrying about!} Meta-researchers have made a number of
replication taxonomies to try and quantify the degree of methodological
consistency between two experiments.

One influential taxonomy distinguishes between \textbf{direct
replications}\sidenote{\footnotesize These also get called \textbf{exact replications}
  sometimes. We think this term is misleading because similarity between
  two different experiments is always going to be on a gradient, and
  where you cut this continuum is always going to be a theory-laden
  decision. One person's ``exact'' is another's ``inexact.''} and
\textbf{conceptual replications}
(\protect\hyperlink{ref-zwaan2018}{Zwaan et al. 2018}). Direct
replications are those that attempt to reproduce all of the salient
features of the prior study, up to whatever invariances the
experimenters believe are present (e.g., color of the paint, gender of
the experimenter, etc.). In contrast, conceptual replications are
typically paradigms that attempt to test the same hypothesis via
different operationalizations of the manipulation and/or the measure. We
agree with Zwaan et al.~(2018): labeling this second type of experiment
as a ``replication'' is a little misleading. Rather, so-called
``conceptual replications'' are alternative tests of the same part of
your theory. Such tests can be extremely valuable, but they serve a
different goal than replication.

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{small-telescopes}{%
\section*{``Small Telescopes''}\label{small-telescopes}}
\addcontentsline{toc}{section}{``Small Telescopes''}

\markright{``Small Telescopes''}

We've been discussing the question of invariance with respect to
procedure and sample, but we haven't really discussed invariance with
respect to the studies' statistical results. To what extent can we
consider two statistical results to be ``the same''? Several obvious
metrics, including those used by RPP, have important limitations
(\protect\hyperlink{ref-simonsohn2015}{Simonsohn 2015}). For example, if
one finding is statistically significant and the other isn't, they still
could have effect sizes that are actually quite close to one another, in
part because one might have a larger sample size than the other. Or you
could have two significant findings that nevertheless have very
different effect sizes.

\begin{figure}[H]

{\centering \includegraphics[width=0.6\textwidth,height=\textheight]{images/replication/telescopes.png}

}

\caption{\label{fig-replication-telescopes}The original finding by
Schwarz and Clore (\protect\hyperlink{ref-schwarz1983}{1983}) and two
replications with much larger samples. All three estimates include a
95\% confidence interval, but the confidence intervals are very small
for the two replication studies. The green dashed line shows the
smallest effect that the origial study could reasonably have detected.
From Simonsohn (\protect\hyperlink{ref-simonsohn2015}{2015}).}

\end{figure}

In a classic study, Schwarz and Clore
(\protect\hyperlink{ref-schwarz1983}{1983}) reported that participants
(N=28) rated their life satisfaction as higher on sunny days than rainy
days, suggesting that they mis-attributed temporary happiness about the
weather to longer-term life satisfaction. However, when two more recent
studies examined very large samples of survey responses, they yielded
estimates of the effect that were much smaller. (All of these effects
have been standardized so they are on the same scale using a metric
called Cohen's \(d\) that we will introduce more formally in
\textbf{?@sec-estimation}). In one survey, the effect was statistically
significant but extremely small; in the other it was essentially zero
(Figure~\ref{fig-replication-telescopes}). Using statistical
significance as the metric of replication success, you might be tempted
to say that the first of these studies was a successful replication and
the second was a failed replication.

Simonsohn points out that this interpretation doesn't make sense, using
the analogy of a study's sample size as a telescope. Following this
analogy, Schwarz and Clore had a very small telescope (i.e., a small
sample size), and they pointed it in a particular direction and claimed
to have observed a planet (i.e., a nonzero effect). Now it might turn
out that there \emph{was} a planet at that location when you look with a
much larger telescope (first replication), and it might turn out that
there \emph{wasn't} (second replication). Regardless, however, the
original small telescope was simply not powerful enough to have seen
whatever was there. Both studies fail to replicate the original
observation, regardless of whether their observed effect was in the same
direction.

Following Simonsohn's example, numerous metrics for replication success
have been proposed (\protect\hyperlink{ref-mathur2020}{Mathur and
VanderWeele 2020}). The best of these move away from the idea that there
is a binary test of whether an individual replication was successful and
towards a comparison of the two effects and whether they appear
consistent with the same theory. Gelman
(\protect\hyperlink{ref-gelman2018}{2018}) suggests the ``time
reversal'' heuristic -- rather than thinking of a replication as a
success or a failure, consider the alternative world in which the
replication study had been performed first and the original study
followed it. What would we say then? If we leave behind the idea that
the original study has precedence, it makes much more sense to consider
the sum total of the evidence across the two. Do they agree or disagree?
Taken together, do they support the presence of the effect, or do they
present a strong case that it's present only under certain conditions?
Using this approach, it seems pretty clear that the weather
mis-attribution effect is, at best, a tiny factor in people's overall
judgments of their life satisfaction.

\end{tcolorbox}

\hypertarget{the-meta-science-of-replication}{%
\subsection{The meta-science of
replication}\label{the-meta-science-of-replication}}

In RPP, replication teams reported subjectively that 39\% of
replications were successful, with 36\% reporting a significant effect
in the same direction as the original. How generalizable is this
estimate -- and how replicable \emph{is} psychological research more
broadly? Based on the discussion above, we hope we've made you skeptical
that this is a well-posed question, at least without a lot of additional
qualifiers. Any answer is going to have to provide details about the
scope of this claim, the definition of replication being used, and the
metric for replication success. On the other hand, \emph{versions} of
this question have led to a number of empirical studies that help us
better understand the scope of replication issues.

Many subsequent empirical studies of replication have focused on
particular subfields or journals, with the goal of informing particular
field-specific practices or questions. For example, Camerer et al.
(\protect\hyperlink{ref-camerer2016}{2016}) replicated all of the
between-subject laboratory articles published in two top economics
journals in the period 2011--2014. They found a replication rate of 61\%
of significant effects in the same direction of the original, higher
than the rate in RPP but lower than the naive expectation based on their
level of statistical power. Another study attempted to replicate all 21
behavioral experiments published in the journals \emph{Science} and
\emph{Nature} from 2010--2015, finding a replication rate of 62\%
significant effects (\protect\hyperlink{ref-camerer2018}{Camerer et al.
2018}).\sidenote{\footnotesize This study was notable because they followed a two-step
  procedure -- after an initial round of replications, they followed up
  on the failures by consulting with the original authors and pursuing
  extremely large sample sizes. The resulting estimate thus is less
  subject to many of the critiques of the original RPP paper.} While
these types of studies do not answer all the questions that were raised
about RPP, they suggest that replication rates for top experiments are
not as high as we'd like them to be, even when care is taken with the
sampling and individual study protocols.

Other scientists working in the same field can often predict when an
experiment will fail to replicate. Dreber et al.
(\protect\hyperlink{ref-dreber2015}{2015}) showed that prediction
markets (where participants bet small sums of real money on replication
outcomes) made fairly accurate estimates of replication success in the
aggregate. This result has itself now been replicated several times
(e.g., in the Camerer et al., 2018 study described earlier). Maybe even
more surprisingly, there's some evidence that machine learning models
trained on the text of papers can predict replication success
(\protect\hyperlink{ref-yang2020}{Yang, Youyou, and Uzzi 2020};
\protect\hyperlink{ref-youyou2023}{Youyou, Yang, and Uzzi 2023}), though
more work still needs to be done to validate these models and understand
the features they use. More generally, these two lines of research
suggest the possibility of isolating consistent factors that lead to
replication success or failure. (In the next section we consider what
these factors are in more depth.)

Although more work still needs to be done to get generalizable estimates
of replicability, taken together, the meta-science literature does
provide some clarity on what we should expect. Altogether, the chance of
a significant finding in a (well-powered) replication study of a generic
experiment in social and cognitive psychology is likely somewhere around
56\%. Furthermore, the replication effect will likely be on average 53\%
as large (\protect\hyperlink{ref-nosek2021}{Nosek et al. 2021}).

On the other hand, these large-scale replication studies have
substantial limitations as well. With relatively few exceptions, the
studies chosen for replication used short, computerized tasks that
mostly would fall into the categories of social and cognitive
psychology. Further, and perhaps most troubling from the perspective of
theory development, they tell us only whether a particular experimental
effect can be replicated. They tell us much less about whether the
construct that the effect was meant to operationalize is in fact real!
We'll return to the difficult issue of how replication and theory
construction relate to one another in the final section of this chapter.

Some have called the narrative that emerges from the sum of these
meta-science studies the ``replication crisis.'' We think of it as a
major tempering of expectations with respect to the published
literature. Your naive expectation might reasonably be that you could
read a typical journal article, select an experiment from it, and
replicate that experiment in your own research. The upshot of this
literature is, unfortunately, if you try selecting and replicating an
exeriment, you might well be disappointed by the result.

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{consequences-for-the-study-consequences-for-the-person}{%
\section*{Consequences for the study, consequences for the
person}\label{consequences-for-the-study-consequences-for-the-person}}
\addcontentsline{toc}{section}{Consequences for the study, consequences
for the person}

\markright{Consequences for the study, consequences for the person}

``Power posing'' is the idea that adopting a more open and expansive
physical posture might also change your confidence. Carney, Cuddy, and
Yap (\protect\hyperlink{ref-carney2010}{2010}) told 42 participants that
they were taking part in a study of physiological recording. They then
held two poses, each for a minute. In one condition, the poses were
expansive (e.g., legs out, hands on head); in another condition, the
poses were contractive (e.g., arms and legs crossed). Participants in
the expansive pose condition showed increases in testosterone and
decreases in salivary cortisol (a stress marker), they took a greater
number of risk in a gambling task, and they reported that they were more
``in charge'' in a survey. This result suggested that a two-minute
manipulation could lead to striking physiological and psychological
changes -- in turn leading to power posing becoming firmly enshrined as
part of the set of recommended strategies in business and elsewhere. The
original publication contributed to the rise of the researchers'
careers, including becoming a principal piece of evidence in a
hugely-popular TED talk by Amy Cuddy, one of the authors.

Followup work has questioned these findings, however. A replication
study with a larger number of participants (N=200) failed to find
evidence for physiological effects of power-posing, even as it did find
some effects on participants' own beliefs
(\protect\hyperlink{ref-ranehill2015}{Ranehill et al. 2015}). And a
review of the published literature suggested that many findings appeared
to be the result of some sort of publication bias, as far too many of
them had \emph{p}-values very close to the .05 threshold
(\protect\hyperlink{ref-simmons2017}{Simmons and Simonsohn 2017}). In
light of this evidence, the first author of the replication study
bravely made a public statement that she
\href{https://faculty.haas.berkeley.edu/dana_carney/pdf_my\%20position\%20on\%20power\%20poses.pdf}{does
not believe that ``power pose'' effects are real}.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{images/replication/powerpose.png}

}

\caption{\label{fig-replication-powerpose}Google trends time series for
``power pose'' from 2004-2021.}

\end{figure}

From the scientific perspective, it's very tempting to take this example
as a case in which the scientific ecosystem corrects itself. Although
many people continue to cite the original power posing work, we suspect
the issues are well-known throughout the social psychology community,
and overall interest from the lay public has gone down (see
Figure~\ref{fig-replication-powerpose}). But this narrative masks the
very real human impacts of the self-correction process, which can raise
ethical questions about the best way to address issues in the scientific
record.

The process of debate and discussion around individual findings can be
bruising and complicated. In the case of power posing, Cuddy herself was
tightly associated with the findings and many critiques of the findings
became critiques of the individual. Several commentators used Cuddy's
name as a stand-in for low-quality psychological results, likely because
of her prominence and perhaps because of her gender and age as well.
These comments were harmful to Cuddy personally and her career more
generally.

Scientists should critique, reproduce, and replicate results -- these
are all parts of the progress of normal science. But it's important to
do this in a way that's sensitive to the people involved. Here are a few
guidelines for courteous and ethical conduct:

\begin{itemize}
\tightlist
\item
  Always communicate about the work, never the person. Try to use
  language that is specific to the analysis or design being critiqued,
  rather than the person who did the analysis or thought up the design.
\item
  Avoid using language that assumes negative intentions, e.g.~``the
  authors misleadingly state that \ldots{}''
\item
  Ask someone to read your paper, email, blogpost, or tweet before you
  hit send. It can be very difficult to predict how someone else will
  experience the tone of your writing; a reader can help you make this
  judgement.
\item
  Consider communicating personally before communicating publicly. As
  Joe Simmons, one critic in the power-posing debate said, ``I wish I'd
  had the presence of mind to pick up the phone and call {[}before
  publishing my critique{]}''
  (\protect\hyperlink{ref-dominus2017}{Dominus 2017}). Personal
  communication isn't always necessary (and can be difficult due to
  asymmetries of power or status), but it can be helpful.
\end{itemize}

As we will argue in the next chapter, we have an ethical duty as
scientists to promote good science and critique low quality science. But
we also have a duty to our colleagues and communities to be good to one
another.

\end{tcolorbox}

\hypertarget{causes-of-replication-failure}{%
\section{Causes of replication
failure}\label{causes-of-replication-failure}}

The general argument of this chapter is that everything is not all right
in experimental psychology, and hence that we need to change our
methodological practices to avoid negative outcomes like irreproducible
papers and unreplicable results. Towards that goal, we have been
presenting meta-scientific evidence on reproducibility and
replicability. But this evidence has been controversial, to say the
least! Do large-scale replication studies like RPP -- or for that
matter, smaller-scale individual replications of effects like ``power
posing'' -- really lead to the conclusion that our methods require
changes? Or are there reasons why a lower replication rate is actually
consistent with a cumulative, positive vision of psychology?\sidenote{\footnotesize One
  line of argument addresses this question through the dynamics of
  scientific change. There are many versions, but one is given by
  Wilson, Harris, and Wixted (\protect\hyperlink{ref-wilson2020}{2020}).
  The idea is that progress in psychology consists of a two-step process
  by which candidate ideas are ``screened'' by virtue of small, noisy
  experiments that reveal promising but tentative ideas that can then be
  ``confirmed'' by large-scale replications. On this kind of view, it's
  business as usual to find that many randomly-selected findings don't
  hold up in large-scale replications and so we shouldn't be distressed
  by results like those of RPP. The key to progress is to finding a
  small set that \emph{do} hold up, which will lead to new areas of
  inquiry. We're not sure this is view is either a good description of
  current practice or a good normative goal for scientific progress, but
  we won't focus on that critique of Wilson et al.'s argument here.
  Instead, since book is written for experimenters-in-training, we
  assume that \emph{you} do not want your experiment to be a false
  positive from a noisy screening procedure, regardless of your feelings
  about the rest of the literature!}

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{context-moderators-and-expertise}{%
\section*{Context, moderators, and
expertise}\label{context-moderators-and-expertise}}
\addcontentsline{toc}{section}{Context, moderators, and expertise}

\markright{Context, moderators, and expertise}

There are many explanations for failed replications. The wonderful thing
about meta-science is that these explanations can be tested empirically!

Let's start with the idea that specific experimental operationalizations
of a theory might be ``context sensitive,'' especially in subfields,
like social psychology, whose theories inherently refer to environmental
context (\protect\hyperlink{ref-van-bavel2016}{Van Bavel et al. 2016}).
Critics brought this issue up for RPP, where there were several studies
in which the original experimental materials were tailored to one
cultural context but then were deployed in another context, potentially
leading to failure due to mismatch
(\protect\hyperlink{ref-gilbert2016}{Gilbert et al. 2016}).

Context sensitivity seems like a great explanation because in some
sense, it \emph{must} be right. If the context of an experiment includes
the vast network of learned associations, practices, and beliefs that we
all hold, then there's no question that an experiment's materials tap
into this context to one degree or another. For example, if your
experiment relies on the association between \emph{doctor} and
\emph{nurse} concepts, you would expect this experiment to work
differently in the past when \emph{nurse} meant something more like
\emph{nanny} (\protect\hyperlink{ref-ramscar2016}{Ramscar 2016}).

On the other hand, as an explanation of specific replication failures,
context sensitivity has not fared very well. The ``Many Labs'' projects
were a series of replication projects in which \emph{multiple} labs
independently attempted to replicate several original studies. (In
contrast, in RPP and similar studies, a single replication was conducted
for each original study.) Some of the Many Labs projects assessed
variation in replication success across different labs. In ManyLabs 2,
Klein et al. (\protect\hyperlink{ref-klein2018b}{2018}) replicated 28
findings, distributed across 125 different samples and more than 15,000
participants. ManyLabs 2 found almost no support for the context
sensitivity hypothesis as an explanation of replication failure. In
general, when effects failed to replicate, they did so when conducted in
person as well as when conducted online, and these failures were
consistent across many cultures and labs.

On the other hand, a review of several Many Labs-style replication
projects indicated, on re-analysis, that population effects differed
across replication labs even when the replication protocols were very
similar to one another
(\protect\hyperlink{ref-olsson2020heterogeneity}{Olsson-Collentine,
Wicherts, and Assen 2020};
\protect\hyperlink{ref-errington2021investigating}{Errington et al.
2021}). So context sensitivity is almost certainly present -- and we'll
return to the broader issues of generalizability, context, and
invariance in the next section -- but so far we have not identified
specific forms of context sensitivity that reliably affect replication
success.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{images/replication/lewis-stims.png}

}

\caption{\label{fig-replication-lf}Stimuli from Lewis and Frank
(\protect\hyperlink{ref-lewis2016}{2016}).}

\end{figure}

These observations -- that 1) direct replications vary in how successful
they are, but 2) we cannot identify specific contextual moderators --
together suggest the possible presence of ``hidden moderators.'' That
is, when faced with a successful original study and a failed
replication, there may be some unknown factor(s) that moderates the
effect. We've personally had several experiences that corroborate this.
For example, in Lewis and Frank
(\protect\hyperlink{ref-lewis2016}{2016}), we were unsuccessful in
replicating a simple categorization experiment. We then made a series of
iterative changes to the stimuli and instructions, for example changing
the color and pattern of the stimuli (Figure~\ref{fig-replication-lf}),
eventually resulting in a larger (and statistically significant) effect
-- though still much smaller than the original. Critically, however,
each alteration that we made to the procedure yielded a very small
change in the effect, and it would have taken us many thousands of
participants to figure exactly which alteration made the difference. (If
you're keeping score, here's a case where stimulus color \emph{did}
matter to the outcome of the experiment!.

Another explanation for replication failure that is often cited is
experimenter expertise (e.g.,
\protect\hyperlink{ref-schwarz2014}{Schwarz and Strack 2014}). On this
hypothesis, replications fail because the researchers performing the
replication do not have sufficient expertise to execute the study. Like
context sensitivity, this explanation is almost certainly true for some
replications. In our own work, we have repeatedly performed experiments
that failed due to our own incompetence!

Yet as an explanation of the pattern of meta-science findings, the
expertise hypothesis hasn't been supported empirically. First, team
expertise was not a predictor of replication success in RPP (cf.
\protect\hyperlink{ref-bench2017}{Bench et al. 2017}). More
convincingly, Many Labs 5 selected ten findings from RPP with
unsuccessful replications and systematically evaluated whether formal
expert peer review of the protocols, including by the authors of the
original study, would lead to a larger effect sizes. Despite a massive
sample size and extremely thorough review process, there was little to
no change in the effects for the vetted protocols relative to the
original protocol used in RPP
(\protect\hyperlink{ref-ebersole2020}{Ebersole et al. 2020}).

Context, moderators, and expertise seem like reasonable explanations for
individual replication failures. Certainly, we should expect them to be
explanatory! But for these hypotheses to be operationalized in such a
way that they carry weight in our evaluation of the meta-scientific
evidence, they must be evaluated empirically rather than accepted
uncritically. When such evaluations have been carried out, they have
failed to support a large role for these factors.

\end{tcolorbox}

In RPP and subsequent meta-science studies, original studies with lower
\(p\)-values, larger effect sizes, and larger sample sizes were more
likely to replicate successfully (\protect\hyperlink{ref-yang2020}{Yang,
Youyou, and Uzzi 2020}). From a theoretical perspective, this result is
to be expected, because the \(p\)-value literally captures the
probability of the data (or any ``more extreme'') under the null
hypothesis of no effect. So a lower \(p\)-value should indicate a lower
probability of a spurious result.\sidenote{\footnotesize In \textbf{?@sec-inference}
  we will have a lot more to say about \(p < .05\) but for now we'll
  mostly just treat it as a particular research outcome.} In some sense,
the fundamental question about the replication meta-science literature
is why the \(p\)-values \emph{aren't} better predictors of
replicability! For example, Camerer et al.
(\protect\hyperlink{ref-camerer2018}{2018}) computes an expected number
of successful replications on the basis of the effects and sample sizes
-- and their proportion of successful replications is substantially
lower than that number.\sidenote{\footnotesize This calculation, as with most other
  metrics of replication success, assumes that the underlying population
  effect is exactly the same for the replication and the original. This
  is a limitation because there could be unmeasured moderators that
  could produce genuine substantive differences between the two
  estimates.}

One explanation is that the statistical evidence that is presented in
papers often dramatically overstates the true evidence from a study.
That's because of two pervasive and critical issues: \textbf{analytic
flexibility} (also known as \textbf{p-hacking} or \textbf{questionable
research practices}) and \textbf{publication bias}.\sidenote{\footnotesize These terms
  basically mean the same thing and are not used very precisely in the
  literature. \(p\)-hacking is an informal term that sounds like you
  know you are doing something bad; sometimes people do, and sometimes
  they don't. Questionable research practices is a more formal-sounding
  term that is in principle vague enough to encompass many ethical
  failings but in practice gets used to talk about \(p\)-hacking. Unless
  \(p\)-hacking intent is crystal clear, we favor two clunkier terms:
  ``data-dependent decision-making'' and ``undisclosed analytic
  flexibility'' describe the actual practices more precisely: trying
  many different things after looking at data, typically without
  reporting all of them.}

Publication bias refers to the relative preference (of scientists and
other stakeholders, like journals) for experiments that ``work'' than
those that do not, where ``work'' is typically defined as yielding a
significant result at \(p<.05\). Because of this preference, it is
typically easier to publish positive (statistically significant)
results. The relative absence of negative results leads to biases in the
literature. Intuitively, this bias will lead to a literature filled with
papers where \(p<.05\). Negative findings will then remain unpublished,
living in the proverbial ``file drawer''
(\protect\hyperlink{ref-rosenthal1979}{Rosenthal 1979}).\sidenote{\footnotesize One
  estimate is that 96\% of (non-preregistered) papers report positive
  findings (\protect\hyperlink{ref-scheel2021}{Scheel, Schijen, and
  Lakens 2021})! We'll have a lot more to say about analytic flexibility
  and publication bias in Chapters \textbf{?@sec-prereg} and
  \textbf{?@sec-meta}, respectively.} In a literature with a high degree
of publication bias, many findings will be spurious because
experimenters got lucky and published the study that ``worked'' even if
that success was due to chance variation. In this situation, these
spurious findings will not be replicable and so the overall rate of
replicability in the literature will be lowered.\sidenote{\footnotesize The
  mathematics of the publication bias scenario strikes some observers as
  implausible: most psychologists don't run dozens of studies and report
  only one out of each group (\protect\hyperlink{ref-nelson2018}{Nelson,
  Simmons, and Simonsohn 2018}). Instead, a more common scenario is to
  conduct many different analyses and then report the most successful,
  creating some of the same effects as publication bias -- a promotion
  of spurious variation -- without a file drawer full of failed studies.}

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{analytic-flexibility-reveals-a-fountain-of-eternal-youth}{%
\section*{Analytic flexibility reveals a fountain of eternal
youth}\label{analytic-flexibility-reveals-a-fountain-of-eternal-youth}}
\addcontentsline{toc}{section}{Analytic flexibility reveals a fountain
of eternal youth}

\markright{Analytic flexibility reveals a fountain of eternal youth}

The way they tell it, Joseph Simmons, Leif Nelson, and Uri Simonsohn
wrote their paper on ``false positive psychology''
(\protect\hyperlink{ref-simmons2011}{Simmons, Nelson, and Simonsohn
2011}) as an attempt at catharsis
(\protect\hyperlink{ref-simmons2018}{Simmons, Nelson, and Simonsohn
2018}). They were fed up with work that they felt exploited flexibility
in data analysis to produce findings blessed with \emph{p} \textless{}
.05 but likely did not reflect replicable effects. They called this
practice \textbf{p-hacking}: trying different things to get your
\emph{p}-value to be below .05.

Their paper reported on a simple experiment: they played participants
either the Beatles song, ``when I'm 64,'' or a control song and then
asked them to report their date of birth
(\protect\hyperlink{ref-simmons2011}{Simmons, Nelson, and Simonsohn
2011}). This manipulation resulted in a significant one and a half year
rejuvenation effect. Listening to the Beatles seemed to have made their
participants younger!

This result is impossible, of course. But the authors produced a
statistically significant difference between the groups that, by
definition, was a \textbf{false positive} -- a case where the
statistical test indicated that there was a difference between groups
despite no difference existing. In essence, they did so by trying many
possible analyses and ``cherry-picking'' the one that produced a
positive result. This practice of course invalidates the inference that
the statistical test is supposed to help you make.

Several of the practices they followed included:

\begin{itemize}
\tightlist
\item
  Selectively reporting dependent measures (e.g., collecting several
  measures and reporting only one),
\item
  Selectively dropping manipulation conditions,
\item
  Conducting their statistical test and then testing extra participants
  in the case that they did not see a significant finding, and
\item
  Adjusting for gender as a covariate in their analysis if doing so
  resulted in a significant effect.
\end{itemize}

Many of the practices that the authors followed in their rejuvenation
study were (and maybe still are!) commonplace in the research
literature. John, Loewenstein, and Prelec
(\protect\hyperlink{ref-john2012}{2012}) surveyed research psychologists
on the prevalence of what they called \textbf{questionable research
practices}. Most participants admitted to following some of these
practices -- including exactly the same practices followed by the
rejuvenation study.

For many in the field, ``false positive psychology'' was a galvanizing
moment, leading them to recognize how common practices could lead to
completely spurious (or even impossible) conclusions. As Simmons,
Nelson, and Simonsohn wrote in their 2018 article, ``Everyone knew
{[}p-hacking{]} was wrong, but they thought it was wrong the way it is
wrong to jaywalk. We decided to write `False-Positive Psychology' when
simulations revealed that it was wrong the way it is wrong to rob a
bank.''

\end{tcolorbox}

It's our view that publication bias and its even more pervasive cousin,
analytic flexibility, are likely to be key drivers of lower
replicability. We admit that the meta-scientific evidence for this
hypothesis isn't unambiguous, but that's because there's no sure-fire
way to diagnose analytic flexibility in a particular paper -- since we
can almost never reconstruct the precise choices that were made in the
data collection and analysis process! On the other hand, it is possible
to analyze indicators of publication bias in specific literatures and
there are several cases where publication bias diagnostics appear to go
hand in hand with replication failure.\sidenote{\footnotesize Here are two examples.
  First, in the ``power posing'' example described above, Simmons and
  Simonsohn (\protect\hyperlink{ref-simmons2017}{2017}) noted strong
  evidence of analytic flexibility throughout the literature, leading
  them to conclude that there was no evidential value in the literature.
  Second, in the case of ``money priming'' (incidental exposures to
  images or text about money that were hypothesized to lead to changes
  in political attitudes), strong evidence of publication bias
  (\protect\hyperlink{ref-vadillo2016}{Vadillo, Hardwicke, and Shanks
  2016}) was accompanied by a string of failed replications
  (\protect\hyperlink{ref-rohrer2015}{Rohrer, Pashler, and Harris
  2015}).}

\hypertarget{replication-reproducibility-theory-building-and-open-science}{%
\section{Replication, reproducibility, theory building, and open
science}\label{replication-reproducibility-theory-building-and-open-science}}

So, empirical measures of reproducibility and replicability in the
experimental psychology literature are low -- lower than we might have
naively suspected and lower than we want. How do we address these
issues? And how do these issues interact with the goal of building
theories? In this last section, we discuss the relationship between
replication and theory -- and the role that open and transparent
research practices can play.

\hypertarget{reciprocity-between-replication-and-theory}{%
\subsection{Reciprocity between replication and
theory}\label{reciprocity-between-replication-and-theory}}

Analytic reproducibility is a prerequisite for theory building because
if the twin goals of theories are to explain and to predict experimental
measurements, then an error-ridden literature undermines this goal. If
some proportion of all numerical values reported in the literature were
simple, unintentional typos, this situation would create an extra level
of noise -- irrelevant random variation -- impeding our goal of getting
precise enough measurements to distinguish between theories. But in
fact, the situation is likely to be worse: errors are much more often in
the direction that favors authors' own hypotheses. Thus,
irreproducibility not only decreases our precision, it also increases
the bias in the literature, creating obstacles to the fair evaluation of
theories with respect to data.

Replicability is also foundational to theory building. Across a wide
range of different conceptions of how science works, scientific theories
are evaluated with respect to their relationship to the world. They must
be supported, or at least fail to be falsified, by specific
observations. It may be that some observations are by their nature
un-repeatable (e.g., a particular astrophysical event might not be
observed again a human lifetime). But for laboratory sciences -- and
experimental psychology can be counted among these, to a certain extent
at least -- the independent and skeptical evaluation of theories
requires repeatability of measurements.

Some authors have argued (following the philosopher Heraclitus), ``you
can't step in the same river twice''
(\protect\hyperlink{ref-mcshane2014}{McShane and Bckenholt 2014}) --
meaning, the circumstances and context of psychological experiments are
constantly changing and no observation will be identical to another.
This is of course technically true from a philosophical perspective. But
that's where theory comes in! As we discussed above, our theories
postulate the invariances that allow us to group together similar
observations and generalize across them.

In this sense, replication is critical to theory, but theory is also
critical to replication. Without a theory of ``what matters'' to a
particular outcome, we really are stepping into an ever-changing river.
But a good theory can concentrate our expectations on a much smaller set
of causal relationships, allowing us to make strong predictions about
what factors should and shouldn't matter to experimental outcomes. To
return to an example we discussed earlier, should stimulus color matter
to the outcome of an experiment? Our theory could tell us that it
shouldn't matter for a priming experiment
(\protect\hyperlink{ref-baribault2018}{Baribault et al. 2018}) but that
it should for a generalization experiment
(\protect\hyperlink{ref-lewis2016}{Lewis and Frank 2016}).

\hypertarget{deciding-when-to-replicate-to-maximize-epistemic-value}{%
\subsection{Deciding when to replicate to maximize epistemic
value}\label{deciding-when-to-replicate-to-maximize-epistemic-value}}

As a scientific community, how much emphasis should we place on
replication? In the words of Newell
(\protect\hyperlink{ref-newell1973}{1973}), ``you can't play 20
questions with nature and win''. A series of well-replicated
measurements does not itself constitute a theory. Theory construction is
its own important activity. We've tried to make the case here that a
reproducible and replicable literature is a critical foundation for
theory building. That doesn't necessarily mean you have to do
replications all the time.

More generally, any scientific community needs to trade off between
exploring new phenomena and confirming previously reported effects. In a
thought-provoking analysis, Oberauer and Lewandowsky
(\protect\hyperlink{ref-oberauer2019}{2019}) suggest that perhaps
replications also aren't the best test of theoretical hypotheses. In
their analysis, if you don't have a theory then it makes sense to try
and discover new phenomena and then to replicate them. If you \emph{do}
have a theory, you should expend your energy in testing new predictions
rather than repeating the same test across multiple replications.
Analyses such Oberauer and Lewandowsky
(\protect\hyperlink{ref-oberauer2019}{2019}) can provide a guide to our
allocation of scientific effort.

Our goal in this book is somewhat different than the general goal of
metascientists considering how science should be conducted. Once
\emph{you} as a researcher decide to do a particular experiment, we
think you will want to maximize its scientific value and so you will
want it to be replicable. But we aren't suggesting that you should
necessarily do a replication study. There are many concerns that go into
whether to replicate -- including not only whether you are trying to
gather evidence about a particular phenomenon, but also whether you are
trying to master techniques and paradigms related to it. As we said at
the beginning of this chapter, not all replication is for the purpose of
verification, and you as a researcher can make an informed decision
about what experimental strategy is best for you.

\hypertarget{open-science}{%
\subsection{Open science}\label{open-science}}

The \textbf{open science movement} is, in part, a response -- really a
set of responses -- to the challenges of reproducibility and
replicability. The open science (and now the broader \textbf{open
scholarship}) movement is a broad umbrella
(Figure~\ref{fig-replication-umbrella}), but in this book we take open
science to be a set of beliefs, research practices, results, and
policies that are organized around the central roles of transparency and
verifiability in scientific practice.\sidenote{\footnotesize Another part of the open
  science umbrella involves a democratization of the scientific process
  through efforts to open access to science. This process involves both
  removal of barriers to access the scientific literature but also
  efforts to remove barriers to scientific training -- especially to
  groups historically underrepresented in the sciences. The hope is that
  these processes increase both the set of people and the range of
  perspectives contributing to science. We view these changes as no less
  critical than the transparency aspects of the open science movement,
  though more indirectly related to the current discussion of
  reproducibility and replicability.} The core of this movement is the
idea of ``nullius in verba'' (the motto of the British Royal Society,
which roughly means ``take no one's word for it.''\sidenote{\footnotesize At least
  that's a reasonable paraphrase; there's some interesting discussion
  about what this quote from Horace really means in a letter by Gould
  (\protect\hyperlink{ref-gould1991}{1991}).}

\begin{marginfigure}

{\centering \includegraphics{images/replication/umbrella2.png}

}

\caption{\label{fig-replication-umbrella}The broad umbrella of open
science (adapted from an image created for the Stanford Lane Library
Blog).}

\end{marginfigure}

Transparency initiatives are critical for ensuring reproducibility. As
we discussed above, you cannot even evaluate reproducibility in the
absence of data sharing. Code sharing can go even further towards
helping reproducibility, as code makes the exact computations involved
in data analysis much more explicit than the verbal descriptions that
are the norm in papers (\protect\hyperlink{ref-hardwicke2018b}{Hardwicke
et al. 2018}). Further, as we will discuss in \textbf{?@sec-management},
the set of practices involved in preparing materials for sharing can
themselves encourage reproducibility by leading to better organizational
practices for research data, materials, and code.

Transparency also plays a major role in advancing replicability. This
point may not seem obvious at first -- why would sharing things openly
lead to more replicable experiments? -- but it is one of the major
theses of this book, so we'll unpack it a bit. Here are a couple of
routes by which transparent practices lead to greater replication rates.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Sharing of experimental materials enables replications that closely
  follow the original study's methods (\textbf{?@sec-management}). One
  critique of many replications has been that they differ in key
  respects from the originals. Sometimes those deviations were
  purposeful, but in other cases they were simply because the
  replicators could not use the original experimental materials. Sharing
  materials solves this problem.
\item
  Sharing sampling and analysis plans allows replication of key aspects
  of design and analysis that may not be clear in verbal descriptions,
  for example exclusion criteria or details of data pre-processing.
\item
  Sharing of analytic decision-making via preregistration can lead to a
  decrease in \(p\)-hacking and other practices that can introduce bias
  (\textbf{?@sec-prereg}). The strength of statistical evidence in the
  original study is a predictor of replicability in subsequent studies.
  If original studies are preregistered, they are more likely to report
  effects that are not subject to inflation via questionable research
  practices.
\item
  Preregistration can also clarify the distinction between confirmatory
  and exploratory findings, helping subsequent experimenters to make a
  more informed judgment about which effects are likely to be good
  targets for replication.
\end{enumerate}

For all of these reasons, we believe that open science practices can
play a critical role in increasing reproducibility and replicability.

\hypertarget{a-crisis}{%
\subsection{A crisis?}\label{a-crisis}}

So, is there a ``replication crisis''? The common meaning of ``crisis''
is ``a difficult time.'' The data we reviewed in this chapter suggest
that there are real problems in the reproducibility and replicability of
the psychology literature. But there's no evidence that things have
gotten worse. If anything, we are optimistic about the changes in
practices that have happened in the last ten years. So in that sense, we
are not sure that a crisis narrative is warranted.

On the other hand, for Kuhn (\protect\hyperlink{ref-kuhn1962}{1962}),
the term ``crisis'' had a special meaning: it is a period of intense
uncertainty in a scientific field brought on by the failure of a
particular paradigm (\textbf{?@sec-theories}). A crisis typically
heralds a shift in paradigm, in which new approaches and phenomena come
to the fore.

In this sense, the replication crisis narrative isn't mutually exclusive
with other crisis narratives, including the ``generalizability crisis''
(\protect\hyperlink{ref-yarkoni2020}{Yarkoni 2020}) and the ``theory
crisis'' (\protect\hyperlink{ref-oberauer2019}{Oberauer and Lewandowsky
2019}). All of these are symptoms of discontent with the status quo. We
share this discontent! We are writing this book to encourage further
changes in experimental methods and practices to improve reproducibility
and replicability outcomes -- many of them driven by the broader set of
ideas referred to as ``open science.'' These changes may not lead to a
paradigm shift in the Kuhnian sense, but we hope that they lead to
eventual improvements. In that sense, we think agree with those who say
that the ``replication crisis'' has led to a ``credibility revolution''
(\protect\hyperlink{ref-vazire2018}{Vazire 2018}).

\hypertarget{chapter-summary-replication}{%
\section{Chapter summary:
Replication}\label{chapter-summary-replication}}

In this chapter we introduce the notions of reproducibility -- getting
the same numbers from the same analysis -- and replicability -- getting
the same conclusions from a new dataset. Both of these are critical
prerequisites of a cumulative scientific literature, yet the
meta-science literature has suggested that the rate of both
reproducibility and replicability in the published literature is quite a
bit lower than we would hope. A strong candidate explanation for low
reproducibility is simply that code and data are rarely shared alongside
published research. Lowered replicability is more difficult to explain,
but our best guess is that analytic flexibility (``\(p\)-hacking'') is
at least partially to blame. On our account, replication is a
meta-scientific tool for understanding the status of the scientific
literature rather than an end in itself. Instead, we see the open
science movement, a movement focused on the role of transparency in the
scientific process, as a promising response to issues of reproducibility
and replicability.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  How would you design a measure of the context sensitivity of an
  experiment? Think of a measure you could apply \emph{post hoc} to a
  description of an experiment (e.g., from reading a paper) so that you
  could take a group of experiments and annotate how context-sensitive
  they are on some scale.
\item
  Take the measure you designed above. How would you test that this
  measure really captured context sensitivity in a way that was not
  circular? What would be an ``objective measure'' of context
  sensitivity?
\item
  What proportion of reproducibility failures do you think are due to
  questionable practices by experimenters vs.~just plain errors? How
  would you test your hypothesis?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\item
  Still a very readable and entertaining introduction to the idea of
  p-hacking: Simmons, J. P., Nelson, L. D., \& Simonsohn, U. (2011).
  False-Positive Psychology: Undisclosed Flexibility in Data Collection
  and Analysis Allows Presenting Anything as Significant. Psychological
  Science, 22(11), 1359-1366.
  \url{https://doi.org/10.1177/0956797611417632}.
\item
  A recent review of issues of replication in psychology: Nosek, B. et
  al.~(2022). Replicability, Robustness, and Reproducibility in
  Psychological Science. Annual Review of Psychology, 73, 719-748.
  \url{https://doi.org/10.1146/annurev-psych-020821-114157}.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-4}{%
\section*{References}\label{bibliography-4}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-4}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-anderson2016}{}}%
Anderson, CJ, S Bahnik, M Barnett-Cowan, FA Bosco, J Chandler, CR
Chartier, and otherss. 2016. {``Response to Comment on {`Estimating the
Reproducibility of Psychological Science'}.''} \emph{Science} 351
(6277): 1037--37.

\leavevmode\vadjust pre{\hypertarget{ref-artner2020}{}}%
Artner, Richard, Thomas Verliefde, Sara Steegen, Sara Gomes, Frits
Traets, Francis Tuerlinckx, and Wolf Vanpaemel. 2020. {``The
Reproducibility of Statistical Results in Psychological Research: An
Investigation Using Unpublished Raw Data.''} \emph{Psychological
Methods}.

\leavevmode\vadjust pre{\hypertarget{ref-bakker2011}{}}%
Bakker, Marjan, and Jelte M Wicherts. 2011. {``The (Mis) Reporting of
Statistical Results in Psychology Journals.''} \emph{Behavior Research
Methods} 43 (3): 666--78.

\leavevmode\vadjust pre{\hypertarget{ref-baribault2018}{}}%
Baribault, Beth, Chris Donkin, Daniel R Little, Jennifer S Trueblood,
Zita Oravecz, Don Van Ravenzwaaij, Corey N White, Paul De Boeck, and
Joachim Vandekerckhove. 2018. {``Metastudies for Robust Tests of
Theory.''} \emph{Proceedings of the National Academy of Sciences} 115
(11): 2607--12.

\leavevmode\vadjust pre{\hypertarget{ref-bench2017}{}}%
Bench, Shane W, Grace N Rivera, Rebecca J Schlegel, Joshua A Hicks, and
Heather C Lench. 2017. {``Does Expertise Matter in Replication? An
Examination of the Reproducibility Project: Psychology.''} \emph{Journal
of Experimental Social Psychology} 68: 181--84.

\leavevmode\vadjust pre{\hypertarget{ref-buckheit1995}{}}%
Buckheit, Jonathan B, and David L Donoho. 1995. {``Wavelab and
Reproducible Research.''} In \emph{Wavelets and Statistics}, 55--81.
Springer.

\leavevmode\vadjust pre{\hypertarget{ref-camerer2016}{}}%
Camerer, Colin F, Anna Dreber, Eskil Forsell, Teck-Hua Ho, Jrgen Huber,
Magnus Johannesson, Michael Kirchler, et al. 2016. {``Evaluating
Replicability of Laboratory Experiments in Economics.''} \emph{Science}
351 (6280): 1433--36.

\leavevmode\vadjust pre{\hypertarget{ref-camerer2018}{}}%
Camerer, Colin F, Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jrgen
Huber, Magnus Johannesson, Michael Kirchler, et al. 2018. {``Evaluating
the Replicability of Social Science Experiments in Nature and Science
Between 2010 and 2015.''} \emph{Nature Human Behaviour} 2 (9): 637--44.

\leavevmode\vadjust pre{\hypertarget{ref-carney2010}{}}%
Carney, Dana R, Amy JC Cuddy, and Andy J Yap. 2010. {``Power Posing:
Brief Nonverbal Displays Affect Neuroendocrine Levels and Risk
Tolerance.''} \emph{Psychological Science} 21 (10): 1363--68.

\leavevmode\vadjust pre{\hypertarget{ref-cesana-arlotti2018}{}}%
Cesana-Arlotti, N, A Martn, E Tgls, L Vorobyova, R Cetnarski, and L L
Bonatti. 2018. {``Erratum for the Report {`Precursors of Logical
Reasoning in Preverbal Human Infants'}.''} \emph{Science} 361 (6408).

\leavevmode\vadjust pre{\hypertarget{ref-dominus2017}{}}%
Dominus, Susan. 2017. {``When the Revolution Came for Amy Cuddy.''}
\emph{When the Revolution Came for Amy Cuddy}.
\url{https://www.nytimes.com/2017/10/18/magazine/when-the-revolution-came-for-amy-cuddy.html}.

\leavevmode\vadjust pre{\hypertarget{ref-dreber2015}{}}%
Dreber, Anna, Thomas Pfeiffer, Johan Almenberg, Siri Isaksson, Brad
Wilson, Yiling Chen, Brian A Nosek, and Magnus Johannesson. 2015.
{``Using Prediction Markets to Estimate the Reproducibility of
Scientific Research.''} \emph{Proceedings of the National Academy of
Sciences} 112 (50): 15343--47.

\leavevmode\vadjust pre{\hypertarget{ref-ebersole2020}{}}%
Ebersole, Charles R, Maya B Mathur, Erica Baranski, Diane-Jo
Bart-Plange, Nicholas R Buttrick, Christopher R Chartier, Katherine S
Corker, et al. 2020. {``Many Labs 5: Testing Pre-Data-Collection Peer
Review as an Intervention to Increase Replicability.''} \emph{Advances
in Methods and Practices in Psychological Science} 3 (3): 309--31.

\leavevmode\vadjust pre{\hypertarget{ref-errington2021investigating}{}}%
Errington, Timothy M, Maya Mathur, Courtney K Soderberg, Alexandria
Denis, Nicole Perfito, Elizabeth Iorns, and Brian A Nosek. 2021.
{``Investigating the Replicability of Preclinical Cancer Biology.''}
\emph{Elife} 10: e71601.

\leavevmode\vadjust pre{\hypertarget{ref-etz2016}{}}%
Etz, Alexander, and Joachim Vandekerckhove. 2016. {``A Bayesian
Perspective on the Reproducibility Project: Psychology.''} \emph{{PLOS}
{ONE}} 11 (2): e0149794.
\url{https://doi.org/10.1371/journal.pone.0149794}.

\leavevmode\vadjust pre{\hypertarget{ref-frank2012}{}}%
Frank, Michael C, and Rebecca Saxe. 2012. {``Teaching Replication.''}
\emph{Perspectives on Psychological Science} 7: 595--99.

\leavevmode\vadjust pre{\hypertarget{ref-frank2013}{}}%
Frank, Michael C, Jonathan A Slemmer, Gary F Marcus, and Scott P
Johnson. 2013. {``" Information from Multiple Modalities Helps
5-Month-Olds Learn Abstract Rules": Erratum.''}

\leavevmode\vadjust pre{\hypertarget{ref-gelman2018}{}}%
Gelman, Andrew. 2018. {``Don't Characterize Replications as Successes or
Failures.''} \emph{Behavioral and Brain Sciences} 41.

\leavevmode\vadjust pre{\hypertarget{ref-gilbert2016}{}}%
Gilbert, Daniel T, Gary King, Stephen Pettigrew, and Timothy D Wilson.
2016. {``Comment on {`Estimating the Reproducibility of Psychological
Science'}.''} \emph{Science} 351 (6277): 1037--37.

\leavevmode\vadjust pre{\hypertarget{ref-gould1991}{}}%
Gould, Stephen Jay. 1991. {``Royal Shorthand.''} \emph{Science} 251
(4990): 142--42.

\leavevmode\vadjust pre{\hypertarget{ref-gould1996}{}}%
Gould, Stephen Jay, Steven James Gold, et al. 1996. \emph{The Mismeasure
of Man}. WW Norton \& company.

\leavevmode\vadjust pre{\hypertarget{ref-hardwicke2021a}{}}%
Hardwicke, Tom E, Manuel Bohn, Kyle MacDonald, Emily Hembacher, Michle
B Nuijten, Benjamin N Peloquin, Benjamin E deMayo, Bria Long, Erica J
Yoon, and Michael C Frank. 2021. {``Analytic Reproducibility in Articles
Receiving Open Data Badges at the Journal Psychological Science : An
Observational Study.''} \emph{Royal Society Open Science}.

\leavevmode\vadjust pre{\hypertarget{ref-hardwicke2018b}{}}%
Hardwicke, Tom E, Maya B Mathur, Kyle Earl MacDonald, Gustav Nilsonne,
George Christopher Banks, Mallory Kidwell, Alicia Hofelich Mohr, et al.
2018. {``Data Availability, Reusability, and Analytic Reproducibility:
Evaluating the Impact of a Mandatory Open Data Policy at the Journal
Cognition.''}

\leavevmode\vadjust pre{\hypertarget{ref-hardwicke2021c}{}}%
Hardwicke, Tom E, Robert T. Thibault, Jessica Kosie, Joshua D. Wallach,
Mallory C. Kidwell, and John Ioannidis. 2021. {``Estimating the
Prevalence of Transparency and Reproducibility-Related Research
Practices in Psychology (2014-2017).''} \emph{Perspectives on
Psychological Science}. \url{https://doi.org/10.1177/1745691620979806}.

\leavevmode\vadjust pre{\hypertarget{ref-john2012}{}}%
John, Leslie K, George Loewenstein, and Drazen Prelec. 2012.
{``Measuring the Prevalence of Questionable Research Practices with
Incentives for Truth Telling.''} \emph{Psychological Science} 23 (5):
524--32.

\leavevmode\vadjust pre{\hypertarget{ref-klein2018b}{}}%
Klein, Richard A, Michelangelo Vianello, Fred Hasselman, Byron G Adams,
Reginald B Adams Jr, Sinan Alper, Mark Aveyard, et al. 2018. {``Many
Labs 2: Investigating Variation in Replicability Across Samples and
Settings.''} \emph{Advances in Methods and Practices in Psychological
Science} 1 (4): 443--90.

\leavevmode\vadjust pre{\hypertarget{ref-kuhn1962}{}}%
Kuhn, Thomas. 1962. \emph{The Structure of Scientific Revolutions}.
Princeton University Press.

\leavevmode\vadjust pre{\hypertarget{ref-lewis2016}{}}%
Lewis, Molly L, and Michael C Frank. 2016. {``Understanding the Effect
of Social Context on Learning: A Replication of Xu and Tenenbaum
(2007b).''} \emph{Journal of Experimental Psychology: General} 145 (9):
e72.

\leavevmode\vadjust pre{\hypertarget{ref-mathur2020}{}}%
Mathur, Maya B, and Tyler J VanderWeele. 2020. {``New Statistical
Metrics for Multisite Replication Projects.''} \emph{J. R. Stat. Soc.
Ser. A Stat. Soc.} 183 (3): 1145--66.

\leavevmode\vadjust pre{\hypertarget{ref-mcshane2014}{}}%
McShane, Blakeley B, and Ulf Bckenholt. 2014. {``You Cannot Step into
the Same River Twice: When Power Analyses Are Optimistic.''}
\emph{Perspectives on Psychological Science} 9 (6): 612--25.

\leavevmode\vadjust pre{\hypertarget{ref-nelson2018}{}}%
Nelson, Leif D, Joseph Simmons, and Uri Simonsohn. 2018. {``Psychology's
Renaissance.''} \emph{Annual Review of Psychology} 69: 511--34.

\leavevmode\vadjust pre{\hypertarget{ref-newell1973}{}}%
Newell, Allen. 1973. {``You Can't Play 20 Questions with Nature and Win:
Projective Comments on the Papers of This Symposium.''}

\leavevmode\vadjust pre{\hypertarget{ref-nosek2021}{}}%
Nosek, Brian A, Tom E Hardwicke, Hannah Moshontz, Aurlien Allard,
Katherine S Corker, Anna Dreber Almenberg, Fiona Fidler, et al. 2021.
{``Replicability, Robustness, and Reproducibility in Psychological
Science.''} \emph{Annual Review of Psychology}.

\leavevmode\vadjust pre{\hypertarget{ref-nuijten2016}{}}%
Nuijten, Michle B, Chris H J Hartgerink, Marcel A L M van Assen, Sacha
Epskamp, and Jelte M Wicherts. 2016. {``The Prevalence of Statistical
Reporting Errors in Psychology (1985--2013).''} \emph{Behav. Res.
Methods} 48 (4): 1205--26.

\leavevmode\vadjust pre{\hypertarget{ref-oberauer2019}{}}%
Oberauer, Klaus, and Stephan Lewandowsky. 2019. {``Addressing the Theory
Crisis in Psychology.''} \emph{Psychonomic Bulletin \& Review} 26 (5):
1596--1618.

\leavevmode\vadjust pre{\hypertarget{ref-olsson2020heterogeneity}{}}%
Olsson-Collentine, Anton, Jelte M Wicherts, and Marcel ALM van Assen.
2020. {``Heterogeneity in Direct Replications in Psychology and Its
Association with Effect Size.''} \emph{Psychological Bulletin} 146 (10):
922.

\leavevmode\vadjust pre{\hypertarget{ref-osc2015}{}}%
Open Science Collaboration. 2015. {``Estimating the Reproducibility of
Psychological Science.''} \emph{Science} 349 (6251).

\leavevmode\vadjust pre{\hypertarget{ref-popper2005}{}}%
Popper, Karl. 2005. \emph{The Logic of Scientific Discovery}. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-ramscar2016}{}}%
Ramscar, Michael. 2016. {``Learning and the Replicability of Priming
Effects.''} \emph{Current Opinion in Psychology} 12: 80--84.

\leavevmode\vadjust pre{\hypertarget{ref-ranehill2015}{}}%
Ranehill, Eva, Anna Dreber, Magnus Johannesson, Susanne Leiberg, Sunhae
Sul, and Roberto A Weber. 2015. {``Assessing the Robustness of Power
Posing: No Effect on Hormones and Risk Tolerance in a Large Sample of
Men and Women.''} \emph{Psychological Science} 26 (5): 653--56.

\leavevmode\vadjust pre{\hypertarget{ref-rohrer2015}{}}%
Rohrer, Doug, Harold Pashler, and Christine R Harris. 2015. {``Do Subtle
Reminders of Money Change People's Political Views?''} \emph{Journal of
Experimental Psychology: General} 144 (4): e73.

\leavevmode\vadjust pre{\hypertarget{ref-rosenthal1979}{}}%
Rosenthal, Robert. 1979. {``The File Drawer Problem and Tolerance for
Null Results.''} \emph{Psychological Bulletin} 86 (3): 638.

\leavevmode\vadjust pre{\hypertarget{ref-rosenthal1990}{}}%
---------. 1990. {``Replication in Behavioral Research.''} \emph{Journal
of Social Behavior \& Personality} 5: 1--30.

\leavevmode\vadjust pre{\hypertarget{ref-scheel2021}{}}%
Scheel, Anne M, Mitchell RMJ Schijen, and Danil Lakens. 2021. {``An
Excess of Positive Results: Comparing the Standard Psychology Literature
with Registered Reports.''} \emph{Advances in Methods and Practices in
Psychological Science} 4 (2): 25152459211007467.

\leavevmode\vadjust pre{\hypertarget{ref-schmidt2009}{}}%
Schmidt, Stefan. 2009. {``Shall We Really Do It Again? The Powerful
Concept of Replication Is Neglected in the Social Sciences.''}
\emph{Review of General Psychology} 13: 90--100.

\leavevmode\vadjust pre{\hypertarget{ref-schwarz1983}{}}%
Schwarz, Norbert, and Gerald L Clore. 1983. {``Mood, Misattribution, and
Judgments of Well-Being: Informative and Directive Functions of
Affective States.''} \emph{Journal of Personality and Social Psychology}
45 (3): 513.

\leavevmode\vadjust pre{\hypertarget{ref-schwarz2014}{}}%
Schwarz, Norbert, and Fritz Strack. 2014. {``Does Merely Going Through
the Same Moves Make for a {`Direct'} Replication? Concepts, Contexts,
and Operationalizations.''}

\leavevmode\vadjust pre{\hypertarget{ref-simmons2011}{}}%
Simmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2011.
{``False-Positive Psychology: Undisclosed Flexibility in Data Collection
and Analysis Allows Presenting Anything as Significant.''}
\emph{Psychological Science} 22 (11): 1359--66.

\leavevmode\vadjust pre{\hypertarget{ref-simmons2018}{}}%
---------. 2018. {``False-Positive Citations.''} \emph{Perspectives on
Psychological Science} 13 (2): 255--59.

\leavevmode\vadjust pre{\hypertarget{ref-simmons2017}{}}%
Simmons, Joseph P, and Uri Simonsohn. 2017. {``Power Posing: P-Curving
the Evidence.''} \emph{Psychological Science}.

\leavevmode\vadjust pre{\hypertarget{ref-simonsohn2015}{}}%
Simonsohn, Uri. 2015. {``Small Telescopes: Detectability and the
Evaluation of Replication Results.''} \emph{Psychol. Sci.} 26 (5):
559--69.

\leavevmode\vadjust pre{\hypertarget{ref-vadillo2016}{}}%
Vadillo, Miguel A, Tom E Hardwicke, and David R Shanks. 2016.
{``Selection Bias, Vote Counting, and Money-Priming Effects: A Comment
on Rohrer, Pashler, and Harris (2015) and Vohs (2015).''} \emph{Journal
of Experimental Psychology: General}.

\leavevmode\vadjust pre{\hypertarget{ref-van-bavel2016}{}}%
Van Bavel, Jay J, Peter Mende-Siedlecki, William J Brady, and Diego A
Reinero. 2016. {``Contextual Sensitivity in Scientific
Reproducibility.''} \emph{Proceedings of the National Academy of
Sciences} 113 (23): 6454--59.

\leavevmode\vadjust pre{\hypertarget{ref-vazire2018}{}}%
Vazire, Simine. 2018. {``Implications of the Credibility Revolution for
Productivity, Creativity, and Progress.''} \emph{Perspectives on
Psychological Science} 13 (4): 411--17.

\leavevmode\vadjust pre{\hypertarget{ref-wicherts2006}{}}%
Wicherts, Jelte M, Denny Borsboom, Judith Kats, and Dylan Molenaar.
2006. {``The Poor Availability of Psychological Research Data for
Reanalysis.''} \emph{American Psychologist} 61 (7): 726.

\leavevmode\vadjust pre{\hypertarget{ref-wilson2020}{}}%
Wilson, Brent M, Christine R Harris, and John T Wixted. 2020. {``Science
Is Not a Signal Detection Problem.''} \emph{Proceedings of the National
Academy of Sciences} 117 (11): 5559--67.

\leavevmode\vadjust pre{\hypertarget{ref-yang2020}{}}%
Yang, Yang, Wu Youyou, and Brian Uzzi. 2020. {``Estimating the Deep
Replicability of Scientific Findings Using Human and Artificial
Intelligence.''} \emph{Proceedings of the National Academy of Sciences}
117 (20): 10762--68.

\leavevmode\vadjust pre{\hypertarget{ref-yarkoni2020}{}}%
Yarkoni, Tal. 2020. {``The Generalizability Crisis.''} \emph{Behav.
Brain Sci.} 45: 1--37.

\leavevmode\vadjust pre{\hypertarget{ref-youyou2023}{}}%
Youyou, Wu, Yang Yang, and Brian Uzzi. 2023. {``A Discipline-Wide
Investigation of the Replicability of Psychology Papers over the Past
Two Decades.''} \emph{Proceedings of the National Academy of Sciences}
120 (6): e2208863120.

\leavevmode\vadjust pre{\hypertarget{ref-zwaan2018}{}}%
Zwaan, Rolf Antonius, Alexander Etz, Richard E Lucas, and Brent
Donnellan. 2018. {``Making Replication Mainstream.''}

\end{CSLReferences}

\hypertarget{sec-ethics}{%
\chapter{Ethics}\label{sec-ethics}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Distinguish between consequentialist, deontological, and virtue ethics
  frameworks
\item
  Identify key ethical issues in performing experimental research
\item
  Discuss ethical responsibilities in analysis and reporting of research
\item
  Describe ethical arguments for open science practices
\end{itemize}

\end{tcolorbox}

The fundamental thesis of this book is that experiments are the way to
estimate causal effects, which are the foundations of theory. And as we
discussed in \textbf{?@sec-experiments}, the reason why experiments
allow for strong causal inferences is because of two ingredients: a
manipulation -- in which the experimenter changes the world in some way
-- and randomization. Put a different way, experimenters learn about the
world by randomly deciding to do things to their participants! Is that
even allowed?

Experimental research raises a host of ethical issues that deserve
consideration. What can and can't we do to participants in an
experiment, and what considerations do we owe to them by virtue of their
decision to participate? To facilitate our discussion of these issues,
we start by briefly introducing the standard philosophical frameworks
for ethical analysis. We then use those to discuss problems of
experimental ethics, first from the perspective of participants and then
second from the perspective of the scientific ecosystem more
broadly.\sidenote{\footnotesize We have placed this chapter in the Foundations section
  of the book because we think it's critical to start the conversation
  about your ethical responsibilities as an experimentalist and
  researcher even before you start planning a study. We'll come back to
  the ethical frameworks we describe here in \textbf{?@sec-collection},
  which deals specifically with participant recruitment and the informed
  consent process.}

\begin{tcolorbox}[colframe=.blue, title=\faMicroscope \enspace Case study]

\hypertarget{shock-treatment}{%
\section*{Shock treatment}\label{shock-treatment}}
\addcontentsline{toc}{section}{Shock treatment}

\markright{Shock treatment}

A decade after surviving prisoners were liberated from the last
concentration camp, Adolf Eichmann, one of the Holocaust's primary
masterminds, was tried for his instrumental role in the mass genocide
(\protect\hyperlink{ref-baade1961}{Baade 1961}). While reflecting on his
rationale for forcibly removing, torturing, and eventually murdering
millions of Jews, an unrepentant Eichmann claimed that he was ``merely a
cog in the machinery that carried out the directives of the German
Reich'' and therefore was not directly responsible
(\protect\hyperlink{ref-kilham1974}{Kilham and Mann 1974}). This
startling admission gave a young researcher an interesting idea: ``Could
it be that Eichmann and his million accomplices in the Holocaust were
just following orders? Could we call them all accomplices?''
(\protect\hyperlink{ref-milgram1974}{Milgram 1974}).

Stanley Milgram aimed to make a direct test of whether people would
comply under the direction of an authority figure no matter how
uncomfortable or harmful the outcome. He invited participants into the
laboratory to serve as a teacher for an activity
(\protect\hyperlink{ref-milgram1963}{Milgram 1963}). Participants were
told that they were to administer electric shocks of increasing voltage
to another participant, the student, in a nearby room whenever the
student provided an incorrect response. In reality, there were no
shocks, and the student was an actor who was in on the experiment and
only pretended to be in pain when the `shocks' were administered.
Participants were encouraged to continue administering shocks despite
clearly audible pleas from the student to stop. In one of Milgram's
studies, nearly 65\% of participants administered the maximum voltage to
the student.

This deeply unsettling result has become, as Ross and Nisbett
(\protect\hyperlink{ref-ross2011}{2011}) say, ``part of our society's
shared intellectual legacy,'' informing our scientific and popular
conversation in myriad different ways. At the same time, modern
re-analyses of archival materials from the study have called into
question whether the deception in the study was effective, casting doubt
on its central findings (\protect\hyperlink{ref-perry2020}{Perry et al.
2020}).

Regardless of its scientific value, Milgram's study blatantly violates
modern ethical norms around the conduct of research. Among other
violations, the procedure involved \textbf{coercion} that undermined
participants' right to withdraw from the experiment. This coercion
appeared to have negative consequences: Milgram noted that a number of
his participants displayed anxiety symptoms and nervousness. This
observation was distressing and led to calls for this sort of research
to be declared unethical (e.g.,
\protect\hyperlink{ref-baumrind1964}{Baumrind 1964}). The ethical issues
surrounding Milgram's study are complex, and some are relatively
specific to the particulars of his study and moment
(\protect\hyperlink{ref-miller2009}{Miller 2009}). But the controversy
around the study was an important part of convincing the scientific
community to adopt stricter policies that protect study participants
from unnecessary harm.

\end{tcolorbox}

\hypertarget{ethical-frameworks}{%
\section{Ethical frameworks}\label{ethical-frameworks}}

Was Milgram's experiment really ethically wrong -- in the sense that it
should not have been performed? You might have the intuition that is was
unethical, due to the harms that the participants experienced or the way
(some of them, at least) were deceived by the experimenter. Others might
consider arguments in defense of the experiment, perhaps that what we
learned from the experiment was sufficiently valuable to justify its
being conducted. Beyond simply arguing back and forth, how could we
approach this issue more systematically?

Ethical frameworks offer tools for analyzing such situations. In this
section, we'll introduce two of the most commonly used frameworks:
consequentialist and deontological ethics. We'll also discuss how each
of these could be applied to Milgram's paradigm.

\hypertarget{consequentialist-theories}{%
\subsection{Consequentialist theories}\label{consequentialist-theories}}

Ethical theories provide principles for what constitute good actions.
The simplest theory of good actions is the \textbf{consequentialist}
theory: good actions lead to good results. The most famous
consequentialist position is the \textbf{utilitarian position},
originally defined by the philosopher John Stuart Mill
(\protect\hyperlink{ref-flinders1992}{Flinders 1992}). This view
emphasizes decision-making based on the ``greatest happiness
principle'', or the idea that an action should be considered morally
good based on the degree of happiness or pleasure people experience
because of it, and likewise that an action should be considered morally
bad based on the degree of unhappiness or pain people experience by the
same action (\protect\hyperlink{ref-mill1859}{Mill 1859}).

A consequentialist analysis of Milgram's study considers the study's
negative and positive effects and weighs these against one another. Did
the study cause harm to its participants? On the other hand, did the
study lead to knowledge that prevented harm or caused positive benefits?

Consequentialist analysis can be a straightforward way to justify the
risks and benefits of a particular action, but in the research setting
it is unsatisfying. Many horrifying experiments would be licensed by a
consequentialist analysis and yet feel untenable to us. Imagine a
researcher forced you to undergo a risky and undesired medical
intervention because the resulting knowledge might benefit thousands of
others. This experiment seems like precisely the kind of thing our
ethical framework should rule out!

\hypertarget{deontological-approaches}{%
\subsection{Deontological approaches}\label{deontological-approaches}}

Harmful research performed against participants' will or without their
knowledge is repugnant; we consider the Tuskegee Syphilis Experiment, a
horrifying example of such research, below. Considering such cases
suggests obvious rules, such as ``researchers must \emph{ask
participants' permission} before conducting research on them.''
Principles like this one are now formalized in all ethical codes for
research. They exemplify an approach called \textbf{deontological} (or
duty-based) ethics.

Deontology emphasizes the importance of taking ethically permissible
actions, regardless of their outcome
(\protect\hyperlink{ref-biagetti2020}{Biagetti, Gedutis, and Ma 2020}).
In general, university ethics boards take a deontological approach to
ethics (\protect\hyperlink{ref-boser2007}{Boser 2007}). In the context
of research, there are four primary principles being applied:

\begin{enumerate}
\def\labelenumi{(\arabic{enumi})}
\item
  \textbf{Respect for autonomy}. This principle requires that people
  participating in research studies can make their own decisions about
  their participation, and that those with diminished autonomy
  (children, neuro-divergent people, etc.) should receive equal
  protections (\protect\hyperlink{ref-beauchamp2001}{Beauchamp,
  Childress, et al. 2001}). Respecting someone's autonomy also means
  providing them with all the information they need to make an informed
  decision about whether to participate in a research study (giving
  \textbf{consent}) and giving them further context about the study they
  have participated in after it is done (\textbf{debriefing}).
\item
  \textbf{Beneficence}. This principle means that researchers are
  obligated (and not simply suggested) to protect the well-being of
  participants for the duration of the study. Beneficence has two parts.
  The first is to do no harm. Researchers must take steps to minimize
  the risks to participants and to disclose any known risks at the
  onset. If risks are discovered during participation, researchers must
  notify participants of their discovery and make reasonable efforts to
  mitigate these risks, even if that means stopping the study
  altogether. The second is to maximize potential benefits to
  participants.\sidenote{\footnotesize In practice, this doesn't mean compensating
    participants with exorbitant amounts of money or gifts, which might
    cause other issues, like exerting an undue influence on low-income
    participants to participate. Instead ``maximizing benefits'' is
    interpreted as identifying all possible benefits of participation in
    the research and making them available where possible.}
\item
  \textbf{Nonmaleficence}. This principle is similar to beneficence (in
  fact, beneficence and nonmaleficence were a single principle when they
  were first introduced in the \textbf{Belmont Report}, which we'll
  discuss later) but differs in its emphasis on doing/causing no harm.
  In general, harm is bad -- but deontology is about intent, not impact,
  so harm is sometimes warranted when the intent is morally good. For
  example, administering a vaccine may cause some discomfort and pain,
  but the intent is to protect the patient from developing a deadly
  virus in the future. The harm is justifiable under this framework.
\item
  \textbf{Justice}. This principle means that both the benefits and
  risks of a study should be equally distributed among all participants.
  For example, participants should not be systematically assigned to one
  condition over another based on features of their identity such as
  socioeconomic status, race and ethnicity, or gender.
\end{enumerate}

Analyzed from the perspective of these principles, Milgram's study
raises several red flags. First, Milgram's study reduced participants
autonomy by making it difficult for them to voluntarily end their
involvement (participants were told up to four times to continue
administering shocks even after they expressed clear opposition).
Second, the paradigm was designed in a way that it was likely to cause
harm to its participants by putting them in a very stressful situation.
Further, Milgram's study may have induced \emph{unnecessary} harm on
certain participants by failing to screen participants for existing
mental health issues before beginning the session.

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{was-milgram-justified}{%
\section*{Was Milgram justified?}\label{was-milgram-justified}}
\addcontentsline{toc}{section}{Was Milgram justified?}

\markright{Was Milgram justified?}

Was the harm done in Milgram's experiment justifiable given that it
informed our understanding of obedience and conformity? We can't say for
sure. What we can say is that in the 10 years following the publication
of Milgram's study, the number of papers on (any kind of) obedience
increased and the nature of these papers expanded from a focus on
religious conformity to a broader interest in social conformity,
suggesting that Milgram changed the direction of this research area.
Additionally, in a followup that Milgram conducted, he reported that
84\% of participants in the original study said they were happy to have
been involved (\protect\hyperlink{ref-milgram1974}{Milgram 1974}). On
the other hand, given concerns about validity in the original study,
perhaps its influence on the field was not warranted
(\protect\hyperlink{ref-perry2020}{Perry et al. 2020}).

Many researchers believe there was no ethical way to conduct Milgram's
experiment while also protecting the integrity of the research goals,
but some have tried. One study recreated a portion of the original
experiment, with some critical changes
(\protect\hyperlink{ref-burger2007}{Burger 2007}). Before enrolling in
the study, participants completed both a phone screening for mental
health concerns, addiction, or extreme trauma, and a formal interview
with a licensed clinical psychologist, who identified signs of
depression or anxiety. Those who passed these assessments were invited
into the lab for a Milgram-type learning study. Experimenters clearly
explained that participation was voluntary and the decision to
participate could be reversed at any point, either by the participant
themselves or by a trained clinical psychologist who was present for the
duration of the session. Additionally, shock administration never
exceeded 150 volts (compared to 450 volts in the original study), and
experimenters debriefed participants extensively following the end of
the session. This modified replication study found similar patterns of
obedience as Milgram's; further, one year later, no participants
expressed any indication of stress or trauma associated with their
involvement in the study.

\end{tcolorbox}

\hypertarget{ethical-responsibilities-to-research-participants}{%
\section{Ethical responsibilities to research
participants}\label{ethical-responsibilities-to-research-participants}}

Milgram's shock experiment was just one of dozens of unethical human
subjects studies that garnered the attention and anger of the public in
the United States. In 1978, the US National Commission for the
Protection of Human Services of Biomedical and Behavioral Research
released \textbf{The Belmont Report}, which described protections for
the rights of human subjects participating in research studies
(\protect\hyperlink{ref-adashi2018}{Adashi, Walters, and Menikoff
2018}). Perhaps the most important message found in the Report was the
notion that ``investigators should not have sole responsibility for
determining whether research involving human subjects fulfills ethical
standards. Others, who are independent of the research, must share the
responsibility.'' In other words, ethical research requires both
transparency and external oversight.

\hypertarget{institutional-review-boards}{%
\subsection{Institutional review
boards}\label{institutional-review-boards}}

The creation of \textbf{institutional review boards} (IRBs) in the
United States was an important result of the Belmont Report. While
regulatory frameworks and standards vary across national boundaries,
ethical review of research is ubiquitous across countries.\sidenote{\footnotesize In
  what follows, we focus on the US regulatory framework as it has been a
  model for other ethical review systems. In other countries, IRBs are
  often referred to as ``ethics review boards,'' which is a clearer
  name.}.

An IRB is a committee of people who review, evaluate, and monitor human
subjects research to make sure that participants' rights are protected
when they participate in research
(\protect\hyperlink{ref-oakes2002}{Oakes 2002}). IRBs are local; every
organization that conducts human subjects or animal research is required
to have its own IRB or to contract with an external one. If you are
based at a university, yours likely has its own, and its members are
probably a mix of scientists, doctors, professors, and community
residents.\sidenote{\footnotesize The local control of IRBs can lead to very different
  practices in ethical review across institutions, which is obviously
  inconsistent consistent with the idea that ethical standards should be
  uniform! In addition, critics have wondered about the structural issue
  that institutional IRBs have an incentive to decrease liability for
  the institution, while private IRBs have an incentive to provide
  approvals to the researchwers who pay them
  (\protect\hyperlink{ref-lemmens2000}{Lemmens and Freedman 2000}).}

When a group of researchers have a research question they are interested
in pursuing with human subjects, they must receive approval from their
local IRB before beginning any data collection. The IRB reviews each
study to assess whether:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A study poses no more than \textbf{minimal risk} to participants. This
  means the anticipated harm or discomfort to the participant is not
  greater than what would be experienced in everyday life. Research that
  involves greater than minimal risk is subjected to added monitoring
  for potential harm to subjects.
\item
  Researchers obtain \textbf{informed consent} from participants before
  collecting any data. This requirement means experimenters must
  disclose all potential risks and benefits so that participants can
  make an informed decision about whether or not to participate in the
  study. Importantly, informed consent does not stop after participants
  sign a consent form. If researchers discover any new potential risks
  or benefits along the way, they must disclose these discoveries to all
  participants (see \textbf{?@sec-collection}).
\item
  Sensitive information remains \textbf{confidential}. Although
  regulatory frameworks vary, researchers typically have an obligation
  to their participants to protect all identifying information recorded
  during the study (see \textbf{?@sec-management}).
\item
  Participants are recruited \textbf{equitably} and without
  \textbf{coercion}. Before IRBs became standard, researchers often
  coercively recruited marginalized and vulnerable populations to test
  their research questions, rather than making participation in research
  studies voluntary and providing equitable access to the opportunity to
  participate.
\end{enumerate}

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{the-tuskegee-syphilis-study}{%
\section*{The Tuskegee Syphilis
Study}\label{the-tuskegee-syphilis-study}}
\addcontentsline{toc}{section}{The Tuskegee Syphilis Study}

\markright{The Tuskegee Syphilis Study}

In 1929, The United States Public Health Service (USPHS) was perplexed
by the effects of syphilis in Macon County, Alabama, an area with an
overwhelmingly Black population
(\protect\hyperlink{ref-brandt1978}{Brandt 1978}). Syphilis is a
sexually transmitted bacterial infection that can either be in a visible
and active stage or in a latent stage. At the time of the study's
inception, roughly 36\% of Tuskegee's adult population had developed
some form of syphilis, one of the highest infection rates in America
(\protect\hyperlink{ref-white2006}{White 2006}).

The USPHS recruited 400 Black males from 25--60 years of age with latent
syphilis and 200 Black males without the infection to serve as a control
group to participate (\protect\hyperlink{ref-brandt1978}{Brandt 1978}).
The USPHS sought the help of the Macon County Board of Health to recruit
participants with the promise that they would provide treatment for
community members with syphilis. The researchers sought poor, illiterate
Black people and, instead of telling them that they were being recruited
for a research study, merely informed them that they would be treated
for ``bad blood''.

Because the study was interested in tracking the natural course of
latent syphilis without any medical intervention, the USPHS had no
intention of providing any care to its participants. To assuage
participants, the USPHS distributed an ointment that had not been shown
to be effective in the treatment of syphilis, and only small doses of a
medication actually used to treat the infection. In addition,
participants underwent a spinal tap which was presented to them as
another form of therapy and their ``last chance for free treatment.''

By 1955, just over 30\% of the original participants had died from
syphilis complications. It took until the 1970s before the final report
was released and (the lack of) treatment ended. In total, 128
participants died of syphilis or complications from the infection, 40
wives became infected, and 19 children were born with the infection
(\protect\hyperlink{ref-katz2011}{Katz and Warren 2011}). The damage
rippled through two generations, and many never actually learned what
had been done to them.

The Tuskegee experiment violates nearly every single guideline for
research described above -- indeed in its many horrifying violations of
research participants' agency, it provides a blueprint for future
regulation to prevent any aspect of it from being repeated:
Investigators did not obtain informed consent. Participants were not
made aware of all known risks and benefits involved with their
participation. Instead, they were deceived by researchers who led them
to believe that diagnostic and invasive exams were directly related to
their treatment.

Perhaps most shocking, participants were denied appropriate treatment
following the discovery that penicillin was effective at treating
syphilis (\protect\hyperlink{ref-mahoney1943}{Mahoney, Arnold, and
Harris 1943}). The USPHS requested that medical professionals overseeing
their care outside of the research study not offer treatment to
participants so as to preserve the study's methodological integrity.
This intervention violated participants' rights to equal access to care,
which should have taken precedence over the results of the study.

Finally, recruitment was both imbalanced and coercive. Not only were
participants selected from the poorest of neighborhoods in the hopes of
finding vulnerable populations with little agency, but they were also
bribed with empty promises of treatment and a monetary incentive
(payment for burial fees, a financial obstacle for many sharecroppers
and tenant farmers at the time).

\end{tcolorbox}

\hypertarget{risks-and-benefits}{%
\subsection{Risks and benefits}\label{risks-and-benefits}}

Imagine that you were approached about participating in a research study
at your local university. You were only told you would be paid \$25 in
exchange for completing an hour of cognitive tasks on a computer. Now
imagine that halfway through the session, the experimenter revealed they
would also need to collect a blood sample, ``which should only take a
couple of minutes and which will really help the research study.'' Would
you agree to the sample? Would you feel uncomfortable in any way?

Participants need to understand the risks and benefits of participation
in an experiment before they give consent. To do otherwise compromises
their autonomy (a key deontological principle). In the case of this
hypothetical experiment, a new and unexpected invasive component of an
experiment is coercive: participants would have to choose to forfeit
their expected compensation to opt out. They also might feel that they
have been deceived by the experimenter.

In human subjects research, \textbf{deception} is a specific technical
term that refers to cases when (1) experimenters withhold any
information about its goals or intentions, (2) experimenters hide their
true identity (such as when using actors), (3) some aspects of the
research are under- or overstated to conceal information, or (4)
participants receive any false or misleading information. The use of
deception requires special consideration from a human subjects
perspective (\protect\hyperlink{ref-kelman2017}{Kelman 2017};
\protect\hyperlink{ref-baumrind1985}{Baumrind 1985}).

Even assuming they are disclosed properly without coercion or deception,
the risks and benefits of a study must be assessed from the perspective
of the \emph{participant}, not the experimenter. By doing so, we allow
participants to make an informed choice. In the case of the blood
sample, the risks to the participant were not disclosed, and the
benefits were stated in terms of the research project (and the
experimenter). Neither of these allow the participant to weigh the
decision based on their own values.

The benefits of participation in research can either be direct or
indirect, and it is important to specify which type participants may
receive. While some clinical studies and interventions may offer some
direct benefit due to participation, many of the benefits of basic
science research may be indirect. Both have their place in science, but
participants must ultimately determine the degree to which each type of
benefit motivates their own involvement in a study
(\protect\hyperlink{ref-shatz1986}{Shatz 1986}).

\hypertarget{ethical-responsibilities-in-analysis-and-reporting-of-research}{%
\section{Ethical responsibilities in analysis and reporting of
research}\label{ethical-responsibilities-in-analysis-and-reporting-of-research}}

As scientists, we not only have a responsibility to participants, we are
also responsible for what we do with our data and for the kinds of
conclusions we draw. Cases like Stapel's (see box) seem stunning, but
they are part of a continuum. Codes of professional ethics for
organizations like the American Psychological Association encourage
researchers to take care in the management and analysis of their data so
as to avoid errors and misstatements
(\protect\hyperlink{ref-apaethics}{Association 2022}).

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{what-data}{%
\section*{What data?}\label{what-data}}
\addcontentsline{toc}{section}{What data?}

\markright{What data?}

Dutch social psychologist Diederick Stapel contributed to more than 200
articles on social comparison, stereotype threat, and discrimination,
many published in the most prestigious journals. Stapel reported that
affirming positive personal qualities buffered against dangerous social
comparison, that product advertisements related to a person's
attractiveness changed their sense of self, and that exposure to
intelligent in-group members boosted a person's performance on future
tasks (\protect\hyperlink{ref-stapel2012}{Stapel and Linde 2012};
\protect\hyperlink{ref-trampe2011}{Trampe, Stapel, and Siero 2011};
\protect\hyperlink{ref-gordijn2012}{Gordijn and Stapel 2012}). These
findings were fresh and noteworthy at the time of publication, and
Stapel's papers were cited thousands of times. The only problem?
Stapel's data were made up.

Stapel has admitted that when he first began fabricating data, he would
make small tweaks to a few data points
(\protect\hyperlink{ref-stapel2012b}{Stapel 2012}). Changing a single
number here and there would turn a flat study into an impressive one.
Having achieved comfortable success (and having aroused little suspicion
from journal editors and others in the scientific community), Stapel
eventually began creating entire data sets and passing them off as his
own. Several colleagues began to grow skeptical of his overwhelming
success, however, and brought their concerns to the Psychology
Department at Tilburg University. By the time the investigation of his
work concluded, 58 of Stapel's papers were \textbf{retracted}, meaning
that the publishing journal withdrew the paper(s) after discovering that
its contents were erroneous or invalid.

Everyone agrees that Stapel's behavior was deeply unethical. But should
we consider cases of falsification and fraud to be different in kind
from other ethical violations in research? Or is it merely the endpoint
in a continuum that might include other practices like p-hacking?
Lawyers and philosophers grapple with the precise boundary between
sloppiness and neglect, and it can be difficult to know which one is at
play when a typo or coding mistake changes the conclusion of a
scientific paper. Similarly, if a researcher engages in so-called
``questionable research practices,'' at what point should they be
considered to have made an ethical violation as opposed to simply
performing their research poorly? These are hard questions, but you
should try to grapple with them, since these situations are not as rare
as we would like.

\end{tcolorbox}

Researchers also have an obligation not to suppress findings based on
their own beliefs about the right answer. One unfortunate way that this
suppression can happen is when researchers selectively report their
research, leading to \textbf{publication bias}, as you learned in
\textbf{?@sec-replication}. Researchers' own biases can be another
(invalid) rationale for not publishing: it's also an ethical violation
to suppress findings that contradict your theoretical commitments.

Importantly, researchers don't have an obligation to publish
\emph{everything} they do. Publishing in the peer-reviewed literature is
difficult and time-consuming. There are plenty of reasons not to publish
an experimental finding! For example, there's no reason to publish a
result if you believe it is truly uninformative because of a confound in
the experimental design. You also aren't committing an ethical violation
if you decide to quit your job in research and so you don't publish a
study from your dissertation.\sidenote{\footnotesize Of course, if your dissertation
  contains the cure to a common and fatal disease, maybe the situation
  is different!} The primary ethical issue arises when you use the
\emph{result} of a study -- and how it relates to your own beliefs or to
a threshold like \(p<.05\) -- to decide whether to publish it or not.

As we'll discuss again and again in this book, the preparation of
research reports must also be done with care and attention to detail
(see \textbf{?@sec-writing}). Sloppiness in writing up results can lead
to imprecise or over-broad claims; and if that sloppiness extends to the
reporting of analyses and data, it may lead to irreproducibility as
well.

Further, professional ethics dictate that published contributions to the
literature be original. In general, the text of a paper must not be
\textbf{plagiarized} (copied) from the text of other reports whether by
you or by another author without attribution. Copying from others
outside of a direct, attributed quotation is obviously an ethical
violation because it leads to credit for text being given to you rather
than the true author. But self-plagiarism is also not acceptable -- it
is a violation to receive credit multiple times for the same
product.\sidenote{\footnotesize Standards on this issue differ from field to field.
  Our sense is that the rule on self-plagiarism applies primarily to
  duplication of content between journal papers. So, for example,
  barring any specific policy of the funder or journal, it is acceptable
  to use text from one of your own grant proposals in a journal paper.
  It is also typically acceptable to reuse text from a conference
  abstract or preregistration (that you wrote, of course) when prepare a
  journal paper.}

\hypertarget{ethical-responsibilities-to-the-broader-scientific-community}{%
\section{Ethical responsibilities to the broader scientific
community}\label{ethical-responsibilities-to-the-broader-scientific-community}}

The open science principles that we will describe throughout this book
are not only important correctives to issues of reproducibility and
replicability, they are also ethical duties.

The sociologist Robert Merton described a set of norms that science is
assumed to follow: communism -- that scientific knowledge belongs to the
community; universalism -- that the validity of scientific results is
independent of the identity of the scientists; disinterestedness -- that
scientists and scientific institutions act for the benefit of the
overall enterprise; and organized skepticism -- that scientific findings
must be critically evaluated (\protect\hyperlink{ref-merton1979}{Merton
1979}).

If the products of science aren't open, it is very hard to be a
scientist by Merton's definition. To contribute to the communal good,
papers need to be openly available. And to be subject to skeptical
inquiry, experimental materials, research data, analytic code, and
software must be all available so that analytic calculations can be
verified and experiments can be reproduced. Otherwise, you have to
accept arguments on authority rather than by virtue of the materials and
data.

Openness is not only definitionally part of the scientific enterprise,
it's also good for science and individual scientists
(\protect\hyperlink{ref-gorgolewski2016}{Gorgolewski and Poldrack
2016}). Open access publications are cited more
(\protect\hyperlink{ref-eysenbach2006}{Eysenbach 2006};
\protect\hyperlink{ref-gargouri2010}{Gargouri et al. 2010}). Open data
also increases the potential for citation and reuse, and maximizes the
chances that errors are found and corrected.

But these benefits mean that researchers have a responsibility to their
funders to pursue open practices so as to seek the maximal return on
funders' investments. And by the same logic, if research participants
contribute their time to scientific projects, the researchers also owe
it to these participants to maximize the impact of their contributions
(\protect\hyperlink{ref-brakewood2013}{Brakewood and Poldrack 2013}).
For all of these reasons, individual scientists have a duty to be open
-- and scientific institutions have a duty to promote transparency in
the science they support and publish.

How should these duties be balanced against researchers' other
responsibilities? For example, how should we balance the benefit of data
sharing against the commitment to preserve participant privacy? And,
since transparency policies also carry costs in terms of time and
effort, how should researchers consider those costs against other
obligations?

First, open practices should be a default in cases where risks and costs
are limited. For example, the vast majority of journals allow authors to
post accepted manuscripts in their un-typeset form to an open
repository. This route to ``green'' open access is easy, cost free, and
-- because it comes only after articles are accepted for publication --
confers essentially no risks of scooping. As a second example, the vast
majority of analytic code can be posted as an explicit record of exactly
how analyses were conducted, even if posting data is sometimes more
fraught. These kinds of ``incentive compatible'' actions towards
openness can bring researchers much of the way to a fully transparent
workflow, and there is no excuse not to take them.

Second, researchers should plan for sharing and build a workflow that
decreases the costs of openness. As we discuss in
\textbf{?@sec-management}, while it can be costly and difficult to share
data after the fact if they were not explicitly prepared for sharing,
good project management practices can make this process far simpler (and
in many cases completely trivial).

Finally, given the ethical imperative towards openness, institutions
like funders, journals, and societies need to use their role to promote
open practices and to mitigate potential negatives
(\protect\hyperlink{ref-nosek2015}{Nosek et al. 2015}). Scholarly
societies have an important role to play in educating scientists about
the benefits of openness and providing resources to steer their members
towards best practices for sharing their publication and other research
products. Similarly, journals can set good defaults, for example by
requiring data and code sharing except in cases where a strong
justification is given not to. Funders of research can -- and
increasingly, do -- signal their interest in openness through data
sharing mandates.

\hypertarget{chapter-summary-ethics}{%
\section{Chapter summary: Ethics}\label{chapter-summary-ethics}}

In this chapter, we discussed three ethical frameworks and evaluated how
they can be applied to our own research through the lens of Milgram's
famous prison experiment. Studies like this one prompted serious
conversations about how best to reconcile experimenter goals with
participant well-being. The publication of the Belmont Report and later
creation of the IRB in the United States standardized the way scientists
approach human subjects research, and created much-needed
accountability. We also addressed our ethical responsibilities to the
scientific community, both in how we report our data and how we
distribute it. We hope that we have convinced you that open science is
an ethical imperative for researchers.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The COVID-19 pandemic led to an immense amount of ``rapid response''
  research in psychology that aimed to discover -- and influence -- the
  way people reasoned about contagion, vaccines, masking, and other
  aspects of the public health situation. What are the specific ethical
  concerns that researchers should be aware of for this type of
  research? Are there reasons for more caution in this kind of research
  than in other ``run of the mill'' research?
\item
  Think of an argument against open science practices -- for example,
  that following open science practices is especially burdensome for
  researchers with more limited resources (you can make up another if
  you want!). Given our argument that researchers have an ethical duty
  to openness, how would you analyze this argument under the three
  different ethical frameworks we discussed?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\tightlist
\item
  The Belmont Report has shaped US research ethics policy from its
  publication to the present day. It's also short and quite readable:
  \href{}{https://www.hhs.gov/ohrp/regulations-and-policy/belmont-report/index.html}.
\item
  A rich reference with several case studies on science misconduct and
  with strong arguments for open science: Ritchie, S. (2020). Science
  fictions: How fraud, bias, negligence, and hype undermine the search
  for truth. Metropolitan Books.
\end{itemize}

\end{tcolorbox}

\leavevmode\vadjust pre{\hypertarget{statistics}{}}%
\part{Statistics}

\hypertarget{bibliography-5}{%
\section*{References}\label{bibliography-5}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-5}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-adashi2018}{}}%
Adashi, Eli Y, LeRoy B Walters, and Jerry A Menikoff. 2018. {``The
Belmont Report at 40: Reckoning with Time.''} \emph{American Journal of
Public Health} 108 (10): 1345--48.

\leavevmode\vadjust pre{\hypertarget{ref-apaethics}{}}%
Association, American Psychological. 2022. {``Ethical Principles of
Psychologists and Code of Conduct.''} 2022.
\url{https://www.apa.org/ethics/code}.

\leavevmode\vadjust pre{\hypertarget{ref-baade1961}{}}%
Baade, Hans W. 1961. {``The Eichmann Trial: Some Legal Aspects.''}
\emph{Duke LJ}, 400.

\leavevmode\vadjust pre{\hypertarget{ref-baumrind1964}{}}%
Baumrind, Diana. 1964. {``Some Thoughts on Ethics of Research: After
Reading Milgram's" Behavioral Study of Obedience.".''} \emph{American
Psychologist} 19 (6): 421.

\leavevmode\vadjust pre{\hypertarget{ref-baumrind1985}{}}%
---------. 1985. {``Research Using Intentional Deception: Ethical Issues
Revisited.''} \emph{American Psychologist} 40 (2): 165.

\leavevmode\vadjust pre{\hypertarget{ref-beauchamp2001}{}}%
Beauchamp, Tom L, James F Childress, et al. 2001. \emph{Principles of
Biomedical Ethics}. Oxford University Press, USA.

\leavevmode\vadjust pre{\hypertarget{ref-biagetti2020}{}}%
Biagetti, Maria Teresa, Aldis Gedutis, and Lai Ma. 2020. {``Ethical
Theories in Research Evaluation: An Exploratory Approach.''}
\emph{Scholarly Assessment Reports}.

\leavevmode\vadjust pre{\hypertarget{ref-boser2007}{}}%
Boser, Susan. 2007. {``Power, Ethics, and the IRB: Dissonance over Human
Participant Review of Participatory Research.''} \emph{Qualitative
Inquiry} 13 (8): 1060--74.

\leavevmode\vadjust pre{\hypertarget{ref-brakewood2013}{}}%
Brakewood, Beth, and Russell A Poldrack. 2013. {``The Ethics of
Secondary Data Analysis: Considering the Application of Belmont
Principles to the Sharing of Neuroimaging Data.''} \emph{Neuroimage} 82:
671--76.

\leavevmode\vadjust pre{\hypertarget{ref-brandt1978}{}}%
Brandt, Allan M. 1978. {``Racism and Research: The Case of the Tuskegee
Syphilis Study.''} \emph{Hastings Center Report}, 21--29.

\leavevmode\vadjust pre{\hypertarget{ref-burger2007}{}}%
Burger, Jerry. 2007. {``Replicating Milgram.''} \emph{APS Observer} 20
(11).

\leavevmode\vadjust pre{\hypertarget{ref-eysenbach2006}{}}%
Eysenbach, Gunther. 2006. {``Citation Advantage of Open Access
Articles.''} \emph{PLoS Biology} 4 (5): e157.

\leavevmode\vadjust pre{\hypertarget{ref-flinders1992}{}}%
Flinders, David J. 1992. {``In Search of Ethical Guidance: Constructing
a Basis for Dialogue.''} \emph{International Journal of Qualitative
Studies in Education} 5 (2): 101--15.

\leavevmode\vadjust pre{\hypertarget{ref-gargouri2010}{}}%
Gargouri, Yassine, Chawki Hajjem, Vincent Larivire, Yves Gingras, Les
Carr, Tim Brody, and Stevan Harnad. 2010. {``Self-Selected or Mandated,
Open Access Increases Citation Impact for Higher Quality Research.''}
\emph{PloS One} 5 (10): e13636.

\leavevmode\vadjust pre{\hypertarget{ref-gordijn2012}{}}%
Gordijn, Ernestine H, and Diederik A Stapel. 2012. {``Behavioural
Effects of Automatic Interpersonal Versus Intergroup Social Comparison
(Retraction of Vol 45, Pg 717, 2006).''} \emph{BRITISH JOURNAL OF SOCIAL
PSYCHOLOGY} 51 (3): 498--98.

\leavevmode\vadjust pre{\hypertarget{ref-gorgolewski2016}{}}%
Gorgolewski, Krzysztof J., and Russell A. Poldrack. 2016. {``A Practical
Guide for Improving Transparency and Reproducibility in Neuroimaging
Research.''} \emph{{PLOS} Biology} 14 (7): e1002506.
\url{https://doi.org/10.1371/journal.pbio.1002506}.

\leavevmode\vadjust pre{\hypertarget{ref-katz2011}{}}%
Katz, Ralph V, and Rueben C Warren. 2011. \emph{The Search for the
Legacy of the USPHS Syphilis Study at Tuskegee}. Lexington Books.

\leavevmode\vadjust pre{\hypertarget{ref-kelman2017}{}}%
Kelman, Herbert C. 2017. {``Human Use of Human Subjects: The Problem of
Deception in Social Psychological Experiments.''} In \emph{Research
Design}, 189--204. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-kilham1974}{}}%
Kilham, Wesley, and Leon Mann. 1974. {``Level of Destructive Obedience
as a Function of Transmitter and Executant Roles in the Milgram
Obedience Paradigm.''} \emph{Journal of Personality and Social
Psychology} 29 (5): 696.

\leavevmode\vadjust pre{\hypertarget{ref-lemmens2000}{}}%
Lemmens, Trudo, and Benjamin Freedman. 2000. {``Ethics Review for Sale?
Conflict of Interest and Commercial Research Review Boards.''} \emph{The
Milbank Quarterly} 78 (4): 547--84.

\leavevmode\vadjust pre{\hypertarget{ref-mahoney1943}{}}%
Mahoney, John F, RC Arnold, and AD Harris. 1943. {``Penicillin Treatment
of Early Syphilis---a Preliminary Report.''} \emph{American Journal of
Public Health and the Nations Health} 33 (12): 1387--91.

\leavevmode\vadjust pre{\hypertarget{ref-merton1979}{}}%
Merton, Robert K. 1979. {``The Normative Structure of Science.''}
\emph{The Sociology of Science: Theoretical and Empirical
Investigations}, 267--78.

\leavevmode\vadjust pre{\hypertarget{ref-milgram1963}{}}%
Milgram, Stanley. 1963. {``Behavioral Study of Obedience.''} \emph{The
Journal of Abnormal and Social Psychology} 67 (4): 371.

\leavevmode\vadjust pre{\hypertarget{ref-milgram1974}{}}%
---------. 1974. \emph{Obedience to Authority: An Experimental View}.
Harper \& Row.

\leavevmode\vadjust pre{\hypertarget{ref-mill1859}{}}%
Mill, John Stuart. 1859. {``Utilitarianism (1863).''}
\emph{Utilitarianism, Liberty, Representative Government}, 7--9.

\leavevmode\vadjust pre{\hypertarget{ref-miller2009}{}}%
Miller, Arthur G. 2009. {``Reflections on" Replicating Milgram"(burger,
2009).''}

\leavevmode\vadjust pre{\hypertarget{ref-nosek2015}{}}%
Nosek, Brian A, G. Alter, G. C. Banks, D. Borsboom, S. D. Bowman, S. J.
Breckler, S. Buck, et al. 2015. {``Promoting an Open Research
Culture.''} \emph{Science} 348 (6242): 1422--25.
\url{https://doi.org/10.1126/science.aab2374}.

\leavevmode\vadjust pre{\hypertarget{ref-oakes2002}{}}%
Oakes, J Michael. 2002. {``Risks and Wrongs in Social Science Research:
An Evaluator's Guide to the IRB.''} \emph{Evaluation Review} 26 (5):
443--79.

\leavevmode\vadjust pre{\hypertarget{ref-perry2020}{}}%
Perry, Gina, Augustine Brannigan, Richard A Wanner, and Henderikus Stam.
2020. {``Credibility and Incredulity in Milgram's Obedience Experiments:
A Reanalysis of an Unpublished Test.''} \emph{Social Psychology
Quarterly} 83 (1): 88--106.

\leavevmode\vadjust pre{\hypertarget{ref-ross2011}{}}%
Ross, Lee, and Richard E Nisbett. 2011. \emph{The Person and the
Situation: Perspectives of Social Psychology}. Pinter \& Martin
Publishers.

\leavevmode\vadjust pre{\hypertarget{ref-shatz1986}{}}%
Shatz, David. 1986. {``Autonomy, Beneficence, and Informed Consent:
Rethinking the Connections.''} \emph{Cancer Investigation} 4 (3):
257--69.

\leavevmode\vadjust pre{\hypertarget{ref-stapel2012b}{}}%
Stapel, Diederik A. 2012. \emph{Ontsporing}. Prometheus Amsterdam.

\leavevmode\vadjust pre{\hypertarget{ref-stapel2012}{}}%
Stapel, Diederik A, and Lonneke AJG van der Linde. 2012. {``"What Drives
Self-Affirmation Effects? On the Importance of Differentiating Value
Affirmation and Attribute Affirmation": Retraction of Stapel and van Der
Linde (2011).''}

\leavevmode\vadjust pre{\hypertarget{ref-trampe2011}{}}%
Trampe, Debra, Diederik A Stapel, and Frans W Siero. 2011. {``Retracted:
The Self-Activation Effect of Advertisements: Ads Can Affect Whether and
How Consumers Think about the Self.''} \emph{Journal of Consumer
Research} 37 (6): 1030--45.

\leavevmode\vadjust pre{\hypertarget{ref-white2006}{}}%
White, Robert M. 2006. {``Effects of Untreated Syphilis in the Negro
Male, 1932 to 1972: A Closure Comes to the Tuskegee Study, 2004.''}
\emph{Urology} 67 (3): 654.

\end{CSLReferences}

\hypertarget{sec-estimation}{%
\chapter{Estimation}\label{sec-estimation}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Estimate the causal effect of a manipulation
\item
  Discuss differences between frequentist and Bayesian estimation
\item
  Reason about standardized effect sizes and their strengths and
  weaknesses
\end{itemize}

\end{tcolorbox}

\begin{quote}
``In every quantitative paper we read, every quantitative talk we
attend, and every quantitative article we write, we should all ask one
question: \emph{what is the estimand}? The estimand is the object of
inquiry -- it is the precise quantity about which we marshal data to
draw an inference. Yet, too often social scientists skip the step of
defining the estimand. Instead, they leap straight to describing the
data they analyze and the statistical procedures they apply. Without a
statement of the estimand, it becomes impossible for the reader to know
whether those procedures were appropriate.''
(\protect\hyperlink{ref-lundberg2021}{Lundberg, Johnson, and Stewart
2021})
\end{quote}

In the first section of this book, our goal was to set up some of the
theoretical ideas that motivate our approach to experimental design and
planning. We introduced our key thesis, namely that experiments are
about measuring causal effects. We also began to discuss some of our key
themes, including precision of measurement, reduction of bias, and
generalization across populations.

In this next section of the book -- treating statistical topics -- we
will integrate these ideas with an analytic toolkit for
\textbf{estimating} effects, \textbf{quantifying the size and precision}
of these estimates (this chapter), making \textbf{inferences} about the
evidence for such effects (\textbf{?@sec-inference}), and building
\textbf{models} for estimation and inference in more complex settings
(\textbf{?@sec-models}). Although this book does not provide an
extensive treatment of statistics, we hope that these chapters provide
some practical foundations for beginning the statistical analysis of
your experimental data.

\begin{tcolorbox}[colframe=.blue, title=\faMicroscope \enspace Case study]

\hypertarget{the-lady-tasting-tea}{%
\section*{The Lady Tasting Tea}\label{the-lady-tasting-tea}}
\addcontentsline{toc}{section}{The Lady Tasting Tea}

\markright{The Lady Tasting Tea}

The birth of modern statistical inference arose from the age old
conundrum of how to best make a cup of tea. Sir Ronald Fisher was
apparently at a party when a lady declared that she could tell the
difference when tea was added to milk vs.~milk to tea. Rather than
taking her at her word, Fisher devised an experimental and data analysis
procedure to test her claim.

The basic schema of the experiment was that the lady would have to judge
a set of new cups of tea and sort them into milk-first vs.~tea-first
sets. Her data would then be analyzed to determine whether her level of
correct choice exceeded that expected by chance. While this process now
sounds like a quotidian experiment that might be done on a cooking
reality show, it seems unremarkable in hindsight only because it set the
standard for the way science was done going forward.

The important and unusual element of the experiment was its treatment of
potential design confounds such which cup of tea was prepared first,
which cup of tea was presented first, or the material that the cups were
made out of. Prior experimental practice would have been to try to
equate all of the cups as closely as possible, decreasing the influence
of confounds. Fisher recognized that this strategy was insufficient
because of the presence of unobserved confounders. Only by randomizing
all other aspects of the experiment could he make strong causal
inferences about the treatment (milk then tea vs.~tea then milk). We
discussed the causal power of random assignment in
\textbf{?@sec-experiments} -- this experiment is a key touchstone in the
popularization of randomized experiments!

\end{tcolorbox}

\hypertarget{estimating-a-quantity}{%
\section{Estimating a quantity}\label{estimating-a-quantity}}

{\marginnote{\begin{footnotesize}An important piece of context for the
work of Ronald Fisher, Karl Pearson, and other early pioneers of
statistical inference is that they were all strong proponents of
eugenics. Fisher was the founding Chairman of the Cambridge Eugenics
Society. Pearson was perhaps even worse, an avowed Social Darwinist who
believed fervently in Eugenic legislation. These views are
repugnant.\end{footnotesize}}}

If experiments are about estimating effects, how do we actually use our
experimental data to make these estimates? For our example we'll design
a slightly more modern version of Fisher's experiment, shown in
Figure~\ref{fig-estimation-expt}.

\begin{figure}

\sidecaption{\label{fig-estimation-expt}The structure of our tea tasting
experiment.}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{images/estimation/tea-expt.png}

}

\end{figure}

Our causal theory is that the tea quality is affected by milk-tea
ordering, so we'll test that by rating tea quality both milk-first and
tea-first, represented by a DAG like the one in
Figure~\ref{fig-estimation-dag}. Our intended population to generalize
to is the set of all tea drinkers, and towards that goal we sample a set
of tea-drinkers. In practice, we might do a field trial in a cafe in
which we approach patrons and ask them to participate in our experiment
in exchange for a free cup of tea. Although this sample size is almost
certainly too small to get precise estimates, for the purpose of this
example, we'll sample 18 tea drinkers -- nine in each condition.

\begin{marginfigure}

{\centering \includegraphics[width=0.3\textwidth,height=\textheight]{images/estimation/tea-dag.png}

}

\caption{\label{fig-estimation-dag}A directed acyclic graph representing
our causal theory of tea quality.}

\end{marginfigure}

As our manipulation, we follow Fisher in randomly assigning participants
(who of course should give consent to participate) into to one of our
two conditions: milk-first and tea-first.\sidenote{\footnotesize Technically,
  randomized experiments were not invented by Fisher. Perhaps the
  earliest example of a (somewhat) randomized experiment was a trial of
  scurvy treatments in the 1700s (\protect\hyperlink{ref-dunn1997}{Dunn
  1997}). Peirce and Jastrow (\protect\hyperlink{ref-peirce1884}{1884})
  also report a strikingly modern use of randomized stimulus
  presentation (via shuffling cards). Nevertheless, Fisher's statistical
  work popularized randomized experiments throughout the sciences, in
  part by integrating them with a set of analytic methods.} This design
is a between-participants design (so each participant gets only one cup
of tea). They receive their cup of tea and taste it. Then as our
measure, we ask for a rating of the tea on a continuous scale from 1
(terrible) to 7 (delicious).\sidenote{\footnotesize Right now we're going to assume
  that our ratings are just simple numerical values and not worry about
  the fact that they come from a rating scale that is bounded (e.g.,
  can't go above 7). If you're curious about \textbf{Likert scales} (the
  name for discrete numerical rating scales), we'll talk a bit more
  about them in \textbf{?@sec-measurement}.}

\begin{marginfigure}

{\centering \includegraphics{images/estimation/data.png}

}

\caption{\label{fig-estimation-data}Schematic data from the tea tasting
experiment.}

\end{marginfigure}

A sample dataset from our experiment is shown in
Figure~\ref{fig-estimation-data}. Eventually, we'll want to estimate the
effect of milk-first preparation on quality ratings (our effect of
interest). But for now, our goal will be to estimate the quality of the
tea when it is milk-first (the better way, according to the real data;
Kennedy (\protect\hyperlink{ref-kennedy2003}{2003})). More formally, we
want to use our \textbf{sample} of 9 milk-first tea judgments to
estimate a number that we can't directly observe, namely the true
perceived quality of all possible milk-first cups. We'll call this
number a \textbf{population parameter} for reasons that will become
clear in a moment.

We'll try to go easy on notation but some amount will hopefully make
things clearer. We will use \(\theta_{\textrm{M}}\) (``theta'') to
denote the parameter we want to estimate (the \textbf{population
parameter}) and \(\widehat{\theta}_{\textrm{M}}\), its \textbf{sample
estimate}.\sidenote{\footnotesize Statisticians use ``hats'' like this to denote
  estimates from a specific sample. One way to remember this is that the
  ``person in the hat'' is wearing a hat to dress up as the actual
  quantity. Feel free to ignore this mnemonic; it helps us.}

\hypertarget{maximum-likelihood-estimation}{%
\subsection{Maximum likelihood
estimation}\label{maximum-likelihood-estimation}}

OK, you are probably saying, if we want our estimate of milk-first
quality, shouldn't we just take the average rating across the 9 cups of
milk-first tea? The answer is yes. But let's unpack that choice: taking
the sample mean as our estimate \(\widehat{\theta}_{\textrm{M}}\) is an
example of an estimation approach called \textbf{maximum likelihood
estimation}. In general terms, maximum likelihood estimation is a
two-step process.

First, we assume a \textbf{model} for how the data were
generated.\sidenote{\footnotesize This sense of ``model'' is actually a formal
  instantiation of the type of causal model we discussed in
  \textbf{?@sec-experiments}. As you get deeper into causal modeling,
  typically what you do is define a causal ``story'' for the statistical
  process that generated a dataset, using both DAGs and the kinds of
  probability distributions we define below. We'll come back to this
  idea in \textbf{?@sec-models}.} This model is specified in terms of
certain population parameters. In our example, the model is as simple as
they come: we just assume there is some average level of tea quality and
that the measurements vary around it.

\begin{marginfigure}

{\centering \includegraphics{images/estimation/maximum-likelihood.png}

}

\caption{\label{fig-estimation-ml}The best-fitting normal distribution
for data from the milk-first condition.}

\end{marginfigure}

Let's take a look at the data from the milk-first condition, shown in
Figure~\ref{fig-estimation-ml}). Our observations are clustered around
the mean, but they also show some variation. Some are higher and some
are lower. Variation of this type is a feature of every data set. This
variation can be summarized via a \textbf{probability distribution}, a
mathematical entity that describes the properties of possible datasets.

The only probability distribution we'll discuss here is the ubiquitous
\textbf{normal distribution} (also sometimes called a ``Gaussian
distribution''). A normal distribution has two \textbf{parameters}
(numbers that define its shape), a \textbf{mean} and a \textbf{standard
deviation}. These two parameters define the shape of the curve. The mean
describes where its center goes, and the standard deviation describes
how wide it is.\sidenote{\footnotesize Now we can see that the population parameter
  that we are trying to estimate is a number that describes the shape of
  the population distribution.} Using a probability distribution to
describe our dataset gives us a way of summarizing our observations
through the parameters of the distribution and encoding an assumption
about what future observations might look like.

How do we fit a normal distribution to our data? We try to find the
values of the population parameters that make our observed data as
likely as possible. Let's start with the mean.

\begin{marginfigure}

{\centering \includegraphics{images/estimation/maximum-likelihood2.png}

}

\caption{\label{fig-estimation-ml2}Comparison of the best-fitting normal
distribution and a substantially worse curve.}

\end{marginfigure}

For example, if our sample mean is
\(\widehat{\theta}_{\textrm{M}} = 4.5\), what underlying value of
\(\theta_{\textrm{M}}\) would make these data most likely to occur?
Well, suppose the underlying parameter were \(\theta_{\textrm{M}}=2.5\).
Then it would be pretty unlikely that our sample mean would be so much
bigger. So \(\widehat{\theta}_{\textrm{M}}=2.5\) is a poor estimate of
the population parameter based on these data
(Figure~\ref{fig-estimation-ml2}). Conversely, if the parameter were
\(\theta_{\textrm{M}}=6.5\), it would be a bit unlikely that our sample
mean would be so much \emph{smaller}. The value of
\(\widehat{\theta}_{\textrm{M}}\) that makes these data most likely is
just 4.5 itself: the sample mean! That is why the sample mean in this
case is the maximum likelihood estimate.

The second parameter of the normal distribution is the standard
deviation. The standard deviation is just exactly what its name says:
the average deviation between the mean and any given observation. We
won't discuss how to estimate the standard deviation for your sample
here; you can do this computation easily in any software package. What's
important for now is just that the standard deviation gives us a way to
describe the width of the normal distribution. Together, the mean and
standard deviation fully describe a particular normal distribution,
allowing us to summarise our knowledge about the data using just two
parameters.

\hypertarget{bayesian-estimation}{%
\subsection{Bayesian estimation}\label{bayesian-estimation}}

The maximum likelihood estimation example above describes a common
approach to estimating parameters, where the researcher completely puts
aside their prior expectations about what these values might be. This
approach is an example of a \textbf{frequentist} statistical approach,
an approach that focuses on the long-run performance of estimation
procedures.

Often this approach makes sense, especially when we have no prior
expectations about the values we are estimating. But sometimes we
\emph{do} have relevant beliefs about the value. For example, before we
perform our tea experiment, we don't know exactly what
\(\theta_{\textrm{M}}\) will be, but it seems a bit unlikely that tea
would be consistently rated as either horrible (1) or perfect (7). We
have what you might call \emph{weak prior expectations} about the kinds
of ratings we'll receive.

These kind of expectations are most useful when we have a very small
amount of data. Remember that our goal is to estimate a population
parameter using the sample data, and small data sets can be rather
noisy. Taking into account our prior expectations can help to temper the
influence of noise. For example, if our very first participant in the
experiment rated their tea as terrible, we wouldn't want to jump to the
conclusion that the tea was actually bad. Instead, we might speculate
that the participant was having a bad day or just brushed their teeth.
On the other hand, if all of our participants gave bad ratings to their
tea, the data would be more persuasive; in that case, we might want to
tell the cafe that they are serving substandard tea. The extent to which
our prior expectations should moderate our conclusions should vary with
the amount of sample data; with only a little data, our prior
expectations should have more influence, but as we gather more, we
should put greater weight on the data.

\begin{marginfigure}

{\centering \includegraphics{images/estimation/bayes.png}

}

\caption{\label{fig-estimation-bayes}Bayes rule, annotated.}

\end{marginfigure}

How do we quantify this tradeoff between our prior expectations and our
current observations? We can do this via \textbf{Bayesian estimation} of
\(\widehat{\theta}_{\textrm{M}}\). Bayesian estimation provides a
principled framework for integrating prior beliefs and data. These
estimation techniques can be very helpful in cases where data are sparse
or prior beliefs are strong.

In Bayesian estimation, we observe some data \(d\), consisting of the
set of responses in the experiment. Now we can use \textbf{Bayes' rule},
a tool from basic probability theory, to estimate this number
(Figure~\ref{fig-estimation-bayes}). Each part of this equation has a
name, and it's worth becoming familiar with them. The thing we want to
compute (\(p(\theta_{\textrm{M}} |\text{data})\)) is called the
\textbf{posterior probability} -- it tell us what we should believe
about the population parameter on tea quality, given the data we
observed.\sidenote{\footnotesize We're making the posterior \textcolor{purple}{purple}
  to indicate the combination of likelihood (\textcolor{red}{red}) and
  prior (\textcolor{blue}{blue}.}

The first part of the numerator is
\(p(\text{data}|\theta_{\textrm{M}})\), the probability of the data we
observed given our hypothesis about the participant's ability. This part
is called the \textbf{likelihood}.\sidenote{\footnotesize Speaking informally,
  ``likelihood'' is just a synonym for probability, but in Bayesian
  estimation, ``likelihood'' is a technical term specifically referring
  to probability of the data given our hypothesis. This ambiguity can
  get a bit confusing.} This term tells us about the relationship
between our hypothesis and the data we observed -- so if we think the
tea is of high quality (say \(\theta_{\textrm{M}} = 6.5\)) then the
probability of observing a bunch of low quality ratings will be fairly
low.

The second term in the numerator, \(p(\theta_{\textrm{M}} )\), is called
the \textbf{prior}. This term encodes our beliefs about the likely
distribution of tea quality. Intuitively, if we think that the tea is
likely of high quality, we should require more evidence to convince us
that it's bad. In contrast, if we think it's probably bad, a few
examples of low ratings might serve to convince us.

\begin{marginfigure}

{\centering \includegraphics{images/estimation/bayes-curves.png}

}

\caption{\label{fig-estimation-bayes-curves}Bayesian inference about tea
ratings with a strong prior on low values.}

\end{marginfigure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  gives an example of the combination of prior and data. In this
  example, we look at what difference the prior makes after observing 9
  ratings. If we go in assuming that the tea is likely to be bad, the
  posterior mean (purple line) will be pushed downward relative to the
  maximum likelihood estimate (red line).\sidenote{\footnotesize Note that this prior
    is operating only over on ratings -- estimates of tea quality. Later
    on when we talk about comparing milk-first and tea-first ratings to
    get an estimate of the experimental effect, we could consider
    putting a prior on tea \emph{discrimination} (e.g., the experimental
    effect.}
\end{enumerate}

\begin{marginfigure}

{\centering \includegraphics{images/estimation/bayes-curves-weak.png}

}

\caption{\label{fig-estimation-bayes-curves-weak}Bayesian inference
about tea ratings with a weak prior on low values.}

\end{marginfigure}

Priors aren't usually as strong as the one shown above.
Figure~\ref{fig-estimation-bayes-curves-weak}) shows how the picture
shifts when we have a weaker prior reflecting a flatter, more widely
spread belief about the distribution of ratings. Now the posterior mean
(purple) is closer to the maximum likelihood mean (red. This situation
is more common -- the prior encodes a weak assumption that ratings won't
cluster around the ends of the scale.

\begin{marginfigure}

{\centering \includegraphics{images/estimation/bayes-curves-moredata.png}

}

\caption{\label{fig-estimation-bayes-curves-moredata}Bayesian inference
about tea ratings with a a strong prior on low values and more data.}

\end{marginfigure}

The effect of the prior is also decreased when you have more data. Take
a look at Figure~\ref{fig-estimation-bayes-curves-moredata}). The prior
is the same as in 1, but we have more data. As a result, the posterior
distribution is much more peaked and also much closer to the data -- the
prior makes much less difference.

Bayesian estimation is most important when you have strong beliefs and
not a lot of data. That can be a case where you have just a few
participants in your experiment, but it's also good -- and perhaps more
common -- to use Bayesian methods when you have a lot of data, but maybe
not that much data about particular units that you care about. For
example, you might have a large dataset about the effects of an
educational intervention but not that much data about how it affects a
particular subgroup. In general, though, Bayesian estimates and maximum
likelihood estimates will exactly coincide either under a flat prior (a
prior that makes any value equally likely) or as you gather infinite
data.

\hypertarget{estimating-and-comparing-effects}{%
\section{Estimating and comparing
effects}\label{estimating-and-comparing-effects}}

We've now covered estimating a single parameter (the mean for people who
had milk-first tea) using both frequentist and Bayesian methods. But
recall that what we really wanted to do was to estimate the causal
effect we were interested in, namely the milk-first vs.~tea-first
effect.\sidenote{\footnotesize This method doesn't have to be used only with a causal
  effect, it can be any between-group difference. In the current
  example, we can say with certainty that this effect is a causal
  because our experiment uses random assignment.} In this section, we'll
discuss how to estimate the effect, and then how to use \textbf{effect
size} measures to compare effects across experiments (as well as some of
the pros and cons of doing so).

\hypertarget{estimating-the-treatment-effect}{%
\subsection{Estimating the treatment
effect}\label{estimating-the-treatment-effect}}

Let's refer to the causal effect we care about as our \textbf{treatment
effect}.\sidenote{\footnotesize This is the effect of our manipulation -- what we
  sometimes call an ``intervention'' as well. ``Treatment'' is a term
  that comes from medical statistics but is used more broadly in
  statistics now.} In practice, estimating \(\beta\) (a parameter
describing the treatment effect) is going to be a pretty straightforward
extension to what we did before.

In the maximum likelihood framework, we could posit that ratings in each
group (milk-first and tea-first) follow a normal distribution, but that
these normal distributions might have different means and standard
deviations. Extending the notation introduced above, let's term the
parameters for the tea-first group \(\theta_{\textrm{T}}\) (the mean)
and \(\sigma\) (the standard deviation). To estimate the treatment
effect, we are positing a \textbf{model} in which the milk-first ratings
are normally distributed with mean
\(\theta_{\textrm{M}} = \theta_{\textrm{T}} + \beta\) and with standard
deviation \(\sigma\).\sidenote{\footnotesize For simplicity, we're assuming that the
  standard deviations in each tea group are equal.} This equation says
that milk-first ratings have the same distribution as tea-first ratings,
except that their average is shifted by \(\beta\). Setting our model up
this way then lets us compute \(\hat{\beta}\), our estimate of the
treatment effect in our sample.

\begin{marginfigure}

{\centering \includegraphics{images/estimation/data-difference.png}

}

\caption{\label{fig-estimation-data-difference}Estimating the average
treatment effect from the tea-tasting data.}

\end{marginfigure}

As in the one-sample case (i.e., estimating the mean of just the
milk-first group), maximum likelihood estimation would then proceed by
finding the value of \(\beta\) that makes the data most likely under the
assumed model. As you'd probably expect, this estimate
\(\widehat{\beta}\) turns out to be simply the difference in sample
means,
\(\widehat{\theta}_{\textrm{M}} - \widehat{\theta}_{\textrm{T}}\). You
can see this difference pictured in
Figure~\ref{fig-estimation-data-difference}.

In the Bayesian framework, we would again specify a prior \(p(\beta)\)
that encodes our prior beliefs about the size and direction of the
treatment effect. If we have no prior beliefs at all, then we could
specify a flat prior, \(p(\beta) \propto 1\).\sidenote{\footnotesize This equation
  says that the probability of any value of \(\beta\) is ``proportional
  to'' 1, meaning that it's constant (``flat'') regardless of what value
  \(\beta\) takes.}. If we believe the treatment effect is likely to
favor milk-first pouring (\(\beta>0\)), we could specify the prior is a
normal distribution centered at some positive value (e.g.,
\(\beta=0.5\)); the standard deviation of this prior would encode how
certain we are about our prior beliefs. And if we have no prior beliefs
about the direction of the treatment effect, but we think it is unlikely
to be very large, we could specify a normal prior centered at 0, which
has the effect of ``shrinking'' the estimates closer to 0.\sidenote{\footnotesize The
  measures of variability that we discuss here account for statistical
  uncertainty reflecting the fact that we have only a finite sample
  size. If the sample size were infinite, there would be no uncertainty
  of this kind. Statistical uncertainty is only one kind of uncertainty,
  though. A more holistic view of the overall credibility of an estimate
  should also account for other things outside of the model, like study
  design issues and bias.}

As in our example above, maximum likelihood estimates and Bayesian
estimates are going to be pretty similar if we have a lot of data or
weak priors. They will only diverge when we have strong priors or
relatively little data. The reason we are setting up these two different
frameworks, however, is that they will provide very different
inferential tools in the next chapter.

\hypertarget{measures-of-effect-size}{%
\subsection{Measures of effect size}\label{measures-of-effect-size}}

Once we have measured something, we need to make a decision about how to
describe this effect to others. Sometimes we are working with fairly
intuitive relationships that are easy to describe. A researcher might
say, for example, that people who received milk-first tea drank the tea,
on average, 5 minutes quicker than people who received tea-first tea
(i.e., that \(\widehat{\beta} = 5\) minutes). Time is measured in units
like minutes and seconds and so we all have a shared understanding of
what 5 minutes means.

But what about our participants' ratings of tea quality, which were
provided on an arbitrary 7-point rating scale that we devised? What does
it mean to that participants who drank milk-first tea rated it 1 point
higher than participants who drank tea-first tea (i.e., that
\(\widehat{\beta} = 1\) point)? And how is this difference comparable
to, for instance, a 1-point change on a scale that has similar anchors
(``terrible'' and ``delicious'') but uses a 100-point rating system?

\begin{figure}

\sidecaption{\label{fig-estimation-es-calc}Schematic effect size
computation.}

{\centering \includegraphics{images/estimation/es-calc.png}

}

\end{figure}

To provide a common language for describing these relationships, some
researchers use \emph{standardized effect sizes.} A common standardized
effect size is Cohen's \emph{d}, which provides a standardized estimate
of the difference between two means. There are many different ways to
calculate Cohen's \emph{d} (\protect\hyperlink{ref-lakens2013}{Lakens
2013}), but all approaches are usually some variant of the following
formula:

\[
d = \frac{\theta_{\textrm{M}} - \theta_{\textrm{T}}}{\sigma_{\text{pooled}}}
\]

\noindent where the difference between means (\(\theta_{\textrm{T}}\)
and \(\theta_{\textrm{M}}\)) is divided by the pooled standard deviation
\(\sigma_{\text{pooled}}\). Intuitively, what you're doing is taking the
study effect (\(\beta\)) and dividing it -- scaling it -- by the
variation we saw between individuals in the study.

Let's compute this measure for our tea-drinking study. We can just plug
in the estimates we see in Figure~\ref{fig-estimation-data-difference}
and compute the standard deviation of our observed data:

\[\widehat{d} = \frac{\widehat{\theta}_{\textrm{M}} - \widehat{\theta}_{\textrm{T}}}{\widehat{\sigma}_{\text{pooled}}} = \frac{4.5- 3.5}{1.25} = \frac{1}{1.25} = 0.80\]
\noindent In other words, the effect size of the difference between the
two conditions is .8 standard deviations.

We previously said that people who drank milk-first tea had quality
ratings that were, on average, 1 point higher on a 7-point scale
(\(\beta = 1\) point). Cohen's \emph{d} translates the arbitrary units
of our rating scale into a \textbf{unit-less} effect size that is
measured in terms of the variation in the data. You may find yourself
wondering: ``why would I ever describe things in terms of standard
deviations?'' The key benefit is that it allows us to compare the size
of the effect between studies that use different measures.

Let's say that we ran a replication of our tea study with two changes:
(1) we studied patrons in a US cafe instead of a UK cafe, and (2) we
used a 100-point quality rating scale instead of a 7-point scale.
Imagine that, just as we found that participants in the UK rated the
milk-first tea 1-point higher on a 7-point quality scale, US
participants rated the milk-first tea 1-point higher on a
\emph{100-point} quality scale. It seems clear that these effects are
different because of the difference in scale. But how different?

It might at first seem reasonable just to normalize by the length of the
scale. So maybe the UK experimental participants showed a 1/7 rating
effect and the US participants showed a 1/100 rating effect. The trouble
with this move is that it presupposes that participants from two
different populations are using two different scales in exactly the same
way! For example, maybe US participants made very clumpy judgments that
were mostly centered around 50 (perhaps because of a lack of milk tea
experience). Standardized effect sizes get around this kind of issue by
scaling according to the variability of the data.

Let's compute the effect size for the cross-cultural replication. We'll
imagine that participants who drank milk-first tea gave an average
rating of 50/100 and participants who drank tea-first tea rated it 49 on
average. But if their variability was also relatively lower, perhaps the
standard deviation of their ratings was only 5. Using the formula above,
we find

\[\widehat{d}_{US} = \frac{\widehat{\theta}_{\textrm{M}} - \widehat{\theta}_{\textrm{T}}}{\widehat{\sigma}_{\text{pooled}}} = \frac{50 - 49}{5} = \frac{1}{5}=  0.2\]

\noindent A Cohen's \emph{d} of .2 means that US cafe patrons rated
their tea .2 standard deviations higher when it was milk-first, much
smaller than the .8 standard deviation difference in the UK patrons.

There are no hard and fast rules for interpreting what makes a big
effect or a small effect, but people often refer back to a standard
suggested by Cohen (\protect\hyperlink{ref-cohen1992}{1992}). On those
standards, \(d = 0.8\) is a ``large effect'', and \(d = 0.2\) is a
``small effect.'' But these effect size interpretation norms are
somewhat arbitrary. More broadly, though US and UK patrons had the same
raw score change in quality ratings (\(\widehat{\beta} = 1\)) and
standardizing the differences allowed us to communicate that the
difference was much larger among the UK patrons.

Cohen's \emph{d} is one of many standardized effect sizes that
researchers can use. Just as Cohen's \emph{d} standardizes differences
in group means, there are also generalizations that allow for continuous
treatment variables or adjusted covariates (e.g., Pearson's \emph{r},
\(r^2\), or \(\eta^2\)). Other effect-size measures apply for
relationships between binary variables (e.g., odds ratio) and more.
We'll be using effect sizes throughout the book, but in most cases --
for example, sample size planning in \textbf{?@sec-sampling} and
meta-analysis in \textbf{?@sec-meta} -- we'll be using Cohen's \emph{d}
as our example.\sidenote{\footnotesize If you'd like to learn more about other
  varieties of effect size, take a look at Fritz, Morris, and Richler
  (\protect\hyperlink{ref-fritz2012}{2012}).}

\hypertarget{pros-and-cons-of-standardizing-effect-sizes}{%
\subsection{Pros and cons of standardizing effect
sizes}\label{pros-and-cons-of-standardizing-effect-sizes}}

There are some pros and cons of standardizing effect sizes. Sure, it
helps communicate that a 1-point change on a 7-point scale is not the
same as a 1-point change on a 100-point scale. But is it any better to
say that the first change represents a 0.80 standard deviation
difference and the second a 0.08 standard deviation difference?

Effect sizes allow us to compare results across studies more easily.
Across studies, researchers use different measures, different study
designs, and different populations. Standardization gives us a ``common
language'' to describe estimated relationships in these varied contexts.
This language is helpful when we want to aggregate and compare effects
across studies via meta-analysis. And it is also helpful when planning
new studies. When trying to figure out how many participants to run in a
study, almost all techniques for sample size planning use standardized
effect sizes to determine how much data would be needed to reliably
detect an effect.

Standardizing effect sizes has limitations, though. For example, if two
interventions produce the same absolute change in the same outcome
measure, but are studied in different populations in which the
variability on the outcome differs substantially, the interventions
would produce different standardized mean differences.

Imagine we conducted our tea experiment again, but this time with
(decaf) tea, and focusing on children. Maybe milk-first tea tastes the
same amount better than tea-first tea for kids and for adults. But kids
are, as a rule, more variable in their responding than adults. This
higher level of variability would lead us to observe a smaller effect
size in kids vs.~adults. Recall that our UK adult SD was 1.25, and our
effect size was \(d = .8\). Imagine that children's SD is 2.5. In this
scenario, even if tea led to the same 1-point absolute change in ratings
among adults and children, the standardized effect size for kids would
look half as big:

\[\widehat{d}_{kids} = \frac{\widehat{\theta}_{\textrm{M}} - \widehat{\theta}_{\textrm{T}}}{\widehat{\sigma}_{\text{pooled}}} = \frac{5- 4}{2.5} = \frac{1}{2.5} =  .4\]

\noindent This example highlights some of the challenges with
standardization. If we focused on the fact that both adults and children
show a 1-point change in ratings levels (\(\widehat{\beta} = 1\)), we
would conclude that milk-first tea ordering is as much better for adults
as kids. If we focused on the standardized effect sizes, however, we
would conclude that the milk ordering effect is twice as big for adults.

So which is better: describing raw measures or standardized effect
sizes? In general our response is ``Why not both?'' But if you wanted to
pick one or the other, we recommend considering what type of measurement
you are using. With measures that yield common measurement units that
are likely to be reported in many studies already, use raw scores. For
example, if your study uses physical units such as milliseconds (e.g.,
for reaction times) or counts (e.g., for a study tracking an outcome
like number of words), these measurements can be quite useful to compare
across studies. Reporting raw measurements also can allow you to check
whether your measurements make sense -- for example, a reaction time of
70 milliseconds is inhumanly fast, while a reaction time of 10 seconds
might be extremely slow (at least, for many speeded tasks).

In contrast, we recommend using standardized effect sizes for cases
where the measurement is relatively unlikely to be comparable with other
studies in its original form, or unlikely to be meaningful on its own.
For example, reporting the effect of an intervention on raw math test
scores is only meaningful if the reader knows how many items are on the
test, how difficult it is, and so forth. In such a case where there it
is hard for a reader to be ``calibrated'' to the specific measurement
units you are using, standardized effect sizes may be the best way to
report your finding.

\hypertarget{chapter-summary-estimation}{%
\section{Chapter summary: Estimation}\label{chapter-summary-estimation}}

In this chapter, we introduced the idea of estimating both individual
measurements and treatment effects from observed data. These ideas are
simple but they lay the foundations for hypothesis testing and modeling
(our next two chapters). Further, we set up the distinction between
Bayesian and frequentist approaches, which we will expand in the next
chapter since these traditions provide different inferential tools.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  In this chapter you learned about estimation, and in this book more
  generally, we have argued that the goal of an experiment is to provide
  a maximally precise estimate of a causal effect. Psychology as a field
  has often been criticized for focusing too much on inference and too
  little on estimation. Find an article in the journal Psychological
  Science that reports on an experiment or series of experiments and
  read the abstract. Does it mention an estimate of any particular
  quantity? What might be the benefits of reporting estimates in the
  study abstract?
\item
  Try the same exercise with a paper in the \emph{New England Journal of
  Medicine} or \emph{Journal of the American Medical Association}. Find
  a paper and check if there is a mention of any specific quantity being
  estimated. (We suspect there will be!) Consider this contrast between
  the medical article and the psychology article. What do you make of
  this difference between fields?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\item
  A great narrative introduction to the history and practice of
  statistics: Salsburg, D. (2001). \emph{The lady tasting tea: How
  statistics revolutionized science in the twentieth century}.
  Macmillan.
\item
  An open source statistics textbook that follows a similar approach as
  Chapters \textbf{?@sec-estimation} -- \textbf{?@sec-models}: Poldrack,
  R. (2022). \emph{Statistical thinking for the 21st century}. Available
  free online at \url{https://statsthinking21.org}.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-7}{%
\section*{References}\label{bibliography-7}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-7}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-cohen1992}{}}%
Cohen, Jacob. 1992. {``A Power Primer.''} \emph{Psychological Bulletin}
112 (1): 155.

\leavevmode\vadjust pre{\hypertarget{ref-dunn1997}{}}%
Dunn, Peter M. 1997. {``James Lind (1716-94) of Edinburgh and the
Treatment of Scurvy.''} \emph{Archives of Disease in Childhood-Fetal and
Neonatal Edition} 76 (1): F64--65.

\leavevmode\vadjust pre{\hypertarget{ref-fritz2012}{}}%
Fritz, Catherine O, Peter E Morris, and Jennifer J Richler. 2012.
{``Effect Size Estimates: Current Use, Calculations, and
Interpretation.''} \emph{Journal of Experimental Psychology: General}
141 (1): 2.

\leavevmode\vadjust pre{\hypertarget{ref-kennedy2003}{}}%
Kennedy, Maeve. 2003. {``How to Make a Perfect Cuppa: Put Milk in
First.''} \emph{How to Make a Perfect Cuppa: Put Milk in First}.
\url{https://www.theguardian.com/uk/2003/jun/25/science.highereducation}.

\leavevmode\vadjust pre{\hypertarget{ref-lakens2013}{}}%
Lakens, Danil. 2013. {``Calculating and Reporting Effect Sizes to
Facilitate Cumulative Science: A Practical Primer for t-Tests and
ANOVAs.''} \emph{Frontiers in Psychology} 4: 863.

\leavevmode\vadjust pre{\hypertarget{ref-lundberg2021}{}}%
Lundberg, Ian, Rebecca Johnson, and Brandon M Stewart. 2021. {``What Is
Your Estimand? Defining the Target Quantity Connects Statistical
Evidence to Theory.''} \emph{American Sociological Review} 86 (3):
532--65.

\leavevmode\vadjust pre{\hypertarget{ref-peirce1884}{}}%
Peirce, Charles Sanders, and Joseph Jastrow. 1884. {``On Small
Differences in Sensation.''} \emph{Memoirs of the National Academy of
Sciences} 3.

\end{CSLReferences}

\hypertarget{sec-inference}{%
\chapter{Inference}\label{sec-inference}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Discuss the purpose of statistical inference
\item
  Define \emph{p}-values and Bayes Factors
\item
  Consider common fallacies about inference (especially for
  \emph{p}-values)
\item
  Reason about sampling variability
\item
  Define and reason about confidence intervals
\end{itemize}

\end{tcolorbox}

We've been arguing that experiments are about measuring effects. The
effects we are interested in are causal effects for a group of people,
but that group is almost always bigger than the participants in an
experiment. \textbf{Statistical inference} is the process of going
beyond the specific characteristics of the sample that you measured to
make generalizations about the broader \textbf{population}.
\textbf{?@sec-estimation} already showed us how to make one simple
inference: estimating population parameters using both frequentist and
Bayesian techniques.

Estimating population parameters is an important first step. But often
we want to make more sophisticated inferences so that we can answer
questions such as:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  How likely is it that this pattern of measurements was produced by
  chance variation?
\item
  Do these data provide more support for one hypothesis or another?
\item
  How precise is our estimate of an effect?
\item
  What portion of the variation in the data is due to a particular
  manipulation (as opposed to variation between participants, stimulus
  items, or other manipulations)?
\end{enumerate}

Question (1) is associated with one particular type of statistical
inference method -- \textbf{null hypothesis significance testing} (NHST)
in the \textbf{frequentist} statistical tradition. NHST has become
synonymous with data analysis, such that in the vast majority of
research papers (and research methods courses), all of the reported
analyses are tests of this type. Yet this equivalence is quite
problematic.

The move to ``go test for significance'' before visualizing your data
and trying to understand sources of variation (participants, items,
manipulations, etc.) is one of the most unhelpful strategies for an
experimenter. Whether \(p < .05\) or not, a test of this sort gives you
literally \emph{one bit} of information about your data.\sidenote{\footnotesize In the
  information theoretic sense, as well as the common sense!} Considering
effect sizes and their variation more holistically, including using the
kinds of visualizations we advocate in \textbf{?@sec-viz}, gives you a
much richer sense of what happened in your experiment!

In this chapter, we will describe NHST, the conventional method that
many students still learn (and many scientists still use) as their
primary method for engaging with data. All practicing experimentalists
need to understand NHST, both to read the literature and also to apply
this method in appropriate situations. For example, NHST may be a
reasonable tool for testing whether an intervention leads to a
difference between a treatment condition and an appropriate control,
although it still doesn't tell you about the size of the intervention
effect! But we will also try to contextualize NHST as a very special
case of a broader set of modeling and inference strategies. Further, we
will continue to flesh out our account of how some of the pathologies of
NHST have been a driver of the replication crisis.

\begin{marginfigure}

{\centering \includegraphics{images/inference/krushke2.png}

}

\caption{\label{fig-inference-krushke}Clarifying the distinctions
between Bayesian and Frequentist paradigms and the ways that they
approach inference and estimation. For many settings, we think the
estimation mindset is more useful. Adapted from Kruschke and Liddell
(\protect\hyperlink{ref-kruschke2018}{2018}).}

\end{marginfigure}

What should replace NHST? Figure~\ref{fig-inference-krushke}) shows one
way of organizing different inferential approaches. There has been a
recent move towards the use of Bayes Factors to quantify the evidence in
support of different candidate hypotheses. Bayes Factors can help answer
questions like (2. We introduce these tools, and believe that they have
broader applicability than the NHST framework and should be known by
students. On the other hand, Bayes Factors are not a panacea. They have
many of the same problems as NHST when they are applied dichotomously.

Instead of dichotomous frequentist or Bayesian inference, we advocate
for \textbf{estimation} and \textbf{modeling} strategies, which are more
suited towards questions (3) and (4)
(\protect\hyperlink{ref-cumming2014}{Cumming 2014};
\protect\hyperlink{ref-kruschke2018}{Kruschke and Liddell 2018}). The
goal of these strategies is to yield an accurate and precise estimate of
the relationships underlying observed variation in the data.

This isn't a statistics book and we won't attempt to teach the full
array of important statistical concepts that will allow students to
build good models of a broad array of datasets. (Sorry!).\sidenote{\footnotesize You
  can find a broad overview of statistical methods in Poldrack
  (\protect\hyperlink{ref-poldrack2023}{2023}), which is available
  freely online at
  \href{http://statsthinking21.org}{statsthinking21.org}. If you're
  interested in going deeper, here are two books that have been really
  influential for us. The first is Gelman and Hill
  (\protect\hyperlink{ref-gelman2006b}{2006}) and its successor Gelman,
  Hill, and Vehtari (\protect\hyperlink{ref-gelman2020}{2020}), which
  teach regression and multi-level modeling from the perspective of data
  description. The second is McElreath
  (\protect\hyperlink{ref-mcelreath2018}{2018}), a course on building
  Bayesian models of the causal structure of your data. Honestly,
  neither is an easy book to sit down and read (unless you are the kind
  of person who reads statistics books on the subway for fun) but both
  really reward detailed study. We encourage you to get together a
  reading group and go through the exercises together. It'll be well
  worth while in its impact on your statistical and scientific thinking.}
But we do want you to be able to reason about inference and modeling. In
this chapter, we'll start by making some inferences about our
tea-tasting example from the last chapter, using this example to build
up intuitions about inference and estimation. Then in
\textbf{?@sec-models}, we'll start to look at more sophisticated models
and how they can be fit to real datasets.

\begin{marginfigure}

{\centering \includegraphics{images/inference/sampling-small.png}

}

\caption{\label{fig-inference-sampling-small}Sampling distribution for
the treatment effect in the tea-tasting experiment, given many different
repetitions of the same experiment, each with N=9 per group. Circles
represent average treatment effects from different individual
experiments, while the thick line represents the form of the underlying
distribution.}

\end{marginfigure}

\hypertarget{sampling-variation}{%
\section{Sampling variation}\label{sampling-variation}}

In \textbf{?@sec-estimation}, we introduced Fisher's tea-tasting
experiment and discussed how to estimate means and differences in means
from our observed data. These so-called ``point estimates'' represent
our best guesses about the population parameters given the data and
possibly also given our prior beliefs. We can also report how much
statistical uncertainty is involved in these point estimates.\sidenote{\footnotesize The
  measures of variability that we discuss here account for statistical
  uncertainty, reflecting the fact that we have only a finite sample
  size. If the sample size were infinite, there would be no uncertainty
  of this kind. Sampling-based uncertainty is only one kind of
  uncertainty, though: a more holistic view of the overall credibility
  of an estimate should also account for study design and bias, among
  other things.} Quantifying and reasoning about this uncertainty is an
important goal: in our original study we only had 9 participants in each
group, which will only provide a low precision (i.e., highly uncertain)
estimate of the population. By contrast, if we repeated the experiment
with 200 participants in each group, the data would be far less noisy,
and we would have much less uncertainty, even if the point estimates
happened to be identical.

\hypertarget{standard-errors}{%
\subsection{Standard errors}\label{standard-errors}}

To characterize the uncertainty in an estimate, it helps to picture what
is called its \textbf{sampling distribution}, which is the distribution
of the estimate across different, hypothetical samples. That is, let's
imagine -- purely hypothetically -- that we conducted the tea experiment
not just once, but dozens, hundreds, or even thousands of times. This
idea is often called \textbf{repeated sampling} as a shorthand. For each
hypothetical sample, we use similar recruitment methods to recruit a new
sample of participants, and we compute \(\widehat{\beta}\) for that
sample. Would we get exactly the same answer each time? No, simply
because the samples will have some random variability (noise). If we
plotted these estimates, \(\widehat{\beta}\), we would get the sampling
distribution in Figure~\ref{fig-inference-sampling-small}.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

In this chapter and the subsequent statistics and visualization chapters
of the book, we'll try to facilitate understanding and illustrate how to
use these concepts in practice by giving the R code we use in
constructing our examples in these code boxes. We'll assume that you
have some knowledge of base R and the Tidyverse -- to get started with
these, go ahead and take a look at \textbf{?@sec-tidyverse} if you
haven't already. Although our figures are often drawn by hand, even the
hand-drawn ones are based on actual simulation results!

Since we're going to be working with lots of data from the tea tasting
example, we wrote a function called \texttt{make\_tea\_data()} that
creates a \texttt{tibble} with some (made up) data from our modern
tea-tasting experiment. You can find the function at
\href{https://raw.githubusercontent.com/langcog/experimentology/main/helper/tea_helper.R}{this
link} if you want to follow along.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tea\_data }\OtherTok{\textless{}{-}} \FunctionTok{make\_tea\_data}\NormalTok{(}\AttributeTok{n\_total =} \DecValTok{18}\NormalTok{)}

\FunctionTok{head}\NormalTok{(tea\_data)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tbl-tea-data}{}
\begin{longtable}[]{@{}lr@{}}
\caption{\label{tbl-tea-data}Part of the tea tasting example
data.}\tabularnewline
\toprule\noalign{}
condition & rating \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
condition & rating \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
milk first & 6 \\
milk first & 4 \\
milk first & 5 \\
milk first & 5 \\
milk first & 5 \\
milk first & 4 \\
\end{longtable}

\end{tcolorbox}

\begin{marginfigure}

{\centering \includegraphics{images/inference/sampling-big.png}

}

\caption{\label{fig-inference-sampling-big}Comparing sampling
distributions for the treatment effect with smaller and larger size
samples.}

\end{marginfigure}

Now imagine we also did thousands of repetitions of the experiment with
\(n=200\) per group instead of \(n=9\) per group.
Figure~\ref{fig-inference-sampling-big} shows what the sampling
distribution might look like in that case. Notice how much narrower the
sampling distribution becomes when we increase the sample size, showing
our decreased uncertainty. More formally, the standard deviation of the
sampling distribution itself, called the \textbf{standard error},
decreases as the sample size increases.

The sampling distribution is not the same thing as the distribution of
tea ratings in a single sample. Instead, it's a distribution of
\emph{estimates across samples of a given size}. In essence, it tells us
what the mean of a new experiment might be, if we ran it with a
particular sample size.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

To do simulations where we repeat the tea-tasting experiment over and
over again, we're using a special tidyverse function from the
\texttt{purrr} library: \texttt{map}. \texttt{map} is an extremely
powerful function that allows us to run another function (in this case,
the \texttt{make\_tea\_data} function that we introduced last chapter)
many times with different inputs. Here we create a tibble made up of a
set of 1000 runs of the \texttt{make\_tea\_data} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{samps }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{sim =} \DecValTok{1}\SpecialCharTok{:}\DecValTok{1000}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{map}\NormalTok{(sim, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{make\_tea\_data}\NormalTok{(}\AttributeTok{n\_total =} \DecValTok{18}\NormalTok{))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \StringTok{"data"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Next, we just use the \texttt{group\_by} and \texttt{summarise} workflow
from \textbf{?@sec-tidyverse} to get the estimated treatment effect for
each of these simulations.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tea\_summary }\OtherTok{\textless{}{-}}\NormalTok{ samps }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(sim) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{delta =} \FunctionTok{mean}\NormalTok{(rating[condition }\SpecialCharTok{==} \StringTok{"milk first"}\NormalTok{]) }\SpecialCharTok{{-}}
              \FunctionTok{mean}\NormalTok{(rating[condition }\SpecialCharTok{==} \StringTok{"tea first"}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

This tibble gives us what we would need to plot the sampling
distributions above in Figure~\ref{fig-inference-sampling-small} and
Figure~\ref{fig-inference-sampling-big}.

\end{tcolorbox}

\hypertarget{the-central-limit-theorem}{%
\subsection{The central limit theorem}\label{the-central-limit-theorem}}

\begin{marginfigure}

{\centering \includegraphics{images/inference/cl-2.png}

}

\caption{\label{fig-inference-coin-2}Samping distribution of samples
from a biased coin with N=2 flips in each sample. Bar height shows the
proportion of flips resulting in a particular mean.}

\end{marginfigure}

We talked in the last chapter about the normal distribution, a
convenient and ubiquitous tool for quantifying the distribution of
measurements. A shocking thing about sampling distributions for many
kinds of estimates -- and for \emph{all} maximum likelihood estimates --
is that they become normally distributed as the sample size gets larger
and larger. This result holds even for estimates that are not even
remotely normally distributed in small samples!

For example, say we are flipping a coin and we want to estimate the
probability that it lands heads (\(p_H\)). If we draw samples each
consisting of only \(n=2\) coin flips, Figure~\ref{fig-inference-coin-2}
is the sampling distribution of the estimates (\(\widehat{p}_H\). This
sampling distribution doesn't look normally distributed at all -- it
doesn't have the characteristic ``bell curve'' shape! In a sample of
only two coin flips, \(\widehat{p}_H\) can only take on the values 0,
0.5, or 1.

\begin{marginfigure}

{\centering \includegraphics{images/inference/cl-ns.png}

}

\caption{\label{fig-inference-coin-ns}Sampling distribution for 2, 8,
32, and 128 flips.}

\end{marginfigure}

But look what happens as we draw increasingly larger samples in
Figure~\ref{fig-inference-coin-ns}: We get a normal distribution! This
tendency of sampling distributions to become normal as \(n\) becomes
very large reflects a deep and elegant mathematical law called the
\textbf{Central Limit Theorem}.

The practical upshot is that the Central Limit Theorem directly helps us
characterize the uncertainty of sample estimates. For example, when the
sample size is reasonably large (approximately \(n>30\) in the case of
sample means) the standard error (i.e., the standard deviation of the
sampling distribution) of a sample mean is approximately
\(\widehat{SE} = \sigma/\sqrt{n}\). The sampling distribution becomes
narrower as the sample size increases because we are dividing by the
square root of the number of observations.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

For our central limit theorem simulations, we again use the \texttt{map}
function. We set up a tibble with the different values we want to to try
(which we call \texttt{n\_flips}). Then we make use of the \texttt{map}
function to run \texttt{rbinom} (random binomial samples) for each value
of \texttt{n\_flips}.

One trick we make use of here is that \texttt{rbinom} takes an extra
argument that says how many of these random values you want to generate.
Here we generate \texttt{nsamps\ =\ 1000} samples, giving us 1000
independent replicates at each \texttt{n}. But returning an array of
1000 values for a single value of \texttt{n\_flips} results in something
odd: the value for each element of \texttt{flips} is an array. To deal
with that, we use the \texttt{unnest} function, which expands the array
back into a normal tibble.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_samps }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{n\_flips\_list }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{32}\NormalTok{, }\DecValTok{128}\NormalTok{)}

\NormalTok{sample\_p }\OtherTok{\textless{}{-}} \FunctionTok{tibble}\NormalTok{(}\AttributeTok{n\_flips =}\NormalTok{ n\_flips\_list) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(n\_flips) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{flips =} \FunctionTok{map}\NormalTok{(n\_flips, }
                     \SpecialCharTok{\textasciitilde{}}\FunctionTok{rbinom}\NormalTok{(}\AttributeTok{n =}\NormalTok{ n\_samps, }\AttributeTok{size =}\NormalTok{ n\_flips, }\AttributeTok{prob =}\NormalTok{ .}\DecValTok{7}\NormalTok{))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(flips)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{p =}\NormalTok{ flips }\SpecialCharTok{/}\NormalTok{ n\_flips)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{from-variation-to-inference}{%
\section{From variation to
inference}\label{from-variation-to-inference}}

Let's go back to Fisher's tea-tasting experiment. The first innovation
of that experiment was the use of randomization to recover an estimate
of the causal effect of milk ordering. But there was more to Fisher's
analysis than we described.

The second innovation of the tea-tasting experiment was the idea of
creating a model of what might happen during the experiment.
Specifically, Fisher described a hypothetical \textbf{null model} that
would arise if the nameless lady had chosen cups by chance rather than
because of some tea sensitivity. In our tea-rating experiment, the null
model describes what happens when there is no difference in ratings
between tea-first and milk-first cups. Under the null model, the
treatment effect (\(\beta\)) is zero.

Even with an actual treatment effect of zero, across repeated sampling,
we should see some variation in \(\widehat{\beta}\), our \emph{estimate}
of the treatment effect. Sometimes we'll get a small positive effect,
sometimes a small negative one. Occasionally just by chance we'll get a
big effect. This is just sampling variation as we described above.

Fisher's innovation was to quantify the probability of observing various
values of \(\hat{\beta}\), given the null model. Then, if the observed
data that were very low probability under the null model, we could
declare that the null was rejected. How unlikely must the observed data
be, in order to reject the null? Fisher declared that it is ``usual and
convenient for experimenters to take 5 percent as a standard level of
convenience,'' establishing the .05 cutoff that has become gospel
throughout the sciences.\sidenote{\footnotesize Actually, right after establishing .05
  as a cutoff, Fisher then writes that ``in the statistical sense, we
  thereby admit that no isolated experiment, however significant in
  itself, can suffice for the experimental demonstration of any natural
  phenomenon\ldots{} in order to assert that a natural phenomenon is
  experimentally demonstrable we need, not an isolated record, but a
  reliable method of procedure. In relation to the test of significance,
  we may say that a phenomenon is experimentally demonstrable when we
  know how to conduct an experiment which will rarely fail to give us a
  statistically significant result.'' In other words, Fisher was all for
  replication!}

Let's take a look at what the null model might look like. We already
tried out repeating our tea-tasting experiment thousands of times in our
discussion of sampling above. Now in
Figure~\ref{fig-inference-null-model}, we do the same thing but we
assume that the \textbf{null hypothesis} of no treatment effect is true.
The plot shows the distribution of treatment effects \(\hat{\beta}\) we
observe: some a little negative, some a little positive, and a few
substantially positive or negative, but mostly zero.

Let's apply Fisher's standard. If our observation has less than a 5\%
probability under the null model, then the null model is likely wrong.
The red dashed lines on Figure~\ref{fig-inference-null-model} show the
point below which only 2.5\% of the data are found and the point above
which only 2.5\% of the data are found. These are called the
\textbf{tails} of the distribution. Because we'd be equally willing to
accept milk-first tea or tea-first tea being better, we consider both
positive and negative observations as possible.\sidenote{\footnotesize Because we're
  looking at both tails of the distribution, this is called a
  ``two-tailed'' test.}

\begin{figure}

\sidecaption{\label{fig-inference-null-model}One example of the
distribution of treatment effects under the null model (with N=9 per
group). The red regions indicate the part of the distribution in which
less than 5\% of observations should fall.}

{\centering \includegraphics{images/inference/p-region.png}

}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

To simulate our null model, we can do the same kind of thing we did
before, just specifying to our \texttt{make\_tea\_data} function that
the true difference in effects is zero!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_model }\OtherTok{\textless{}{-}} \FunctionTok{expand\_grid}\NormalTok{(}\AttributeTok{sim =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_sims, }
                          \AttributeTok{n =} \DecValTok{18}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{map}\NormalTok{(n, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{make\_tea\_data}\NormalTok{(}\AttributeTok{n\_total =}\NormalTok{ n, }\AttributeTok{delta =} \DecValTok{0}\NormalTok{))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(data))}
\end{Highlighting}
\end{Shaded}

Now again we can \texttt{group\_by} and \texttt{summarise} to get the
distribution of treatment effects under the null hypothesis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{null\_model\_summary }\OtherTok{\textless{}{-}}\NormalTok{ null\_model }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(n, sim) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{delta =} \FunctionTok{mean}\NormalTok{(rating[condition }\SpecialCharTok{==} \StringTok{"milk first"}\NormalTok{]) }\SpecialCharTok{{-}} 
              \FunctionTok{mean}\NormalTok{(rating[condition }\SpecialCharTok{==} \StringTok{"tea first"}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

That's the logic of Fisherian NHST: if the observed data fall in the
region that has a probability of less than .05 under the null model,
then we reject the null. So then when we observe some particular
treatment effect \(\hat{\beta}\) in a single (real) instance of our
experiment, we can compute the probability of these data or any data
more extreme than ours under the null model.\sidenote{\footnotesize The ``more
  extreme'' part deserves a little explanation. Any individual outcome
  is relatively unlikely by itself, just because it's surprising that
  the estimate is that exact value (we're simplifying here, it gets a
  bit trickier when you are talking about real numbers). What we care
  about instead is a \emph{group} of values. The ones that are in the
  middle of the distribution are, considered as a group, quite likely;
  the ones on the tails are, as a group, less likely. We want to know if
  the probability of the group of datapoints that includes our
  observation and anything even further out on the tails is collectively
  less than .05.} This probability is our \(p\)-value, and if it is
small, it gives us license to conclude that the null is false.

As we saw before, the larger the sample size, the smaller the standard
error. That's true for the null model too!
Figure~\ref{fig-inference-null-model2} shows the expected null
distribution for a bigger experiment.

\begin{marginfigure}

{\centering \includegraphics{images/inference/p-region-bigger.png}

}

\caption{\label{fig-inference-null-model2}Example distribution of
treatment effects under the null model for a larger experiment.}

\end{marginfigure}

The more participants in the experiment, the tighter the null
distribution becomes, and hence the smaller the region in which we
should expect a null treatment effect to fall. Because our expectation
based on the null becomes more precise, we will be able to reject the
null based on smaller treatment effects. In inference of this type, as
with estimation, our goals matter. If we're merely testing a hypothesis
out of curiosity, perhaps we don't want to measure too many cups of tea.
But if we were designing the tea strategy for a major cafe chain, the
stakes would be higher and a more precise estimate might necessary; in
that case, maybe we'd want to do a more extensive experiment!

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

We can do a more systematic simulation of the null regions for different
sample sizes by simply adding a parameter to our simulation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n\_sims }\OtherTok{\textless{}{-}} \DecValTok{10000}

\NormalTok{null\_model\_multi\_n }\OtherTok{\textless{}{-}} \FunctionTok{expand\_grid}\NormalTok{(}\AttributeTok{sim =} \DecValTok{1}\SpecialCharTok{:}\NormalTok{n\_sims, }
                          \AttributeTok{n =} \FunctionTok{c}\NormalTok{(}\DecValTok{12}\NormalTok{,}\DecValTok{24}\NormalTok{,}\DecValTok{48}\NormalTok{,}\DecValTok{96}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{rowwise}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{data =} \FunctionTok{map}\NormalTok{(n, }\SpecialCharTok{\textasciitilde{}}\FunctionTok{make\_tea\_data}\NormalTok{(}\AttributeTok{n\_total =}\NormalTok{ n, }\AttributeTok{delta =} \DecValTok{0}\NormalTok{))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{unnest}\NormalTok{(}\AttributeTok{cols =} \FunctionTok{c}\NormalTok{(data))}

\NormalTok{null\_model\_summary\_multi\_n }\OtherTok{\textless{}{-}}\NormalTok{ null\_model\_multi\_n }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(n, sim) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{delta =} \FunctionTok{mean}\NormalTok{(rating[condition }\SpecialCharTok{==} \StringTok{"milk first"}\NormalTok{]) }\SpecialCharTok{{-}} 
              \FunctionTok{mean}\NormalTok{(rating[condition }\SpecialCharTok{==} \StringTok{"tea first"}\NormalTok{]))}

\NormalTok{null\_model\_quantiles\_multi\_n }\OtherTok{\textless{}{-}}\NormalTok{ null\_model\_summary\_multi\_n }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(n) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{q25 =} \FunctionTok{quantile}\NormalTok{(delta, .}\DecValTok{025}\NormalTok{), }
            \AttributeTok{q975 =} \FunctionTok{quantile}\NormalTok{(delta, .}\DecValTok{975}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

Here is the plotting code to produce a comparable figure to our
illustration:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(null\_model\_summary\_multi\_n, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{delta)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =}\NormalTok{ .}\DecValTok{25}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{data =}\NormalTok{ null\_model\_quantiles\_multi\_n, }
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ q25), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{data =}\NormalTok{ null\_model\_quantiles\_multi\_n, }
             \FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ q975), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lty =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{n) }\SpecialCharTok{+}
  \FunctionTok{xlim}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{2.5}\NormalTok{, }\FloatTok{2.5}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{ylab}\NormalTok{(}\StringTok{"Frequency"}\NormalTok{) }\SpecialCharTok{+} \FunctionTok{xlab}\NormalTok{(}\StringTok{"Difference in rating"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

One last note: You might notice an interesting parallel between Fisher's
paradigm for NHST and Popper's falsificationist philosophy (introduced
in \textbf{?@sec-theories}). In both cases, you never get to
\emph{accept} the actual hypothesis of interest. The only thing you can
do is observe evidence that is inconsistent with the null hypothesis.
The added limitation of NHST is that the only hypothesis you can falsify
is the null!

\hypertarget{making-inferences}{%
\section{Making inferences}\label{making-inferences}}

In the tea-tasting example we were just considering, we were trying to
make an inference from our sample to the broader population. In
particular, we were trying to test whether milk-first tea was rated as
better than tea-first tea. Our inferential goal was a clear, binary
answer: is milk-first tea better?

\begin{marginfigure}

{\centering \includegraphics{images/inference/bf.png}

}

\caption{\label{fig-inference-bf}The Bayes Factor (BF).}

\end{marginfigure}

By defining a \(p\)-value, we got one procedure for giving this answer.
If \(p < .05\), we reject the null. Then we can look at the direction of
the difference and, if it's positive, declare that milk-first tea is
``significantly'' better. Let's compare this procedure to a different
process that builds on the Bayesian estimation ideas we described in the
previous chapter. We can then come back to examine NHST in light of that
framework.

\hypertarget{bayes-factors}{%
\subsection{Bayes Factors}\label{bayes-factors}}

\marginnote{\begin{footnotesize}

\hypertarget{tbl-inference-jeffreys}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-inference-jeffreys}Jeffreys
(\protect\hyperlink{ref-jeffreys1998}{1998}) interpretation guidelines
for Bayes Factors.}\tabularnewline
\toprule\noalign{}
BF range & Interpretation \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
BF range & Interpretation \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textless{} 1 & Negative evidence \\
& (supports \(H_0\)) \\
1 -- 3 & Barely worth mentioning \\
3 -- 10 & Substantial \\
10 -- 30 & Strong \\
30 -- 100 & Very strong \\
\textgreater{} 100 & Decisive \\
\end{longtable}

\end{footnotesize}}

Bayes Factors are a method for quantifying the support for one
hypothesis over another, based on an observed dataset. They don't tell
you the probability that a particular hypothesis is right, but they let
you compare two different ones.

Informally, we've now discussed two different distinct hypotheses about
the tea situation: our participants could have \emph{no} tea
discrimination ability -- leading to chance performance. We call this
\(H_0\). Or they could have some non-zero ability -- leading to greater
than chance performance. We call this \(H_1\). The Bayes Factor is
simply the likelihood of the data (in the technical sense used above)
under \(H_1\) vs.~under \(H_0\) (Figure~\ref{fig-inference-bf}). The
Bayes Factor is a ratio, so if it is greater than 1, the data are more
likely under \(H_1\) than they are under \(H_0\) -- and vice versa for
values between 1 and 0. A BF of 3 means there is three times as much
evidence for \(H_1\) than \(H_0\), or equivalently 1/3 as much evidence
for \(H_0\) as \(H_1\).\sidenote{\footnotesize Sometimes people refer to the BF in
  favor of \(H_1\) as the \(BF_{10}\) and the BF in favor of \(H_0\) as
  the \(BF_{01}\). This notation strikes us as a bit confusing because a
  reader might wonder what the 10 in the subscript means.}

There are a couple of things to notice about the Bayes Factor. The first
is that, like a p-value, it is inherently a continuous measure. You can
artificially dichotomize decisions based on the Bayes Factor by
declaring a cutoff (say, BF \textgreater{} 3 or BF \textgreater{} 10),
but there is no intrinsic threshold at which you would say the evidence
is ``significant.'' Many people follow guidelines from Jeffreys
(\protect\hyperlink{ref-jeffreys1998}{1998}), shown in
Table~\ref{tbl-inference-jeffreys}. On the other hand, cutoffs like BF
\textgreater{} 3 or \(p < .05\) are not very informative. So although we
provide this table to guide interpretation, we caution that you should
always report and interpret the actual Bayes Factor, not whether it is
above or below some cutoff.

The second thing to notice about the Bayes Factor is that it doesn't
depend on our prior probability of \(H_1\) vs.~\(H_0\). We might think
of \(H_1\) as very implausible. But the BF is independent of that prior
belief. So that means it's a measure of how much the evidence should
shift our beliefs away from our prior. One nice way to think about this
is that the Bayes Factor computes how much our beliefs -- whatever they
are -- should be changed by the data
(\protect\hyperlink{ref-morey2011}{Morey and Rouder 2011}).

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Bayes Factors are delightfully easy to compute using the
\texttt{BayesFactor} R package. All we do is feed in the two sets of
ratings to the \texttt{ttestBF} function!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tea\_bf }\OtherTok{\textless{}{-}} \FunctionTok{ttestBF}\NormalTok{(}\AttributeTok{x =}\NormalTok{ tea\_data}\SpecialCharTok{$}\NormalTok{rating[tea\_data}\SpecialCharTok{$}\NormalTok{condition }\SpecialCharTok{==} \StringTok{"milk first"}\NormalTok{],}
                  \AttributeTok{y =}\NormalTok{ tea\_data}\SpecialCharTok{$}\NormalTok{rating[tea\_data}\SpecialCharTok{$}\NormalTok{condition }\SpecialCharTok{==} \StringTok{"tea first"}\NormalTok{],}
                  \AttributeTok{paired =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

In practice, the thing that is both tricky and good about Bayes Factors
is that you need to define an actual model of what \(H_0\) and \(H_1\)
are. That process involves making some assumptions explicit. We won't go
into how to make these models here -- this is a big topic that is
covered extensively in books on Bayesian data analysis.\sidenote{\footnotesize Two
  good ones are Gelman et al. (\protect\hyperlink{ref-gelman1995}{1995})
  (a bit more statistical) and Kruschke
  (\protect\hyperlink{ref-kruschke2014}{2014}) (a bit more focused on
  psychological data analysis). An
  \href{https://vasishth.github.io/bayescogsci/book/}{in-prep web-book
  by Nicenboim et al.} also looks great.} Below we will provide some
guidance for how to compute Bayes Factors for simple experimental
designs. The goal here is just to give a sense of how they work.

\hypertarget{p-values}{%
\subsection{\texorpdfstring{\emph{p}-values}{p-values}}\label{p-values}}

Now let's turn back to NHST and the \(p\)-value. We already have a
working definition of what a \(p\)-value is from our discussion above:
it's the \textbf{probability of the data (or any data that would be more
extreme) under the null hypothesis}. How is this quantity related to
either our Bayesian estimate or the BF? Well, the first thing to notice
is that the \(p\)-value is very close (but not identical) to the
likelihood itself.\sidenote{\footnotesize The likelihood -- for both Bayesians and
  frequentists -- is the probability of the data, just like the
  \(p\)-value. But unlike the \(p\)-value, it doesn't include the
  probability of more extreme data as well.}

Next we can use a simple statistical test, a \(t\)-test, to compute
\(p\)-values for our experiment. In case you haven't encountered one, a
\(t\)-test is a procedure for computing a \(p\)-value by comparing the
distribution of two variables using the null hypothesis that there is no
difference between them.\sidenote{\footnotesize \(t\)-tests can also be used in cases
  where one sample is being compared to some baseline.} The \(t\)-test
uses the data to compute a \textbf{test statistic} whose distribution
under the null hypothesis is known. Then the value of this statistic can
be converted to \(p\)-values for making an inference.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

The standard \texttt{t.test} function is built into R via the default
\texttt{stats} package. Here we simply make sure to specify the variety
of test we want by using the flags \texttt{paired\ =\ FALSE} and
\texttt{var.equal\ =\ TRUE} (denoting the assumption of equal
variances).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tea\_t }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ tea\_data}\SpecialCharTok{$}\NormalTok{rating[tea\_data}\SpecialCharTok{$}\NormalTok{condition }\SpecialCharTok{==} \StringTok{"milk first"}\NormalTok{],}
                \AttributeTok{y =}\NormalTok{ tea\_data}\SpecialCharTok{$}\NormalTok{rating[tea\_data}\SpecialCharTok{$}\NormalTok{condition }\SpecialCharTok{==} \StringTok{"tea first"}\NormalTok{], }
                \AttributeTok{paired =} \ConstantTok{FALSE}\NormalTok{, }\AttributeTok{var.equal =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

Imagine we conduct a tea-tasting experiment with \(N=48\) and perform a
\(t\)-test on our experimental results. In this case, we see that the
difference between the two groups is significant at \(p<.05\):
\(t(46) = 2.86\), \(p = .006\).

The expression \(t(46) = 2.86\), \(p = .006\) is the standard way to
report of a \(t\)-test according to the American Psychological
Association. The first part of this report gives the \(t\) value,
qualified by the \textbf{degrees of freedom} for the test in
parentheses. We won't focus much on the idea of degrees of freedom here,
but for now it's enough to know that this number quantifies the amount
of information given by the data, in this case 48 datapoints minus the
two means (one for each of the samples).

\marginnote{\begin{footnotesize}

\hypertarget{tbl-p-bf-comparison}{}
\begin{longtable}[]{@{}rrrr@{}}
\caption{\label{tbl-p-bf-comparison}Comparison of p-value and BF for
several different (randomly-generated) tea-tasting
scenarios.}\tabularnewline
\toprule\noalign{}
N & Effect size & p-value & BF \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
N & Effect size & p-value & BF \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
12 & 0.5 & \textgreater{} .999 & 0.5 \\
12 & 1.0 & .076 & 1.4 \\
12 & 1.5 & .002 & 18.7 \\
24 & 0.5 & .858 & 0.4 \\
24 & 1.0 & .061 & 1.5 \\
24 & 1.5 & .009 & 5.6 \\
48 & 0.5 & .002 & 17.7 \\
48 & 1.0 & .033 & 2.0 \\
48 & 1.5 & \textless{} .001 & 133.6 \\
96 & 0.5 & .038 & 1.5 \\
96 & 1.0 & \textless{} .001 & 12218.2 \\
96 & 1.5 & \textless{} .001 & 3081.4 \\
\end{longtable}

\end{footnotesize}}

Let's compare \(p\) values and Bayes Factors (computed using the default
setup in the \texttt{BayesFactor} R package). In
Table~\ref{tbl-p-bf-comparison}), the rows represent simulated
experiments with varying total numbers of participants (N and varying
average treatment effects. Both \(p\) and BF go up with more
participants and larger effects. In general, BFs tend to be a bit more
conservative than \(p\)-values, such that \(p<.05\) can sometimes
translate to a BF of less than 3
(\protect\hyperlink{ref-benjamin2018}{Benjamin et al. 2018}). For
example, take a look at the row with 48 participants and an effect size
of 1: the \(p\) value is less than .05, but the Bayes Factor is only
2.0.

The critical thing about \(p\)-values, though, is not just that they are
a kind of data likelihoods. It is that they are used in a \emph{specific
inferential procedure}. The logic of NHST is that we make a binary
decision about the presence of an effect. If \(p < .05\), the null
hypothesis is rejected; otherwise not. As Fisher
(\protect\hyperlink{ref-fisher1949}{1949}) wrote,

\begin{quote}
It should be noted that the null hypothesis is never proved or
established, but is possibly disproved, in the course of
experimentation. Every experiment may be said to exist only in order to
give the facts a chance of disproving the null hypothesis. (p.~19)
\end{quote}

\begin{marginfigure}

{\centering \includegraphics{images/inference/power-alpha.png}

}

\caption{\label{fig-inference-power-alpha}Standard decision matrix for
NHST.}

\end{marginfigure}

The main problem with \(p\)-values from a scientific perspective is that
researchers are usually interested in not just rejecting the null
hypothesis but also in the evidence for the alternative (the one we are
interested in). The Bayes Factor is one approach to quantifying positive
evidence \emph{for} the alternative hypothesis in a Bayesian framework.
This issue with the Fisher approach to \(p\)-values has been known for a
long time, though, and so there is an alternative frequentist approach
as well.

\hypertarget{sec-neyman-pearson}{%
\subsection{The Neyman-Pearson approach}\label{sec-neyman-pearson}}

One way to ``patch'' NHST is to introduce a decision-theoretic view,
shown in Figure~\ref{fig-inference-power-alpha}.\sidenote{\footnotesize A little bit
  of useful history here is given in Cohen
  (\protect\hyperlink{ref-cohen1990}{1990}).} On this view, called the
Neyman-Pearson view, there is a real \(H_1\), albeit one that is not
specified. Then the true state of the world could be that \(H_0\) is
true or \(H_1\) is true. The \(p<.05\) criterion is the threshold at
which we are willing to reject the null, and so this constitutes our
\textbf{false positive} rate \(\alpha\). But we also need to define a
\textbf{false negative} rate, which is conventionally called
\(\beta\).\sidenote{\footnotesize Unfortunately, \(\beta\) is very commonly used for
  regression coefficients -- and for that reason we've used it as our
  symbol for causal effects. We'll be using these \(\beta\)s in the next
  chapter as well. Those \(\beta\)s are not to be confused with false
  negative rates. Sorry, this is just a place where statisticians have
  used the same Greek letter for two different things.}

Setting these rates is a decision problem: If you are too conservative
in your criteria for the intervention having an effect, then you risk a
false negative, where you incorrectly conclude that it doesn't work. And
if you're too liberal in your assessment of the evidence, then you risk
a false positive.\sidenote{\footnotesize To make really rational decisions, you could
  couple this chart to some kind of utility function that assessed the
  costs of different outcomes. For example, you might think it's worse
  to proceed with an intervention that doesn't work than to stay with
  business as usual. In that case, you'd assign a higher cost to a false
  positive and accordingly try to adopt a more conservative criterion.
  We won't cover this kind of decision analysis here, but Pratt et al.
  (\protect\hyperlink{ref-pratt1995}{1995}) is a classic textbook on
  statistical decision theory if you're interested.} In practice,
however, people usually leave \(\alpha\) at .05 and try to control the
false negative rate by increasing their sample size.

As we saw in Figure~\ref{fig-inference-null-model}, the larger the
sample, the better your chance of rejecting the null for any given
non-null effect. But these chances will depend also on the effect size
you are estimating. This formulation gives rise to the idea of classical
power analysis, which we cover in \textbf{?@sec-sampling}. Most folks
who defend binary inference using \(p\)-values are interested in using
the Neyman-Pearson approach. In our view, this approach has its place
(it's especially useful for power analysis but it still suffers from the
substantial issues that plague all binary inference techniques,
especially those that use \(p\)-values.

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{nonparametric-resampling-under-the-null}{%
\section*{Nonparametric resampling under the
null}\label{nonparametric-resampling-under-the-null}}
\addcontentsline{toc}{section}{Nonparametric resampling under the null}

\markright{Nonparametric resampling under the null}

Hypothesis testing requires knowing the null distribution. In the
examples above, it was easy to use statistical theory to work out the
null distribution using knowledge of the binomial or normal
distribution. But sometimes we don't know what the null distribution
would look like. What if the ratings data from our tea-tasting
experiment was very skewed, such that there were many low ratings and a
few very high ratings (as in Figure~\ref{fig-inference-permutation})?

\begin{figure}[H]

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{images/inference/skewed.png}

}

\caption{\label{fig-inference-permutation}A small tea-tasting experiment
with a skewed distribution of ratings.}

\end{figure}

With skewed data like this, we couldn't proceed with a \(t\)-test in
good conscience because, with only \(n=18\), we can't necessarily trust
that the Central Limit Theorem has ``kicked in'' sufficiently for the
test to work despite the skewness. Put another way, we can't be sure
that the null distribution is normal (Gaussian) in this case.

An alternative way to approximate a null distribution is through
nonparametric resampling. \textbf{Resampling} means that we're going to
draw new samples \emph{from our existing sample}, and
\textbf{nonparametric} means that we will do this in a way that obviates
assumptions about the shape of the null distribution -- in contrast to
\textbf{parametric} approaches that do rely on such assumptions). These
techniques are sometimes called ``bootstrapping'' techniques.

The idea is, if the treatment truly had no effect on the outcome, then
the observations would be \textbf{exchangeable} between the treatment
and control groups. That is, there would not be systematic differences
between the treatment and control groups. This property may or may not
be true in our observed sample (after all, that's why we're doing a
hypothesis test in the first place), but we can draw new samples from
our existing sample in a manner that forces exchangability.

To perform this kind of test with our tea-tasting data, we would
randomly shuffle the ratings in our dataset while leaving the condition
assignments fixed. If we did this thousands of times and computed the
treatment effect in each case, the result would be a null distribution:
what we might expect the treatment effect to look like if there was
\emph{no} condition effect. In essence we're using a simulated version
of ``random assignment'' here to \emph{break} the dependency between the
condition manipulation and the observed data.

We can then compare our \emph{actual} treatment effect to this
nonparametric null distribution. If the actual treatment was smaller
than the 2.5th percentile or larger than the 97.5th percentile in the
null distribution, we would reject the null with \(p < .05\), just the
same as if we had used a \(t\)-test.

Resampling-based tests are extremely useful in a wide variety of cases.
They can sometimes be less powerful than parametric approaches and they
almost always require more computation, but their versatility makes them
a great generic tool for data analysis.

\end{tcolorbox}

\hypertarget{inference-and-its-discontents}{%
\section{Inference and its
discontents}\label{inference-and-its-discontents}}

In earlier sections of this chapter, we reviewed NHST and Bayesian
approaches to inference. Now it's time to step back and think about some
of the ways that inference practices -- especially those related to NHST
-- have been problematic for psychology research. We'll begin with some
issues surrounding \(p\)-values and then give a specific accident report
related to the process of ``\(p\)-hacking'' and some general
philosophical discussion of how statistical testing relates to human
reasoning.

\hypertarget{problems-with-the-interpretation-of-p-values}{%
\subsection{\texorpdfstring{Problems with the interpretation of
\emph{p}-values}{Problems with the interpretation of p-values}}\label{problems-with-the-interpretation-of-p-values}}

\(p\)-values are basically likelihoods, in the sense we introduced in
the previous chapter.\sidenote{\footnotesize The only thing that is different is the
  idea that they are the likelihood of the observed data \emph{or any
  more extreme}.} They are the likelihood of the data under the null
hypothesis! This likelihood is a critical number to know -- for
computing the Bayes Factor among other reasons. But it doesn't tell us a
lot of things that we might like to know!

For example, \(p\)-values don't tell us the probability of the data
under a specific alternative hypothesis that we might be interested in
-- that's the posterior probability \(p(H_1 | \text{data})\). When our
tea-tasting \(t\)-test yielded \(t(46) = 2.86\), \(p = .006\), that's
\emph{not} the probability of the null hypothesis being true! And it's
definitely not the probability of milk-first tea being better.

What can you conclude when \(p>.05\)? According to the classical logic
of NHST, the answer is ``nothing''! A failure to reject the null
hypothesis doesn't give you any additional evidence \emph{for} the null.
Even if the probability of the data (or some more extreme data) under
\(H_0\) is high, their probability might be just as high or higher under
\(H_1\).\sidenote{\footnotesize Of course, weighing these two against one another
  brings you back to the Bayes Factor.} But many practicing researchers
make this mistake. Aczel et al.
(\protect\hyperlink{ref-aczel2018}{2018}) coded a sample of articles
from 2015 and found that 72\% of negative statements were inconsistent
with the logic of their statistical paradigm of choice -- most were
cases where researchers said that an effect was not present when they
had simply failed to reject the null.

\footnotesize

\hypertarget{tbl-dirty-dozen}{}
\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.0274}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.9726}}@{}}
\caption{\label{tbl-dirty-dozen}A ``dirty dozen'' \(p\)-value
misconceptions. Adapted from Goodman
(\protect\hyperlink{ref-goodman2008}{2008}).}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Misconception
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Misconception
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & ``If \(p = .05\), the null hypothesis has only a 5\% chance of being
true.'' \\
2 & ``A nonsignificant difference (e.g., \(p \geq .05\)) means there is
no difference between groups.'' \\
3 & ``A statistically significant finding is clinically important.'' \\
4 & ``Studies with \(p\)-values on opposite sides of .05 are
conflicting.'' \\
5 & ``Studies with the same \(p\)-value provide the same evidence
against the null hypothesis.'' \\
6 & ``\(p\) = .05 means that we have observed data that would occur only
5\% of the time under the null hypothesis.'' \\
7 & ``\(p\) = .05 and \(p \leq .05\) mean the same thing.'' \\
8 & ``\(p\)-values are properly written as inequalities (e.g.,
`\(p \leq .02\)' when \(p = .015\))'' \\
9 & ``\(p\) = .05 means that if you reject the null hypothesis, the
probability of a false positive error is only 5\%.'' \\
10 & ``With a \(p\) = .05 threshold for significance, the chance of a
false positive error will be 5\%.'' \\
11 & ``You should use a one-sided \(p\)-value when you don't care about
a result in one direction, or a difference in that direction is
impossible.'' \\
12 & ``A scientific conclusion or treatment policy should be based on
whether or not the \(p\) value is significant.'' \\
\end{longtable}

\normalsize

These are not the only issues with \(p\)-values. In fact, people have so
much trouble understanding what \(p\)-values \emph{do} say that there
are whole articles written about these misconceptions.
Table~\ref{tbl-dirty-dozen} shows a set of misconceptions documented and
refuted by Goodman (\protect\hyperlink{ref-goodman2008}{2008}).

Let's take a look at just a few. Misconception 1 is that, if \(p= .05\),
the null has a 5\% chance of being true. This misconception is a result
of confusing \(p(H_0 | \text{data})\) (the posterior) and
\(p(\text{data} | H_0)\) (the likelihood -- also known as the
\(p\)-value). Misconception 2 -- that \(p > .05\) allows us to
\emph{accept} the null -- also stems from this reversal of posterior and
likelihood. And misconception 3 is a misinterpretation of the
\(p\)-value as an effect size (which we learned about in the last
chapter): a large effect is likely to be clinically important, but with
a large enough sample size, you can get a small \(p\)-value even for a
very small effect. We won't go through all the misconceptions here, but
we encourage you to challenge yourself to work through them (as in the
exercise below).

Beyond these misconceptions, there's another problem. The \(p\)-value is
a probability of a certain set of events happening (corresponding to the
observed data or any ``more extreme'' data, that is to say, data further
from the null). Since \(p\)-values are probabilities, we can combine
them together across different events. If we run a ``null experiment''
-- an experiment where the true effect is zero -- the probability of a
dataset with \(p < .05\) is of course .05. But if we run two such
experiments, we can get \(p < .05\) with probability 0.1. By the time we
run 20 experiments, we have an 0.64 chance of getting a positive result.

It would obviously be a major mistake to run 20 experiments and then
report only the positive ones (which, by design, are false positives) as
though these still were ``statistically significant.'' The same thing
applies to doing 20 different statistical tests within a single
experiment. There are many statistical corrections that can be made to
adjust for this problem, which is known as the problem of
\textbf{multiple comparisons}.\sidenote{\footnotesize The simplest and most versatile
  one, the Bonferroni correction, just divides .05 (or technically,
  whatever your threshold is) by the number of comparisons you are
  making. Using that correction, if you do 20 null experiments, you
  would have a 3\% chance of a false positive, which is actually a
  little conservative.} But the the broader issue is one of
transparency: unless you \emph{know} what the appropriate set of
experiments or tests is, it's not possible to implement one of these
corrections!\sidenote{\footnotesize This issue is especially problematic with
  \(p\)-values because they are so often presented as an independent set
  of tests, but the problem of multiple comparisons comes up when you
  compute a lot of independent Bayes Factors as well. ``Posterior
  hacking'' via selective reporting of Bayes Factors is perfectly
  possible (\protect\hyperlink{ref-simonsohn2014}{Simonsohn 2014}).}

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{do-extraordinary-claims-require-extraordinary-evidence}{%
\section*{Do extraordinary claims require extraordinary
evidence?}\label{do-extraordinary-claims-require-extraordinary-evidence}}
\addcontentsline{toc}{section}{Do extraordinary claims require
extraordinary evidence?}

\markright{Do extraordinary claims require extraordinary evidence?}

In a blockbuster paper that may have inadvertently kicked off the
replication crisis, Bem (\protect\hyperlink{ref-bem2011}{2011})
presented nine experiments he claimed provided evidence for precognition
-- that participants somehow had foreknowledge of the future. In the
first of these experiments, Bem showed each of a group of 100
undergraduates 36 two-alternative forced choice trials in which they had
to guess which of two locations on a screen would reveal a picture
immediately before the picture was revealed. By chance, participants
should choose the correct side 50\% of the time of course. Bem found
that, specifically for erotic pictures, participants' guesses were
53.1\% correct. This rate of guessing was unexpected under the null
hypothesis of chance guessing (\(p = .01\)). Eight other studies with a
total of more than 1,000 participants yielded apparently supportive
evidence, with participants appearing to show a variety of psychological
effects even before the stimuli were shown!

Based on this evidence, should we conclude that precognition exists?
Probably not. Wagenmakers et al.
(\protect\hyperlink{ref-wagenmakers2011}{2011}) presented a critique of
Bem's findings, arguing that 1) Bem's experiments were exploratory (not
pre-registered) in nature, 2) that Bem's conclusions were \emph{a
priori} unlikely, and 3) that the level of statistical evidence from his
experiments was quite low. We find each of these arguments alone
compelling; together they present a knockdown case.

First, we've already discussed the need to be skeptical about situations
where experimenters have the opportunity for analytic flexibility in
their choice of measures, manipulations, samples, and analyses.
Flexibility leads to the possibility of cherry-picking those set of
decisions from the ``garden of forking paths'' that lead to a positive
outcome for the researcher's favored hypothesis (for more details, see
\textbf{?@sec-prereg}). And there is plenty of flexibility on display
even in Experiment 1 of Bem's paper. Although there were 100
participants in the study, they may have been combined post hoc from two
distinct samples of 40 and 60, each of which saw different conditions.
The 40 made guesses about the location of erotic, negative, and neutral
pictures; the 60 saw erotic, positive non-romantic, and positive
romantic pictures. The means of each of these conditions was presumably
tested against chance (at least 6 comparisons, for a false positive rate
of 0.26). Had positive romantic pictures been found significant, Bem
certainly could have interpreted this finding the same way he
interpreted the erotic ones.

Second, as we discussed, a \(p\)-value close to .05 does not necessarily
provide strong evidence against the null hypothesis. Wagenmakers et
al.~computed the Bayes Factor for each of experiments in Bem's paper and
found that, in many cases, the amount of evidence for \(H_1\) was quite
modest under a default Bayesian \(t\)-test. Experiment 1 was no
exception: the BF was 1.64, giving only ``anecdotal'' support for the
hypothesis of some non-zero effect, even before the multiple-comparisons
problem mentioned above.

Finally, since precognition is not supported by any prior compelling
scientific evidence (despite many attempts to obtain such evidence) and
defies well-established physical laws, perhaps we should assign a low
prior probability to Bem's \(H_1\), a non-zero precognition effect.
Taking a strong Bayesian position, Wagenmakers et al.~suggest that we
might do well to adopt a prior reflecting how unlikely precognition is,
say \(p(H_1) = 10^{-20}\). And if we adopt this prior, even a very
well-designed, highly informative experiment (with a Bayes factor
conveying substantial or even decisive evidence) would still lead to a
very low posterior probability of precognition.

Wagenmakers et al.~concluded that, rather than supporting precognition,
the conclusion from Bem's paper should be psychologists should revise
how they think about analyzing their data!

\end{tcolorbox}

\begin{tcolorbox}[colframe=.violet, title=\faFastForward \enspace Advanced topic]

\hypertarget{philosophical-and-empirical-views-of-probability-1}{%
\subsection{Philosophical (and empirical) views of
probability}\label{philosophical-and-empirical-views-of-probability-1}}

Up until now we've presented Bayesian and frequentist tools as two
different sets of computations. But in fact, these different tools
derive from fundamentally different philosophical perspectives on what a
probability even is. Very roughly, frequentist approaches tend to
believe that probabilities quantify the long-run frequencies of certain
events. So, if we say that some outcome of an event has probability .5,
we're saying that if that event happened thousands of times, the long
run frequency of the outcome would be 50\% of the total events. In
contrast, the Bayesian viewpoint doesn't depend on this sense that
events could be exactly repeated. Instead, the \textbf{subjective
Bayesian} interpretation of probability is that it quantifies a person's
degree of belief in a particular outcome.\sidenote{\footnotesize This is really a very
  rough description. If you're interested in learning more about this
  philosophical background, we recommend the Stanford Encyclopedia of
  Philosophy entry,
  ``\href{\%3Chttps://plato.stanford.edu/entries/probability-interpret/}{Interpretations
  of Probability}''.}

You don't have to take sides in this deep philosophical debate about
what probability is. But it's helpful to know that people actually seem
to reason about the world in ways that are well described by the
subjective Bayesian view of probability. Recent cognitive science
research has made a lot of headway in describing reasoning as a process
of Bayesian inference {[}for review, see @probmods2{]} where
probabilities describe degrees of belief in different hypotheses. These
hypotheses in turn are a lot like the theories we described in
\textbf{?@sec-theories}: they describe the relationships between
different abstract entities {[}@tenenbaum2011{]}. You might think that
scientists are different from lay-people in this regard, but one of the
striking findings from research on probabilistic reasoning and judgment
is that expertise doesn't matter that much. Statistically-trained
scientists -- and even statisticians -- make many of the same reasoning
mistakes as their un-trained students {[}@kahneman1979{]}. Even children
seem to reason intuitively in a way that looks a bit like Bayesian
inference {[}@gopnik2012{]}.

These cognitive science findings help to explain some of the problems
that people (scientists included) have in reasoning about \(p\)-values.
If you are an intuitively Bayesian reasoner, the quantity that you're
probably tracking is how much you believe in your hypothesis (its
posterior probability). So, many people treat the \(p\)-value as the
posterior probability of the null hypothesis.\sidenote{\footnotesize @cohen1994 is a
  great treatment of this issue.} That's exactly what fallacy \#1 in
Table dirty-dozen states -- ``If \emph{p} = .05, the null hypothesis has
only a 5\% chance of being true.'' But this equivalence is incorrect!
Written in math, \(p(\text{data} | H_0)\) (the likelihood that lets us
compute the p-value) is not the same thing as \(p(H_0 | \text{data})\)
(the posterior that we want). Pulling from our accident report above,
even if the \emph{probability of the observed ESP data given the null
hypothesis} is low, that doesn't mean that the \emph{probability of ESP}
is high.

\end{tcolorbox}

\hypertarget{what-framework-to-use}{%
\subsection{What framework to use?}\label{what-framework-to-use}}

The problem with binary inferences is that they enable behaviors that
can introduce bias into the scientific ecosystem. By the logic of
statistical significance, either an experiment ``worked'' or it didn't.
Because everyone would usually rather have an experiment that worked
than one that didn't, inference criteria like \(p\)-values often become
a target for selection, as we discussed in
\textbf{?@sec-replication}.\sidenote{\footnotesize More generally, this is probably an
  example of Goodhart's law, which states that when a measure becomes a
  target, it ceases to be a good measure
  (\protect\hyperlink{ref-strathern1997}{Strathern 1997}). Once the
  outcomes of statistical inference procedures become targets for
  publication, they are subject to selection biases that make them less
  meaningful.}

If you want to quantify evidence for or against a hypothesis, it's worth
considering whether Bayes Factors address your question better than
\(p\)-values. In practice, \(p\)-values are hard to understand and many
people misuse them -- though to be fair, BFs are misused plenty too.
These issues may be rooted in basic facts about how human beings reason
about probability.

Despite the reasons to be worried about \(p\)-values, for many
practicing scientists (at least at time of writing) there is no one
right answer about whether to use them or not. Even if we'd like to be
Bayesian all the time, there are a number of obstacles. First, though
new computational tools make fitting Bayesian models and extracting
Bayes Factors much easier than before, it's still on average quite a bit
harder to fit a Bayesian model than it is a frequentist one. Second,
because Bayesian analyses are less familiar, it may be an uphill battle
to convince advisors, reviewers, and funders to use them.

As a group, some of us are more Bayesian than frequentist, while others
are more frequentist than Bayesian -- but all of us recognize the need
to move flexibly between statistical paradigms. Furthermore, a lot of
the time we're not so worried about which paradigm we're using. The
paradigms are at their most divergent when making binary inferences, and
they often look much more similar when they are used in the context of
estimation.

\hypertarget{estimating-precision}{%
\section{Estimating precision}\label{estimating-precision}}

Our last section presented an argument against using \(p\)-values for
making \emph{dichotomous} inferences. But we still want to move from
what we know about our own limited sample to some inference about the
population. How should we do this?

\hypertarget{confidence-intervals}{%
\subsection{Confidence intervals}\label{confidence-intervals}}

One alternative to binary hypothesis testing is to ask about the
precision of our estiamates, in particular how similar an estimate from
a particular sample is to the population parameter of interest. For
example, how close is our tea-tasting effect estimate to the true effect
in the population? We don't know what the true effect is, but our
knowledge of sampling distributions lets us make some guesses about how
precise our estimate is.

The \textbf{confidence interval} is a convenient frequentist way to
summarize the variability of the sampling distribution -- and hence how
precise our point estimate is. The confidence interval represents the
range of possible values for the parameter of interest that are
plausible given the data. More formally, a 95\% confidence interval for
some estimate (call it \(\widehat{\beta}\), as in our example) is
defined as a range of possible values for \(\beta\) such that, if we did
repeated sampling, 95\% of the intervals generated by those samples
would contain the true parameter, \(\beta\).

Confidence intervals are constructed by estimating the middle 95\% of
the sampling distribution of \(\widehat{\beta}\). Because of our hero,
the Central Limit Theorem, we can treat the sampling distribution as
normal for reasonably large samples. Given this, it's common to
construct a 95\% confidence intervals
\(\widehat{\beta} \pm 1.96 \; \widehat{SE}\).\sidenote{\footnotesize This type of CI
  is called a ``Wald'' confidence interval.} If we were to conduct the
experiment 100 times and calculate a confidence interval each time, we
should expect 95 of the intervals to contain the true \(\beta\), whereas
we would expect the remaining 5 to not contain \(\beta\).\sidenote{\footnotesize In
  case you don't have enough tea to do the experiment 100 times to
  confirm this, you can do it virtually using
  \href{https://istats.shinyapps.io/ExploreCoverage/}{this nice
  simulation tool}.}

Confidence intervals are like betting on the inferences drawn from your
sample. The sample you drew is like one pull of a slot machine that will
pay off (i.e., have the confidence interval contain the true parameter)
95\% of the time. Put more concisely: 95\% of 95\% confidence intervals
contain the true value of the population parameter.

\begin{figure}

\sidecaption{\label{fig-inference-condition-cis}Confidence intervals on
each of the two condition estimates, as well as on the difference
between conditions.}

{\centering \includegraphics{images/inference/cis.png}

}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Computing confidence intervals analytically is pretty easy. Here we
first compute the standard error for the difference between conditions.
The only tricky bit here is that we need to compute a pooled standard
deviation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tea\_ratings }\OtherTok{\textless{}{-}}\NormalTok{ tea\_data}\SpecialCharTok{$}\NormalTok{rating[tea\_data}\SpecialCharTok{$}\NormalTok{condition }\SpecialCharTok{==} \StringTok{"tea first"}\NormalTok{]}
\NormalTok{milk\_ratings }\OtherTok{\textless{}{-}}\NormalTok{ tea\_data}\SpecialCharTok{$}\NormalTok{rating[tea\_data}\SpecialCharTok{$}\NormalTok{condition }\SpecialCharTok{==} \StringTok{"milk first"}\NormalTok{]}
\NormalTok{n\_tea }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(tea\_ratings)}
\NormalTok{n\_milk }\OtherTok{\textless{}{-}} \FunctionTok{length}\NormalTok{(milk\_ratings)}
\NormalTok{sd\_tea }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(tea\_ratings)}
\NormalTok{sd\_milk }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(milk\_ratings)}

\NormalTok{tea\_sd\_pooled }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(((n\_tea}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{sd\_tea}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{+}\NormalTok{ (n\_milk}\DecValTok{{-}1}\NormalTok{)}\SpecialCharTok{*}\NormalTok{sd\_milk}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/} 
\NormalTok{                        (n\_tea }\SpecialCharTok{+}\NormalTok{ n\_milk }\SpecialCharTok{{-}} \DecValTok{2}\NormalTok{))}

\NormalTok{tea\_se }\OtherTok{\textless{}{-}}\NormalTok{ tea\_sd\_pooled }\SpecialCharTok{*} \FunctionTok{sqrt}\NormalTok{((}\DecValTok{1}\SpecialCharTok{/}\NormalTok{n\_tea) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{/}\NormalTok{n\_milk))}
\end{Highlighting}
\end{Shaded}

Once we have the standard error, we can get the estimated difference
between conditions and compute the confidence intervals by multiplying
the standard error by 1.96.

\begin{verbatim}
delta_hat <- mean(milk_ratings) - mean(tea_ratings)
tea_ci_lower <- delta_hat - tea_se *1.96
tea_ci_upper <- delta_hat + tea_se *1.96
\end{verbatim}

\end{tcolorbox}

For visualization purposes, we can show the confidence intervals on
individual estimates (left side of
Figure~\ref{fig-inference-condition-cis})). These tell us about the
precision of our estimates of each quantity relative to the population
estimate. But we've been talking primarily about the CI on the treatment
effect \(\widehat{\beta}\) (right side of
Figure~\ref{fig-inference-condition-cis}). This CI allows us to make an
inference about whether or not it overlaps with zero -- which is
actually equivalent in this case to whether or not the \(t\)-test is
statistically significant.

\hypertarget{confidence-in-confidence-intervals}{%
\subsection{Confidence in confidence
intervals?}\label{confidence-in-confidence-intervals}}

In practice, confidence intervals are often misinterpreted by students
and researchers alike (\protect\hyperlink{ref-hoekstra2014}{Hoekstra et
al. 2014}). In Hoekstra et al.'s example, a researcher conducts an
experiment and reports ``The 95\% confidence interval for the mean
ranges from 0.1 to 0.4''. But then all of the statements in
Table~\ref{tbl-inference-ci-false}, though tempting to make about this
situation, are \emph{technically false}.

\footnotesize

\hypertarget{tbl-inference-ci-false}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.0252}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.9748}}@{}}
\caption{\label{tbl-inference-ci-false}Confidence interval
misconceptions for a confidence interval {[}0.1,0.4{]}. Adapted from
Hoekstra et al.
(\protect\hyperlink{ref-hoekstra2014}{2014}).}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Misconception
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Misconception
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & ``The probability that the true mean is greater than 0 is at least
95\%.'', \\
2 & ``The probability that the true mean equals 0 is smaller than
5\%.'', \\
3 & ``The `null hypothesis' that the true mean equals 0 is likely to be
incorrect.'', \\
4 & ``There is a 95\% probability that the true mean lies between 0.1
and 0.4.'', \\
5 & ``We can be 95\% confident that the true mean lies between 0.1 and
0.4.'', \\
6 & ``If we were to repeat the experiment over and over, then 95\% of
the time the true mean falls between 0.1 and 0.4.'' \\
\end{longtable}

\normalsize

The problem with all of these statements is that, in the frequentist
framework, there is only one true value of the population parameter, and
the variability captured in confidence intervals is about the
\emph{samples}, not the parameter itself.\sidenote{\footnotesize In contrast,
  Bayesians think of parameters themselves as variable rather than
  fixed.} For this reason, we can't make any statements about the
probability of the value of the parameter or of our confidence in
specific numbers. To reiterate, what we \emph{can} say is: if we were to
repeat the procedure of conducting the experiment and calculating a
confidence interval many times, in the long run, 95\% of those
confidence intervals would contain the true parameter.

The Bayesian analog to a confidence interval is a \textbf{credible
interval}. Recall that for Bayesians, parameters themselves are
considered probabilistic (i.e., subject to random variation), not fixed.
A 95\% credible interval for an estimate, \(\widehat{\beta}\),
represents a range of possible values for \(\beta\) such that there is a
95\% probability that \(\beta\) falls inside the interval. Because we
are now wearing our Bayesian hats, we are ``allowed'' to talk about
\(\beta\) as if it were probabilistic rather than fixed. In practice,
credible intervals are constructed by finding the posterior distribution
of \(\beta\), as in \textbf{?@sec-estimation}, and then taking the
middle 95\%, for example.

Credible intervals are nice because they don't give rise to many of the
inference fallacies surrounding confidence intervals. They actually do
represent our beliefs about where \(\beta\) is likely to be, for
example. Despite the technical differences between credible intervals
and confidence intervals, in practice -- with larger sample sizes and
weaker priors -- they turn out to be quite similar to one another in
many cases. On the other hand, they can diverge sharply in cases with
less data or stronger priors (\protect\hyperlink{ref-morey2016}{Morey et
al. 2016}).

\hypertarget{chapter-summary-inference}{%
\section{Chapter summary: Inference}\label{chapter-summary-inference}}

Inference tools help you move from characteristics of the sample to
characteristics of the population. This move is a critical part of
generalization from research data. But we hope we've convinced you that
inference doesn't have to mean making a binary decision about the
presence or absence of an effect. A strategy that seeks to estimate the
size of a particular effects and then make inferences about the
precision of that estimate is often much more helpful as a building
block for theory. As we move towards estimating causal effects in more
complex experimental designs, this estimation process will require more
sophisticated models. Towards that goal, the next chapter provides some
guidance for how to build such models.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Can you write the definition of a \(p\)-value and a Bayes Factor
  without looking them up? Try this out -- what parts of the definitions
  did you get wrong?
\item
  Take three of Goodman's (2008) ``dirty dozen'' in
  Table~\ref{tbl-dirty-dozen}) and write a description of why each is a
  misconception. (These can be checked against the original article,
  which gives a nice discussion of each.
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\item
  A fun, polemical critique of NHST: Cohen, J. (1994). The earth is
  round (p \textless{} .05). American Psychologist, 49, 997--1003.
  \url{https://doi.org/10.1037/0003-066X.49.12.997}.
\item
  A nice introduction to Bayesian data analysis: Kruschke, J. K., \&
  Liddell, T. M. (2018). Bayesian data analysis for newcomers.
  Psychonomic bulletin \& review, 25(1), 155-177.
  \url{https://doi.org/10.3758/s13423-017-1272-1}.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-8}{%
\section*{References}\label{bibliography-8}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-8}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-aczel2018}{}}%
Aczel, Balazs, Bence Palfi, Aba Szollosi, Marton Kovacs, Barnabas
Szaszi, Peter Szecsi, Mark Zrubka, Quentin F Gronau, Don van den Bergh,
and Eric-Jan Wagenmakers. 2018. {``Quantifying Support for the Null
Hypothesis in Psychology: An Empirical Investigation.''} \emph{Advances
in Methods and Practices in Psychological Science} 1 (3): 357--66.

\leavevmode\vadjust pre{\hypertarget{ref-bem2011}{}}%
Bem, Daryl J. 2011. {``Feeling the Future: Experimental Evidence for
Anomalous Retroactive Influences on Cognition and Affect.''}
\emph{Journal of Personality and Social Psychology} 100 (3): 407.

\leavevmode\vadjust pre{\hypertarget{ref-benjamin2018}{}}%
Benjamin, Daniel J, James O Berger, Magnus Johannesson, Brian A Nosek,
E-J Wagenmakers, Richard Berk, Kenneth A Bollen, et al. 2018.
{``Redefine Statistical Significance.''} \emph{Nature Human Behaviour} 2
(1): 6--10.

\leavevmode\vadjust pre{\hypertarget{ref-cohen1990}{}}%
Cohen, Jacob. 1990. {``Things i Have Learned (so Far).''} \emph{American
Psychologist} 45: 1304--12.

\leavevmode\vadjust pre{\hypertarget{ref-cumming2014}{}}%
Cumming, Geoff. 2014. {``The New Statistics: Why and How.''}
\emph{Psychol. Sci.} 25 (1): 7--29.

\leavevmode\vadjust pre{\hypertarget{ref-fisher1949}{}}%
Fisher, Ronald A. 1949. {``The Design of Experiments.''}

\leavevmode\vadjust pre{\hypertarget{ref-gelman1995}{}}%
Gelman, Andrew, John B Carlin, Hal S Stern, and Donald B Rubin. 1995.
\emph{Bayesian Data Analysis}. Chapman; Hall/CRC.

\leavevmode\vadjust pre{\hypertarget{ref-gelman2006b}{}}%
Gelman, Andrew, and Jennifer Hill. 2006. \emph{Data Analysis Using
Regression and Multilevel/Hierarchical Models}. Cambridge university
press.

\leavevmode\vadjust pre{\hypertarget{ref-gelman2020}{}}%
Gelman, Andrew, Jennifer Hill, and Aki Vehtari. 2020. \emph{Regression
and Other Stories}. Cambridge University Press.

\leavevmode\vadjust pre{\hypertarget{ref-goodman2008}{}}%
Goodman, Steven. 2008. {``A Dirty Dozen: Twelve p-Value
Misconceptions.''} In \emph{Seminars in Hematology}, 45:135--40. 3.
Elsevier.

\leavevmode\vadjust pre{\hypertarget{ref-hoekstra2014}{}}%
Hoekstra, Rink, Richard D Morey, Jeffrey N Rouder, and Eric-Jan
Wagenmakers. 2014. {``Robust Misinterpretation of Confidence
Intervals.''} \emph{Psychonomic Bulletin \& Review} 21 (5): 1157--64.

\leavevmode\vadjust pre{\hypertarget{ref-jeffreys1998}{}}%
Jeffreys, Harold. 1998. \emph{The Theory of Probability}. OUP Oxford.

\leavevmode\vadjust pre{\hypertarget{ref-kruschke2014}{}}%
Kruschke, John K. 2014. \emph{Doing Bayesian Data Analysis: A Tutorial
with r, JAGS, and Stan}. Academic Press.

\leavevmode\vadjust pre{\hypertarget{ref-kruschke2018}{}}%
Kruschke, John K, and Torrin M Liddell. 2018. {``The Bayesian New
Statistics: Hypothesis Testing, Estimation, Meta-Analysis, and Power
Analysis from a Bayesian Perspective.''} \emph{Psychon. Bull. Rev.} 25
(1): 178--206.

\leavevmode\vadjust pre{\hypertarget{ref-mcelreath2018}{}}%
McElreath, Richard. 2018. \emph{Statistical Rethinking: A Bayesian
Course with Examples in r and Stan}. Chapman; Hall/CRC.

\leavevmode\vadjust pre{\hypertarget{ref-morey2016}{}}%
Morey, Richard D, Rink Hoekstra, Jeffrey N Rouder, Michael D Lee, and
Eric-Jan Wagenmakers. 2016. {``The Fallacy of Placing Confidence in
Confidence Intervals.''} \emph{Psychonomic Bulletin \& Review} 23 (1):
103--23.

\leavevmode\vadjust pre{\hypertarget{ref-morey2011}{}}%
Morey, Richard D, and Jeffrey N Rouder. 2011. {``Bayes Factor Approaches
for Testing Interval Null Hypotheses.''} \emph{Psychological Methods} 16
(4): 406.

\leavevmode\vadjust pre{\hypertarget{ref-poldrack2023}{}}%
Poldrack, R. A. 2023. \emph{Statistical Thinking: Analyzing Data in an
Uncertain World}. Princeton University Press.

\leavevmode\vadjust pre{\hypertarget{ref-pratt1995}{}}%
Pratt, John Winsor, Howard Raiffa, Robert Schlaifer, et al. 1995.
\emph{Introduction to Statistical Decision Theory}. MIT press.

\leavevmode\vadjust pre{\hypertarget{ref-simonsohn2014}{}}%
Simonsohn, Uri. 2014. {``Posterior-Hacking: Selective Reporting
Invalidates Bayesian Results Also.''} \emph{Available at SSRN 2374040}.

\leavevmode\vadjust pre{\hypertarget{ref-strathern1997}{}}%
Strathern, Marilyn. 1997. {``{`Improving Ratings'}: Audit in the British
University System.''} \emph{European Review} 5 (3): 305--21.

\leavevmode\vadjust pre{\hypertarget{ref-wagenmakers2011}{}}%
Wagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, and Han LJ Van Der
Maas. 2011. {``Why Psychologists Must Change the Way They Analyze Their
Data: The Case of Psi: Comment on Bem (2011).''}

\end{CSLReferences}

\hypertarget{sec-models}{%
\chapter{Models}\label{sec-models}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Articulate a strategy for estimating experimental effects using
  statistical models
\item
  Build intuitions about how classical statistical tests relate to
  linear regression models
\item
  Explore variations of the linear model, including generalized linear
  models and mixed effects models
\item
  Reason about tradeoffs and strategies for model specification,
  including the use of control variables
\end{itemize}

\end{tcolorbox}

In the previous two chapters, we introduced concepts surrounding
estimation of an experimental effect and inference about its
relationship to the effect in the population. The tools we introduced
there are for fairly specific research questions, and so are limited in
their applicability. Once you get beyond the world of two-condition
experiments in which each participant contributes one data point from a
continuous measure, the simple \(t\)-test is not sufficient.

In some statistics textbooks, the next step would be to present a whole
host of other statistical tests that are designed for other special
cases. We could even show a decision-tree: you have repeated measures?
Use Test X! Or categorical data? Use Text Y! Or three conditions? Use
Test Z! But this isn't a statistics book, and even if it were, we don't
advocate that approach. The idea of finding a specific narrowly-tailored
test for your situation is part and parcel of the dichotomous NHST
approach that we tried to talk you out of in the last chapter. If all
you want is your \(p<.05\), then it makes sense to look up the test that
can allow you to compute a \(p\) value in your specific case. But we
prefer an approach that is more focused on getting a good estimate of
the magnitude of the causal effect.

In this chapter, we begin to explore how to select an appropriate
\textbf{statistical model} to clearly and flexibly reason about these
effects. A statistical model is a way of writing down a set of
assumptions about how particular data are generated, the \textbf{data
generating process}. Statistical models are the bread and butter tools
for estimating particular \textbf{parameters} of interest from empirical
data -- like the magnitude of a causal effect associated with an
experimental manipulation.

For example, suppose you watch someone tossing a coin and observe a
sequence of heads and tails. A simple statistical model might assume
that the observed data are generated via the flip of a weighted coin.
From the perspective of the last two chapters, we could estimate a
standard error for the estimated proportion of flips that are heads
(e.g., for 6 heads out of 8 flips, we have \(\hat{p}= 0.75 \pm 0.17\)),
or we could compare the observed proportion against a null hypothesis.
From a model-based perspective, however, we instead begin by thinking
about where the data came from: we might assume the coin being flipped
has some weight (a \textbf{latent}, or unobservable, parameter of the
data generating process), and our goal is to determine the most likely
value of that weight given the observed data. This single unified model
can then also be used to make inferences about whether the coin's weight
differs from some null model (a fair coin, perhaps), or to predict
future flips.

This example sounds a lot like the kinds of simple inferential tests we
talked about in the previous chapter; not very ``model-y.'' But things
get more interesting when there are multiple parameters to be estimated,
as in many real-world experiments. In the tea-tasting scenario we've
belabored over the past two chapters, a real experiment might involve
multiple people tasting different types of tea in different orders, all
with some cups randomly assigned to be milk-first or tea-first. What
we'll learn to do in this chapter is to make a model of this situation
that allows us to reason about the magnitude of the milk-order effect
while also estimating variation due to different people, orders, and tea
types. This is the advantage of using models: once you are able to
reason about estimation and inference in model-based terms, you will be
set free from long decision trees and will be able to flexibly make the
assumptions that make sense for your data.\sidenote{\footnotesize We won't explore the
  connection to DAGs and Bayesian models here, but one way to think of
  this model building is as creating a causal theory of the experiment.
  This approach, which is advocated by McElreath
  (\protect\hyperlink{ref-mcelreath2018}{2018}), creates powerful
  connections between the ideas about theory we presented in Chapters
  \textbf{?@sec-experiments} and \textbf{?@sec-models} and the ideas
  about models here. If this sounds intriguing, we encourage you to go
  down the rabbit hole!}

We'll begin by discussing the ubiquitous framework for building
statistical models, \textbf{linear regression}.\sidenote{\footnotesize The name
  regression originally comes from Galton
  (\protect\hyperlink{ref-galton1877}{1877})'s work on heredity. He was
  looking at the relationship between the heights of parents and
  children. He found that children's heights \emph{regressed}, and he
  did so by creating a \emph{regression model}. Now we use the term
  ``regression'' to mean any model of this form.} We will then build
connections between regression and the \(t\)-test. This section will
discuss how to add covariates to regression models, and when linear
regression does and doesn't work. In the following section, we'll
discuss the \textbf{generalized linear model}, an innovation that allows
us to make models of a broader range of data types, including
\textbf{logistic regression}. We'll then briefly introduce \textbf{mixed
models}, which allow us to model clustering in our datasets (such as
clusters of observations from a single individual or single stimulus
item). We'll end with some opinionated practical advice on model
building.

If you're interested in building up intuitions about statistical model
building, then we recommend reading this chapter all the way through. On
the other hand, if you are already engaged in data analysis and want to
see an example, we suggest that you skip to the last section, where we
give some opinionated practical advice on model building and provide a
worked example of fitting a mixed effects model and interpreting it in
context.

\hypertarget{regression-models}{%
\section{Regression models}\label{regression-models}}

There are many types of statistical models, but this chapter will focus
primarily on regression, a broad and extremely flexible class of models.
A regression model relates a dependent variable to one or more
independent variables. Dependent variables are sometimes called
\textbf{outcome variables}, and independent variables are sometimes
called \textbf{predictor variables}, \textbf{covariates}, or
\textbf{features}.\sidenote{\footnotesize The reverse is not true -- not every
  predictor or covariate is an independent variable! One of the tricky
  things about relating regression models to causal hypotheses is that,
  just because something is on the right side of a regression equation,
  that doesn't mean it's a causal manipulation. And of course, just
  because you've got an estimate of some predictor in a regression, that
  doesn't mean the estimate tells you about the magnitude of the
  \emph{causal} effect. It could, but it also might not!} We will see
that many common statistical estimators (like the sample mean) and
methods of inference (like the \(t\)-test) are actually simple
regression models. Understanding this point will help you see many
statistical methods as special cases of the same underlying framework,
rather than as unrelated, \emph{ad hoc} tests.

\hypertarget{regression-for-estimating-a-simple-treatment-effect}{%
\subsection{Regression for estimating a simple treatment
effect}\label{regression-for-estimating-a-simple-treatment-effect}}

Let's start with one of these special cases, namely estimating a
treatment effect, \(\beta\), in a two-group design. In
\textbf{?@sec-estimation}, we solved this exact challenge for the
tea-tasting experiment. We applied a classical model in which the
milk-first ratings are assumed to be normally distributed with mean
\(\theta_{\text{<}} = \theta_{\text{T}} + \beta\) and standard deviation
\(\sigma\).\sidenote{\footnotesize Here's a quick reminder that ``model'' here is a
  way of saying ``set of assumptions about the data generating
  procedure.'' So saying that some equation is a ``model'' is the same
  as saying that we think this is where the data came from. We can
  ``turn the crank'' -- generate data through the process that's
  specified in those equations, e.g., pulling numbers from a normal
  distribution with mean \(\theta_{\text{M}}\) and standard deviation
  \(\sigma\). In essence, we're committing to the idea that this process
  will give us data that are substantively similar to the ones we have
  already.}

Let's now write that model as a regression model, that is, as a model
that predicts each participant's tea rating, \(Y_i\), given that
participant's treatment assignment, \(X_i\). \(X_i=0\) represents the
control (milk-first) group and \(X_i=1\) represents the treatment
(tea-first) group.\sidenote{\footnotesize Using 0 and 1 is known as \textbf{dummy
  coding}, and allows us to interpret the parameter as the difference of
  the treatment group (tea-first) from the baseline (milk-first). There
  are many other ways to code categorical variables, with other
  interpretations. As a practical tip, be careful to check how your
  variables are coded before reporting anything!} Here, \(Y_i\) is the
dependent variable, and \(X_i\) is the independent variable. The
subscripts \(i\) index the participants. To make this concrete, you can
see some sample tea-tasting data (the first six observations from each
condition) below (Table~\ref{tbl-models-datatable}), with the index
\(i\), the condition and its predictor \(X_i\), and the rating \(Y\).

\footnotesize

\hypertarget{tbl-models-datatable}{}
\begin{margintable}

\begin{longtable}[]{@{}rlrr@{}}
\caption{\label{tbl-models-datatable}Example tea tasting
data.}\tabularnewline
\toprule\noalign{}
id & condition & X & rating (Y) \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
id & condition & X & rating (Y) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & milk first & 0 & 6 \\
2 & milk first & 0 & 4 \\
3 & milk first & 0 & 5 \\
4 & milk first & 0 & 5 \\
5 & milk first & 0 & 5 \\
6 & milk first & 0 & 4 \\
7 & tea first & 1 & 1 \\
8 & tea first & 1 & 5 \\
9 & tea first & 1 & 3 \\
10 & tea first & 1 & 1 \\
11 & tea first & 1 & 3 \\
12 & tea first & 1 & 5 \\
\end{longtable}

\end{margintable}

\normalsize

Let's write this model more formally as a \textbf{linear regression of Y
on X}. Conventionally, regression models are written with
``\(\beta\)'\,' symbols for all parameters, so we'll now use
\(\beta_0 = \theta_M\) for the mean in the milk-first group and
\(\beta_1 = \theta_T - \theta_M\) as the average difference between the
tea-first and milk-first groups.\sidenote{\footnotesize This \(\beta\) is a
  generalization of the one were using to denote the causal effect above
  and in the previous two chapters.}

\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\]

The term \(\beta_0 + \beta_1 X_i\) is called the \textbf{linear
predictor}, and it describes the expected value of an individual's tea
rating, \(Y_i\), given that participant's treatment group \(X_i\) (the
single independent variable in this model). That is, for a participant
in the control group (\(X_i=0\)), the linear predictor is just equal to
\(\beta_0\), which is indeed the mean for the control group that we
specified above. On the other hand, for a participant in the treatment
group, the linear predictor is equal to \(\beta_0 + \beta_1\), which is
the mean for the treatment group that we specified. In regression
jargon, \(\beta_0\) and \(\beta_1\) are \textbf{regression
coefficients}, where \(\beta_1\) represents the association of the
independent variable \(X\) with the outcome \(Y\).

The term \(\epsilon_i\) is the \textbf{error term}, referring to random
variation of participants' ratings around the group mean.\sidenote{\footnotesize Formally,
  we'd write \(\epsilon_i \sim \mathcal{N}(0, \sigma^2)\). The tilde
  means ``is distributed as'', and what follows is a normal distribution
  with mean 0 and variance \(\sigma^2\).} Note that this is a very
specific kind of ``error''; it does not include ``error'' due to bias,
for example. Instead, you can think of the error terms as capturing the
``error'' that would be associated with predicting any given
participant's rating based on just the linear predictor. If you
predicted a control group participant's rating as \(\beta_0\), that
would be a good guess -- but you still expect the participant's rating
to deviate somewhat from \(\beta_0\) (i.e., due to variability across
participants beyond what is captured by their treatment groups). In our
regression model, the linear predictor and error terms together say that
participants' ratings scatter randomly (in fact, normally) around their
group means with standard deviation \(\sigma\). And that is exactly the
same model we posited in \textbf{?@sec-estimation}.\sidenote{\footnotesize You may be
  wondering why so much effort was put into building boutique solutions
  for these special cases when a unified framework was available the
  whole time. A partial answer is that the classical infrastructure of
  statistics was developed before computers were widespread, and these
  special cases were chosen because they were easy to work with
  ``analytically'' (meaning to work out all the math with pen-and-paper,
  using values from big numerical tables). Now that we have computers
  with more flexible algorithms, the model-based perspective is more
  practical and accessible than it used to be.}

Now we have the model. But how do we estimate the regression
coefficients \(\beta_0\) and \(\beta_1\)? The usual method is called
\textbf{ordinary least squares (OLS)}. Here's the basic idea. For any
given regression coefficient estimates \(\widehat{\beta}_0\) and
\(\widehat{\beta}_1\), we would obtain different \textbf{predicted
values}, \(\widehat{Y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 X_i\)
for each participant. Some regression coefficient estimates will yield
better predictions than others. OLS estimation is designed to find the
values of the regression coefficients that optimize these predictions,
meaning that the predictions are as close as possible to participants'
true outcomes, \(Y_i\).\sidenote{\footnotesize Specifically, OLS minimizes squared
  error loss, in the sense that it will choose the regression
  coefficient estimates whose predictions minimize
  \(\sum_{i=1}^n \left( Y_i - \widehat{Y}_i\right)^2\), where \(n\) is
  the sample size. A wonderful thing about OLS is that those optimal
  regression coefficients (generically termed
  \(\widehat{\mathbf{\beta}}\)) turn out to have a very simple closed
  form solution:
  \(\widehat{\mathbf{\beta}} = \left( \mathbf{X}'\mathbf{X} \right)^{-1} \mathbf{X}'\mathbf{y}\).
  We are using more general notation here that supports multiple
  independent variables: \(\widehat{\mathbf{\beta}}\) is a vector,
  \(\mathbf{X}\) is a matrix of independent variables for each subject,
  and \(\mathbf{y}\) is a vector of participants' outcomes. As more good
  news, the standard error for \(\widehat{\mathbf{\beta}}\) has a
  similarly simple closed form!}

\begin{figure}

\sidecaption{\label{fig-models-ols-plot}(left) Best-fitting regression
coefficients for the tea-tasting experiment. (right) Much worse
coefficients for the same data. Dotted lines: residuals. Circles: data
points for individual participants.}

{\centering \includegraphics{007-models_files/figure-pdf/fig-models-ols-plot-1.png}

}

\end{figure}

Figure~\ref{fig-models-ols-plot} illustrates the tea tasting data for
each condition (the dots) along with the model predictions for each
condition \(\beta_0\) and \(\beta_0 + \beta_1\) (blue lines). The gap
between each data point and the corresponding predictions (the thing
that OLS wants to minimize) is shown by the dotted lines. These
distances are sample estimates, called \textbf{residuals}, of the true
errors (\(\epsilon_i\). The left-hand plot shows the OLS coefficient
values -- the ones that move the model's predictions as close as
possible to the data points, in the sense of minimizing the total
squared length of the dashed lines. The right-hand plot shows a
substantially worse set of coefficient values.

You'll notice that we aren't talking much about \(p\)-values in this
chapter. Regression models can be used to produce \(p\)-values for
specific coefficients, representing inferences about the likelihood of
the observed data under some null hypothesis regarding the coefficients.
You can also compute Bayes Factors for specific regression coefficients,
or use Bayesian inference to fit these coefficients under some prior
expectation about their distribution. We won't talk much about this, or
more generally how to fit the models we describe. As we said, we're not
going to give a full treatment of all the relevant statistical topics.
Instead we want to help you begin thinking about making models of your
data.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

As it turns out, fitting an OLS regression model in R is extremely easy.
The underlying call is \texttt{lm}, which stands for linear model. You
can fit the model with a single call to this function with a ``formula''
as its argument. Here's the call:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ condition, }\AttributeTok{data =}\NormalTok{ tea\_data)}
\end{Highlighting}
\end{Shaded}

Formulas in R are a special kind of terse notation for regression
equations where you write the outcome, \texttt{\textasciitilde{}}
(distributed as), and the predictors. R assumes that you want an
intercept by default, and there are also a number of other handy
defaults that make R formulas a nice easy way to specify relatively
complex regression models, as we'll see below.

Once you've fit the model and assigned it to a variable, you can call
\texttt{summary} to see a summary of the parameters of the model:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

You can also extract the coefficient values using \texttt{coef(mod)},
and put them in a handy dataframe using \texttt{tidy(mod)} from the
\texttt{broom} package.

\end{tcolorbox}

\hypertarget{adding-predictors}{%
\subsection{Adding predictors}\label{adding-predictors}}

The regression model we just wrote down is the same model that underlies
the \(t\)-test from \textbf{?@sec-inference}. But the beauty of
regression modeling is that much more complex estimation problems can
also be written as regression models that extend the model we made
above. For example, we might want to add another predictor variable,
such as the age of the participant.\sidenote{\footnotesize The ability to estimate
  multiple coefficients at once is a huge strength of regression
  modeling, so much so that sometimes people use the label
  \textbf{multiple regression} to denote that there is more than one
  predictor + coefficient pair.}

Let's add this new independent variable and a corresponding regression
coefficient to our model:

\[
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2}  + \epsilon_i 
\]

Now that we have multiple independent variables, we've labeled them
\(X_{1}\) (treatment group) and \(X_{2}\) (age) for clarity.

To illustrate how to interpret the regression coefficients in this
model, let's use the linear predictor to compare the model's predicted
tea ratings for two hypothetical participants who are both in the
treatment group: 20-year-old Alice and 21-year old Bob. Alice's linear
predictor tells us that her expected rating is
\(\beta_0 + \beta_1 + \beta_2 \cdot 20\). In contrast, Bob's linear
predictor is \(\beta_0 + \beta_1 + \beta_2 \cdot 21\). We could
therefore calculate the expected difference in ratings for 21-year-olds
versus 20-year olds by subtracting Alice's linear predictor from Bob's,
yielding just \(\beta_2\).

We would get the same result if Alice and Bob were instead 50 and 51
years old, respectively. This equivalence illustrates a key point about
linear regression models in general:

\begin{quote}
The regression coefficient represents the expected difference in outcome
when comparing any two participants who differ by 1 unit of the relevant
independent variable, and who do not differ on any other independent
variables in the model.
\end{quote}

Here, the coefficient compares participants who differ by 1 year of age.
In ``Practical modeling considerations'' below, we discuss whether and
when to ``control for'' additional variables (i.e., when to add them to
your model).

\hypertarget{interactions}{%
\subsection{Interactions}\label{interactions}}

In our running example, we now have two predictors: condition and age.
But what if the effect of condition varies depending on the age of the
participant? This situation would correspond to a case where (say) older
people were more sensitive to tea ordering, perhaps because of their
greater tea experience. We call this an \textbf{interaction} effect: the
effect of one predictor depends on the state of another.

Interaction effects are easily accommodated in our modeling framework.
We simply add a term to our model that is the product of condition
(\(X_1\)) and age (\(X_2\)), and weight this product by another beta,
which represents the strength of this interaction:

\[
Y_i = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2}  + \beta_3 X_{i1} X_{i2}  + \epsilon_i
\] Statistical interactions are a very powerful modeling tool that can
help us understand the relationship between different experimental
manipulations or between manipulations and covariates (such as age). We
discuss their role in experimental design -- as well as some of the
interpretive challenges that they pose -- in much more detail in
\textbf{?@sec-design}.\sidenote{\footnotesize We won't go into this topic here, but we
  do want to provide a pointer to one of the most persistent challenges
  that come up when you specify regression models with categorical
  predictors -- and especially their interactions: how you ``code''
  these categorical predictors. Above we created a variable \(X\) that
  encoded milk-first tea as 0 and tea-first tea as 1. This coding system
  is known as \textbf{dummy coding}, because \(X\) is a ``dummy''
  variable that stands in for milk-first tea. Dummy variables are very
  easy to think about, but in models with interactions, they can cause
  some problems. Because the interaction in our example model is a
  product of the dummy-coded condition variable and age, the interaction
  term \(\beta_3\) is interpreted as the effect of age \emph{for the
  tea-first condition} (\(X=1\)) and hence the effect of age \(\beta_2\)
  is actually the effect of age \emph{for the milk-first condition}. The
  way to deal with this issue is to use a different coding system, such
  as \textbf{contrast coding}. Davis
  (\protect\hyperlink{ref-davis2010}{2010}) gives a good tutorial on
  this tricky topic.}

\hypertarget{when-does-linear-regression-work}{%
\subsection{When does linear regression
work?}\label{when-does-linear-regression-work}}

Linear regression modeling with OLS is an incredibly powerful technique
for creating models to estimate the influence of multiple predictors on
a single dependent variable. In fact, OLS is in a mathematical sense the
\emph{best} way to fit a linear model!\sidenote{\footnotesize There is a precise sense
  in which OLS gives the \emph{very best} predictions we could ever get
  from any model that posits linear relationships between the
  independent variables and the outcome. That is, you can come up with
  any other linear, unbiased model you want, and yet if the assumptions
  of OLS are fulfilled, predictions from OLS will always be less noisy
  than those of your model. This is because of an elegant mathematical
  result called the Gauss-Markov Theorem.} But OLS only ``works'' -- in
the sense of yielding good estimates -- if three big conditions are met.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{The predictor relationships being modeled must be linear.} In
  our comparison of Alice's and Bob's expected outcomes based on their
  1-year age difference, we were able to interpret the coefficient
  \(\beta_2\) as the average difference in \(Y_i\) when comparing
  participants who differ by 1 year of age, \emph{regardless} of whether
  those ages are 20 vs.~21 or 50 vs.~51. But that's not always true:
  plenty of things vary \textbf{non-linearly} with age -- for example,
  imagine growth in height over age! Linear regression will give bad
  answers in such cases.\sidenote{\footnotesize One way to accommodate
    \textbf{non-linearities} is to modify the linear predictor to
    include polynomial terms, such as \(X_2^2\), which then allow us to
    fit a curve rather than just a straight line. It is always a good
    idea to use visualizations like scatter plots to look for possible
    problems with linearity.}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \textbf{Errors must be independent.} In our example, observations in
  the regression model (i.e., rows in the dataset) were sampled
  independently: each participant was recruited independently to the
  study and each performed a single trial. On the other hand, suppose we
  have repeated-measures data in which we sample participants, and then
  obtained multiple measurements for each participant. Within each
  participant, measurements would likely be correlated (perhaps because
  participants differ on their general level of tea enjoyment). This
  correlation can invalidate inferences from a model that does not
  accommodate the correlation. We'll discuss this problem in detail
  below.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Errors must be normally distributed and unrelated to the
  predictor.} Imagine older people have very consistent tea-ordering
  preferences while younger people do not. In that case, the models'
  error term would be less variable for older participants than younger
  ones. This issue is called \textbf{heteroskedasticity}. It is a good
  idea to plot each independent variable versus the residuals to see if
  the residuals are more variable for certain values of the independent
  variable than for others.
\end{enumerate}

If any of these three conditions are violated, it can undermine the
estimates and inferences you draw from your model.

\hypertarget{generalized-linear-models}{%
\section{Generalized linear models}\label{generalized-linear-models}}

So far we have considered continuous outcome measures, like tea ratings.
What if we instead had a binary outcome, such as whether a participant
liked or didn't like the tea, or a count outcome, such as the number of
cups a participant chose to drink? These and other non-continuous
outcomes often violate the assumptions of OLS, in particular because
they often induce heteroskedastic errors.

Binary outcomes inherently produce heteroskedastic errors because the
variance of a binary variable depends directly on the outcome
probability. Errors will be more variable when the outcome probability
is closer to 0.50, and much less variable for when the probability is
closer to 0 or 1.\sidenote{\footnotesize Specifically, the variance of a binary
  variable with probability \(p\) is simply \(p(1-p)\), which is largest
  when \(p=0.50\).} This heteroskedasticity in turn means that
inferences from the model (e.g., \(p\)-values) can be incorrect;
sometimes just a little bit off but sometimes dramatically
incorrect.\sidenote{\footnotesize OLS can also be used with binary outcomes, in which
  case the coefficients represent differences in probabilities. However,
  the usual model-based standard errors will be incorrect.}

Happily, \textbf{generalized linear models} (GLMs) are regression models
closely related to OLS that can handle non-continuous outcomes. These
models are called ``generalized'' because OLS is one of many members of
this large class of models. To see the connection, let's first write an
OLS model more generally in terms of what it says about the expected
value of the outcome, which we notate as \(E[Y_i]\):

\[
E[Y_i] = \beta_0 + \sum_{j=1}^p \beta_j X_{ij}
\]

where \(p\) is the number of independent variables, \(\beta_0\) is the
intercept, and \(\beta_j\) is the regression coefficient for the
\(j^{th}\) independent variable. This equation is just a math-y way of
saying that you predict from a regression model by adding up each of the
predictors' contributions to the expected outcome (\(\beta_j X_{ij}\)).

The linear predictor of a GLM (i.e.,
\(\beta_0 + \sum_{j=1}^p \beta_j X_{ij}\)) looks exactly the same as for
OLS, but instead of modeling \(E[Y_i]\), a GLM models some
\textbf{transformation}, \(g(.)\), of the expectation:

\[
g( E[Y_i] ) = \beta_0 + \sum_{j=1}^p \beta_j X_{ij} 
\]

GLMs involve transforming the \emph{expectation} of the outcome, not the
outcome itself! That is, in GLMs, we are not just taking the outcome
variable in our dataset and transforming it before fitting an OLS model,
but rather we are fitting a different model entirely, one that posits a
fundamentally different relationship between the predictors and the
expected outcomes. This transformation is called the \textbf{link
function}. In other words, to fit different kinds of outcomes, all we
need to do is construct a standard linear model and then just transform
its output via the appropriate link function.

Perhaps the most common link function is the \textbf{logit} link, which
is suitable for binary data. This link function looks like this, where
\(w\) is any probability that is strictly between 0 and 1:

\[g(w) = \log \left( \frac{w}{1 - w} \right)\]

The term \(w / (1 - w)\) is called the \textbf{odds} and represents the
probability of an event occurring divided by the probability of its not
occurring. The resulting model is called \textbf{logistic regression}
and looks like:

\[
\text{logit}( E[Y_{it}] ) = \log \left( \frac{ E[Y_i] }{1 - E[Y_i] } \right) = \beta_0 + \sum_{j=1}^p \beta_j X_{ij} 
\]

Exponentiating the coefficients (i.e., \(e^{\beta}\)) would yield
\textbf{odds ratios}, which are the \emph{multiplicative} increase in
the odds of \(Y_i=1\) that is associated with a one-unit increase in the
relevant predictor variable.

\begin{figure}

\sidecaption{\label{fig-models-logistic-ex}An example of how logistic
regression transforms a change in the mean-centered predictor X into a
change in the expected outcome Y. The same absolute change in X is
associated in a large difference in the probability of the outcome when
X is near its mean (blue) vs.~a small change in the outcome when X is
large (red) or small.}

{\centering \includegraphics{007-models_files/figure-pdf/fig-models-logistic-ex-1.png}

}

\end{figure}

Figure~\ref{fig-models-logistic-ex} shows the way that a logistic
regression model transforms a predictor (\(X\)) into an outcome
probability that is bounded at 0 and 1. Critically, although the
predictor is still linear, the logit link means that the same change in
\(X\) can result in a different change in the absolute probability of
\(Y\) depending on where you are on the \(X\) scale. In this example, if
you are in the middle of the predictor range, a one-unit change in \(X\)
results in a 0.24 change in probability (blue). At a higher value, the
change is much smaller (0.02. Notice how this is different from the
linear regression model above, where the same change in age always
resulted in the same change in preference!

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

GLMs are as easy to fit in R as standard LMs. You simply need to call
the \texttt{glm} function -- and to specify the link function. For our
example above of a binary ``liking'' judgment, the call would be:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glm}\NormalTok{(liked\_tea }\SpecialCharTok{\textasciitilde{}}\NormalTok{ condition, }\AttributeTok{data =}\NormalTok{ tea\_data, }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \texttt{family} argument specifies the type of distribution being
used, where ``binomial'' is the logistic link function.

\end{tcolorbox}

We have only scratched the surface of GLMs here. First, there are many
different link functions that are suitable for different outcome types.
And second, GLMs differ from OLS not only in their link functions, but
also in how they handle the error terms. Our broader goal in this
chapter is to show you how regression models are \emph{models of data}.
In that context, GLMs use link functions as a way to make models that
generate many different times types of outcome data.\sidenote{\footnotesize We
  sometimes think of linear models as a set of tinker toys you can snap
  together to stack up a set of predictors. In that context, link
  functions are an extra ``attachment'' that you can snap onto your
  linear model to make it generate a different response type.}

\hypertarget{linear-mixed-effects-models}{%
\section{Linear mixed effects
models}\label{linear-mixed-effects-models}}

Experimental data often contain multiple measurements for each
participant (so-called \textbf{repeated measures}). In addition, these
measurements are often based on a sample of stimulus items (which then
each have multiple measures as well). This clustering is problematic for
OLS models, because the error terms for each datapoint are not
independent.

Non-independence of datapoints may seem at first glance like a small
issue, but it can present a deep problem for making inferences. Take the
tea-tasting data we looked at above, where we had 24 observations in
each condition. If we fit an OLS model, we observe a highly significant
tea-first effect. Here is the estimate and confidence interval for that
coefficient: \(b = -2.42\), 95\% CI \([-3.50, -1.33]\). Based on what we
talked about in the previous chapter, it seems like we'd be licensed in
rejecting the null hypothesis that this effect is due to sampling
variation and interpret this instead as evidence for a generalizable
difference in tea preference in our sampled population.

But suppose we told you that all of those 48 total observations (24 in
each condition) were from one individual named George. That would change
the picture considerably. Now we'd have no idea whether the big effect
we observed reflected a difference in the population, but we would have
a very good sense of what George's preference is!\sidenote{\footnotesize We discuss
  the strengths and weaknesses of repeated-measures designs like this in
  \textbf{?@sec-design} and the statistical tradeoffs of having many
  people with a small number of observations per person vs.~a small
  number of people with many observations per person in
  \textbf{?@sec-sampling}.} The confidence intervals and p-values from
our OLS model would be wrong now because all of the error terms would be
highly correlated -- they would all reflect George's preferences.

How can we make models that deal with clustered data? There are a number
of widely-used approaches for solving this problem including
\textbf{linear mixed effects models}, \textbf{generalized estimating
equations}, and \textbf{clustered standard errors} (often used in
economics). Here we will illustrate how the problem gets solved in
linear mixed models, which are an extension of OLS models that are fast
becoming a standard in many areas of psychology
(\protect\hyperlink{ref-bates2014}{Bates et al. 2014}).

\hypertarget{modeling-random-variation-in-clusters}{%
\subsection{Modeling random variation in
clusters}\label{modeling-random-variation-in-clusters}}

In linear mixed effects models, we modify the linear predictor itself to
model differences across clusters. Instead of just measuring George's
preferences, suppose we modified the original tea-tasting experiment
(without the age covariate) to collect ten ratings from each
participant: five milk-first and five tea-first. We define the model the
same way as we did before, with some minor differences:

\[
Y_{it} = \beta_0 + \beta_1 X_{it} + \gamma_i + \epsilon_{it} 
\]

where \(Y_{it}\) is participant \(i\)'s rating in trial \(t\) and
\(X_{it}\) is the participant's assigned treatment in trial \(t\) (i.e.,
milk-first or tea-first).

If you compare this equation to the OLS equation above, you will notice
that we added two things. First, we've added subscripts that distinguish
trials from participants. But the big one is that we added \(\gamma_i\),
a separate intercept value for each participant. We call this a
\textbf{random intercept} because it varies across participants (who are
randomly selected from the population).\sidenote{\footnotesize Formally, we'd notate
  this random variation by saying that \(\gamma_i \sim N(0, \tau^2)\) --
  in other words, that participants' random intercepts are sampled from
  a normal distribution around the shared intercept \(\beta_0\) with
  standard deviation \(\tau\).}

The random intercept means that we have assumed that each participant
has their own typical ``baseline'' tea rating -- some participants
overall just like tea more than others -- and these baseline ratings are
normally distributed across participants. Thus, ratings are correlated
within participants because ratings cluster around each participant's
\emph{distinct} baseline tea rating. This model is better able to block
misleading inferences. For example, suppose we only had one participant
in each condition (say, George provided 24 milk-first ratings and Alice
provided 24 tea-first ratings). If we found higher ratings in one
condition, we would be able to attribute this difference to
participant-level variation rather than to the treatment.\sidenote{\footnotesize Of
  course, this would be a terrible experiment! Ideally, we would address
  this problem upstream in our experiment design; see
  \textbf{?@sec-design}.}

Following the same logic, we could fit random intercepts for different
stimulus items (for example, if we used different types of tea for
different trials). We modeled participants as having normally
distributed variation, and we can model stimulus variation the same way.
Each stimulus item is assumed to produce a particular average outcome
(i.e.~some teas are tastier than others), with these average outcomes
sampled from a normally distributed population.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Remarkably, GLMMs are not much harder to specify in R than standard LMs.
One very popular package is \texttt{lme4}
(\protect\hyperlink{ref-bates2014}{Bates et al. 2014}), which provides
the \texttt{lmer} and \texttt{glmer} functions (the latter for
generalized linear mixed effect models). For our example here, we'd
write:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(lme4)}
\FunctionTok{lmer}\NormalTok{(rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ condition }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1} \SpecialCharTok{|}\NormalTok{ id), }\AttributeTok{data =}\NormalTok{ tea\_data)}
\end{Highlighting}
\end{Shaded}

In this model, the syntax \texttt{(1\ \textbar{}\ id)} specifies that we
want a random intercept for each level of \texttt{id}.

\end{tcolorbox}

\hypertarget{random-slopes-and-the-challenges-of-mixed-effects-models}{%
\subsection{Random slopes and the challenges of mixed effects
models}\label{random-slopes-and-the-challenges-of-mixed-effects-models}}

Linear mixed effects models can be further extended to model clustering
of the independent variables' \emph{effects} within subjects, not just
clustering of average \emph{outcomes} within subjects. To do so, we can
introduce \textbf{random slopes} (\(\delta_i\)) to the model, which are
multiplied by the condition variable \(X\) and represent differences
across participants in the effect of tea-tasting:

\[
Y_i = \beta_0 + \beta_1 X_{it} + \gamma_i + \delta_{i} X_{it} + \epsilon_{it} 
\]

Just like the random intercepts, these random slopes will be assumed to
vary across participants, following a normal distribution.\sidenote{\footnotesize These
  random slopes and intercepts can be assumed to be independent or
  correlated with one another, depending on the modeler's preference.}

This model now describes random variation in both overall how much
someone likes tea \emph{and} how strong their ordering preference is.
Both of these likely do vary in the population and so it seems like a
good thing to put these in your model. Indeed under some circumstances,
adding random slopes is argued to be very important for making
appropriate inferences.\sidenote{\footnotesize There's lots of debate in the
  literature about the best random effect structure for mixed effects
  models. This is a very tricky and technical subject. In brief, some
  folks argue for so-called \textbf{maximal} models, in which you
  include every random effect that is justified by the design
  (\protect\hyperlink{ref-barr2013}{Barr et al. 2013}). Here that would
  mean including random slopes for each participant. The problem is that
  these models can get very complex, and can be very hard to fit using
  standard software. We won't weigh in on this topic, but as you start
  to use these models on more complex experimental designs, it might be
  worth reading up.}

On the other hand, the model is much more complicated. When we had a
simple OLS model above, we had only two parameters to fit (\(\beta_0\)
and \(\beta_1\)) but now we have those two plus two more, representing
the standard deviations of the individual participant intercepts and
slopes, plus parameters for each participant and for the condition
effect for each participant. So we went from two parameters to
24!\sidenote{\footnotesize Though we should note that these parameters aren't
  technically all independent from one another due to the structure of
  the mixed effect model.} This complexity can lead to problems in
fitting the models, especially with very small datasets (where these
parameters are not very well-constrained by the data) or very large
datasets (where computing all these parameters can be
tricky).\sidenote{\footnotesize Many R users may be familiar with the widely-used
  \texttt{lme4} package for fitting mixed effects models using
  frequentist tools related to maximum likelihood. Such models can also
  be fit using Bayesian inference with the \texttt{brms} package, which
  provides many powerful methods for specifying complex models.}

More generally, linear mixed effects models are very flexible, and they
have become quite common in psychology. But they do have significant
limitations. As we discussed, they can be tricky to fit in standard
software packages. Further, the accuracy of these models relies on our
ability to specify the structure of the random effects
correctly.\sidenote{\footnotesize One particularly problematic situation is when the
  correlation structure of the errors is mis-specified, for example if
  observations within a participant are more correlated for participants
  in the treatment group than in the control group; in such cases, mixed
  model estimates can be substantially biased
  (\protect\hyperlink{ref-bie2021fitting}{Bie et al. 2021}).} If we
specify an incorrect model, our inferences will be wrong! But it is
sometimes difficult to know how to check whether your model is
reasonable, especially with a small number of clusters or observations.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Specifying random slopes in the \texttt{lme4} package is also relatively
straightforward:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lmer}\NormalTok{(rating }\SpecialCharTok{\textasciitilde{}}\NormalTok{ condition }\SpecialCharTok{+}\NormalTok{ (condition }\SpecialCharTok{|}\NormalTok{ id), }\AttributeTok{data =}\NormalTok{ tea\_data)}
\end{Highlighting}
\end{Shaded}

Here, \texttt{(condition\ \textbar{}\ id)} means ``a separate random
slope for \texttt{condition} should be fit for each level of
\texttt{id}.'' Of course, specifying such a model is easier than fitting
it correctly.

\end{tcolorbox}

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{an-alternative-approach-generalized-estimating-equations}{%
\section*{An alternative approach: Generalized estimating
equations}\label{an-alternative-approach-generalized-estimating-equations}}
\addcontentsline{toc}{section}{An alternative approach: Generalized
estimating equations}

\markright{An alternative approach: Generalized estimating equations}

A second class of methods that helps resolve issues of clustering is
\textbf{generalized estimating equations} (GEE). In this approach, we
leave the linear predictor alone. We do not add random intercepts or
slopes, nor do we assume anything about the distribution of the errors
(i.e., we no longer assume that they are normal, independent, and
homoskedastic).

In GEE, we instead provide the model with an initial ``guess'' about how
we think the errors might be related to one another; for example, in a
repeated-measures experiment, we might guess that the errors are
exchangeable, meaning that they are correlated to the same degree within
each participant but are uncorrelated across participants. Instead of
\emph{assuming} that our guess is correct, as do linear mixed models
(LMM), GEE estimates the correlation structure of the errors
empirically, using our guess as a starting point, and it uses this
correlation structure to arrive at point estimates and inference for the
regression coefficients. Remarkably, as the number of clusters and
observations become very large, GEE will \emph{always} provide unbiased
point estimates and valid inference, \emph{even if} our guess about the
correlation structure was bad. Additionally, with simple finite-sample
corrections (\protect\hyperlink{ref-mancl2001covariance}{Mancl and
DeRouen 2001}), GEE seems to provide valid inference at smaller numbers
of clusters than does LMM.

The price paid for these nice safeguards against model misspecification
is that, in principle, GEE will typically have less statistical power
than LMM \emph{if} the LMM is in fact correctly specified, but the
difference may be surprisingly slight in practice
(\protect\hyperlink{ref-bie2021fitting}{Bie et al. 2021}). For these
reasons, some of this book's authors actually favor GEE with
finite-sample corrections over LMM as the default model for clustered
data, though they are much less common in psychology.

\end{tcolorbox}

\hypertarget{how-do-you-use-models-to-analyze-data}{%
\section{How do you use models to analyze
data?}\label{how-do-you-use-models-to-analyze-data}}

In the prior parts of this chapter, we've described a suite of
regression-based techniques -- standard OLS, the generalized linear
model, and linear mixed effects models -- that can be used to model the
data resulting from randomized experiments (as well as many other kinds
of data). The advantage of regression models over the simpler estimation
and inference methods we described in the prior two chapters is that
these models can more effectively take into account a range of different
kinds of variation including covariates, multiple manipulations, and
clustered structure. Further, when used appropriately to analyze a
well-designed randomized experiment, regression models can give an
unbiased estimate of a causal effect of interest, our main goal in doing
experiments.

But -- practically speaking -- how should go you about building a model
for your experiment? What covariates should you include and what should
you leave out? There are many ways to use models to explore datasets,
but in this section we will try to sketch a default approach for the use
of models to estimate causal effects in experiments in the most
straightforward way. Think of this as a starting point. We'll begin this
section by giving a set of rules of thumb, then discuss a worked
example. Our final subsections will deal with the issues of when you
should include covariates in your model and how to check if your result
is robust across multiple different model specifications.

\hypertarget{modeling-rules-of-thumb}{%
\subsection{Modeling rules of thumb}\label{modeling-rules-of-thumb}}

Our approach to statistical modeling is to start with a ``default
model'' that is known in the literature as a \textbf{saturated model}.
The saturated model of an experiment includes the full design of the
experiment -- all main effects and interactions -- and nothing else. If
you are manipulating a variable, include it in your model. If you are
manipulating two, include them both and their interaction. If your
design includes repeated measurements for participants, include a random
effect of participant; if it includes experimental items for which
repeated measurements are made, include random effect of
stimulus.\sidenote{\footnotesize As discussed above, you can also include the
  ``maximal'' random effect structure
  (\protect\hyperlink{ref-barr2013}{Barr et al. 2013}), which involves
  random slopes as well as intercepts -- but recognize that you cannot
  always fit such models.}

Don't include lots of other stuff in your default model. You are doing a
randomized experiment, and the strength of randomized experiments is
that you don't have to worry about confounding based on the population
(see \textbf{?@sec-experiments}). So don't put a lot of covariates in
your default model -- usually don't put in any!\sidenote{\footnotesize One corollary
  to having this kind of default perspective on data analysis: When you
  see an analysis that deviates substantially from the default, these
  deviations should provoke some questions. If someone drops a
  manipulation from their analysis, adds a covariate or two, or fails to
  control for some clustering in the data, did they deviate because of
  different norms in their sub-field, or was there some other rationale?
  This line of reasoning sometimes leads to questions about the extent
  to which particular analytic decisions are post-hoc and driven by the
  data (in other words, \(p\)-hacked). For an example, see the case
  study in \textbf{?@sec-prereg}.}

This default saturated model then represents a simple summary of your
experimental results. Its coefficients can be interpreted as estimates
of the effects of interest, and it can be used as the basis for
inferences about the relation of the experimental effect to the
population using either frequentist or Bayesian tools.

Here's a bit more guidance about this modeling strategy.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Preregister your model}. If you change your analysis approach
  after you see your data, you risk \(p\)-hacking -- choosing an
  analysis that biases the estimate of your effect of interest. As we
  discussed in \textbf{?@sec-replication} and as we will discuss in more
  detail in \textbf{?@sec-prereg}, one important strategy for minimizing
  this problem is to \textbf{preregister} your analysis.\sidenote{\footnotesize A side
    benefit of preregistration is it makes you think through whether
    your experimental design is appropriate -- that is, is there
    actually an analysis capable of estimating the effect you want from
    the data you intend to collect?}
\item
  \textbf{Visualize the model predictions against the observed data}. As
  we'll discuss in \textbf{?@sec-viz}, the ``default model'' for an
  experiment should go alongside a ``default visualization'' known as
  the \textbf{design plot} that similarly reflects the full design
  structure of the experiment and any primary clusters. In visualizing
  data like those from our tea-tasting experiment, show individual
  participants' average ratings (as in Figure~\ref{fig-models-ex-viz}).
  One way to check whether a model fits your data is then to plot it on
  top of those data. Sometimes this combination of model and data can be
  as simple as a scatter plot with a regression line. But seeing the
  model plotted alongside the data can often reveal a mismatch between
  the two. A model that does not describe the data very well is not a
  good source of generalizable inferences!
\end{enumerate}

\begin{marginfigure}

{\centering \includegraphics{007-models_files/figure-pdf/fig-models-ex-viz-1.png}

}

\caption{\label{fig-models-ex-viz}Raw data, means, and confidence
intervals for the tea-tasting experiment.}

\end{marginfigure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{Interpret the model predictions}. Once you have a model, don't
  just read off the \(p\)-values for your coefficients of interest. Walk
  through the each coefficient, considering how it relates to your
  outcome variable. For a simple two group design like we've been
  considering, the condition coefficient is the estimate of the causal
  effect that you intended to measure! Consider its sign, its magnitude,
  and its precision (standard error or confidence interval).
\end{enumerate}

That said, there are some contexts in which it does make sense to depart
from the default saturated model. For example, there may be insufficient
statistical power to estimate multiple interaction terms, or covariates
might be included in the model to help handle certain forms of missing
data. The default model simply represents a very good starting point.

\hypertarget{a-worked-example}{%
\subsection{A worked example}\label{a-worked-example}}

\begin{marginfigure}

{\centering \includegraphics{images/models/sgf.png}

}

\caption{\label{fig-models-sgf-stim}Example stimulus materials from
Stiller, Goodman, and Frank
(\protect\hyperlink{ref-stiller2015}{2015}).}

\end{marginfigure}

All this advice may seem abstract, so let's put it into practice on a
simple example. For a change, let's look at an experiment that's not
about tea tasting. Here we'll consider data from an experiment testing
preschool children's language comprehension
(\protect\hyperlink{ref-stiller2015}{Stiller, Goodman, and Frank 2015}).
In this experiment, 2--5 year old children saw displays like the one in
Figure~\ref{fig-models-sgf-stim}. In the experimental condition, a
puppet might say, for example, ``My friend has glasses! Which one is my
friend?'' The goal was to measure how many children made the ``pragmatic
inference'' that the puppet's friend was the face with glasses and
\emph{no} hat.

To estimate the effect, participants were randomly assigned to either
the experimental condition or to a control condition in which the puppet
had eaten too much peanut butter and couldn't talk, but they still had
to guess which face was his friend. There were also three other types of
experimental stimuli (houses, beds, and plates of pasta). Data from this
experiment consisted of 588 total observations from 147 children, with
all four stimuli presented to each child. The primary hypothesis of this
experiment was that that preschool children could make pragmatic
inferences by correctly inferring which of the three faces (for example)
the puppet was describing.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

If you want to follow along with this example, you'll have to load the
example data and do a little bit of preprocessing (also covered in
\textbf{?@sec-tidyverse}):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{repo }\OtherTok{\textless{}{-}} \StringTok{"https://raw.githubusercontent.com/langcog/experimentology/main/"}
\NormalTok{sgf }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(repo, }\StringTok{"data/tidyverse/stiller\_scales\_data.csv"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age\_group =} \FunctionTok{cut}\NormalTok{(age, }\DecValTok{2}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\AttributeTok{include.lowest =} \ConstantTok{TRUE}\NormalTok{), }
         \AttributeTok{condition =} \FunctionTok{fct\_recode}\NormalTok{(condition,}
                                \StringTok{"Experimental"} \OtherTok{=} \StringTok{"Label"}\NormalTok{,}
                                \StringTok{"Control"} \OtherTok{=} \StringTok{"No Label"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

All this advice may seem abstract, so let's put it into practice on a
simple example. For a change, let's look at an experiment that's not
about tea tasting. Here we'll consider data from an experiment testing
preschool children's language comprehension \textbf{?@sec-tidyverse}. In
this experiment, 2--5 year old children saw displays like the one in
Figure~\ref{fig-models-sgf-stim}. In the experimental condition, a
puppet might say, for example, ``My friend has glasses! Which one is my
friend?'' The goal was to measure how many children made the ``pragmatic
inference'' that the puppet's friend was the face with glasses and
\emph{no} hat.

To estimate the effect, participants were randomly assigned to either
the experimental condition or to a control condition in which the puppet
had eaten too much peanut butter and couldn't talk, but they still had
to guess which face was his friend. There were also three other types of
experimental stimuli (houses, beds, and plates of pasta). Data from this
experiment consisted of 588 total observations from 147 children, with
all four stimuli presented to each child. The primary hypothesis of this
experiment was that that preschool children could make pragmatic
inferences by correctly inferring which of the three faces (for example)
the puppet was describing.

\begin{figure}

\sidecaption{\label{fig-models-sgf-plot}Data for Stiller, Goodman, and
Frank (\protect\hyperlink{ref-stiller2015}{2015}). Each point shows a
single participant's proportion correct trials (out of 4 experimental
stimuli) plotted by age group, jittered slightly to avoid overplotting.
Larger points and associated confidence intervals show mean and 95\%
confidence intervals for each condition.}

{\centering \includegraphics{007-models_files/figure-pdf/fig-models-sgf-plot-1.png}

}

\end{figure}

This experimental design looks a lot like some versions of our
tea-tasting experiment. We have one primary condition manipulation (the
puppet provides information versus does not), presented
between-participants so that some participants are in the experimental
condition and others are in the control condition. Our measurements are
repeated within participants across different experimental stimuli.
Finally, we have one important, pre-planned covariate: children's age.
Experimental data are plotted in
Figure~\ref{fig-models-sgf-plot}.\sidenote{\footnotesize Our sampling plan for this
  experiment was actually \textbf{stratified} across age, meaning that
  we intentionally recruited the same number of participants for each
  one-year age group -- because we anticipated that age was highly
  correlated with children's ability to succeed in this task. We'll
  describe this kind of sampling in more detail in
  \textbf{?@sec-sampling}.}

How should we go about making our default model for this
dataset?\sidenote{\footnotesize This experiment was not preregistered, but the paper
  includes a separate replication dataset with the same analysis.} We
simply include each of these design factors in a mixed effects model; we
use a logistic link function for our mixed effects model (a
\textbf{generalized linear mixed effects model}) because we would like
to predict correct performance on each trial, which is a binary
variable. So that gives us an effect of condition and age as a
covariate. We further add an interaction between condition and age in
case the condition effect varies meaningfully across groups. Finally, we
add random effects of participant, \(\gamma_i\), and experimental item,
\(\gamma_t\).\sidenote{\footnotesize As discussed above, this is a tricky
  decision-point; we could very reasonably have added random slopes as
  well.}

The resulting model looks like this:

\[
\text{logit}( E[Y_{it}] ) = \beta_0 + \beta_1 X_{i1} + \beta_2 X_{i2} + \beta_3 X_{i1} X_{i2} + \gamma_i + \delta_t 
\]

Let's break this complex equation down left to right:

\begin{itemize}
\tightlist
\item
  \(\text{logit}( E[Y_{it}] )\) says that we are predicting a logistic
  function of \(E[Y_{it}]\) (where \(Y_{it}\) indicates whether child
  \(i\) was correct on trial \(t\)).
\item
  \(\beta_0\) is the \textbf{intercept}, our estimate of the average
  log-odds (i.e., the log of the odds ratio) of correct responses for
  participants in the control condition.
\item
  \(\beta_1 X_{i1}\) is the condition predictor. \(\beta_1\) represents
  the change in log-odds associated with being in the experimental
  condition (the causal effect of interest!), and \(X_{i1}\) is an
  indicator variable that is 1 if child \(i\) is in the experimental
  condition and 0 for the control condition. Multiplying \(\beta_1\) by
  this indicator means that the predictor has the value 0 for
  participants in the control condition and \(\beta_1\) for those in the
  experimental condition.
\item
  \(\beta_2 X_{i2}\) is the age predictor. \(\beta_2\) represents the
  difference in log-odds associated with one additional year of age for
  participants int he control condition{[}The age coefficient is a
  \textbf{simple effect}, meaning it is the effect of age in the control
  condition only. That's because we have dummy coded the condition
  predictor. If we wanted the average age effect (the \textbf{main
  effect}) then we would need to use contrast coding, per the note in
  the Interactions section above.{]}, and \(X_{i2}\) is the age for each
  participant.\sidenote{\footnotesize We have \textbf{centered} our age predictor in
    this example so that all estimates from our model are for the
    average age of our participants. Centering is a good practice for
    modeling continuous predictors because it increases the
    interpretability of other parts of the model. For example, because
    age is centered in this model, the intercept \(\beta_0\) can be
    interpreted as the predicted odds of a correct trial for a
    participant in the control condition at the average age.}
\item
  \(\beta_3 X_{i1} * X_{i2}\) is the interaction between experimental
  condition and age. \(\beta_3\) represents the difference in log odds
  (i.e., the log of the odds ratio) that is associated with being one
  year older \emph{and} in the experimental condition versus the control
  condition. This term is multiplied by both each child's age \emph{and}
  the condition indicator \(X_i\).
\item
  \(\gamma_i\) is the random intercept for participant \(i\), capturing
  individual variation in the odds of success across trials.
\item
  \(\gamma_t\) is the random intercept for stimulus \(t\), capturing
  variation in the odds of success across the four different stimuli.
\end{itemize}

\footnotesize

\hypertarget{tbl-models-sgf-model-print}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.4085}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1268}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2113}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1408}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.1127}}@{}}
\caption{\label{tbl-models-sgf-model-print}Estimated effects for our
generalized linear mixed effects model on data from Stiller, Goodman,
and Frank (2015).}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
conf.int
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
p.value
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
term
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
estimate
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
conf.int
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
statistic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
p.value
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Control condition & 0.80 & {[}0.42, 1.18{]} & 4.16 & \textless{} .001 \\
Age (years) & 0.55 & {[}0.21, 0.88{]} & 3.19 & .001 \\
Expt condition & -2.26 & {[}-2.70, -1.82{]} & -10.07 & \textless{}
.001 \\
Age (years) * Expt condition & -0.92 & {[}-1.43, -0.42{]} & -3.60 &
\textless{} .001 \\
\end{longtable}

\normalsize

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

To fit the model described above, the first step is to prepare your
predictors. In this case, we center the \texttt{age} predictor.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sgf}\SpecialCharTok{$}\NormalTok{age\_centered }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(sgf}\SpecialCharTok{$}\NormalTok{age, }\AttributeTok{scale=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Again we use the \texttt{lme4} package, this time with the
\texttt{glmer} function. Again we have to specify our link function,
just like in a standard GLM, by choosing the distribution family.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{glmer}\NormalTok{(correct }\SpecialCharTok{\textasciitilde{}}\NormalTok{ age\_centered }\SpecialCharTok{*}\NormalTok{ condition }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{subid) }\SpecialCharTok{+}\NormalTok{ (}\DecValTok{1}\SpecialCharTok{|}\NormalTok{item), }
             \AttributeTok{family =} \StringTok{"binomial"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ sgf)}
\end{Highlighting}
\end{Shaded}

You can see a summary of the fitted model using \texttt{summary(mod)} as
before. The only big difference from \texttt{lm} is that here you can
extract both fixed and random effects (with \texttt{fixef(mod)} and
\texttt{ranef(mod)} respectively).

\end{tcolorbox}

Let's estimate this model and see how it looks. We'll focus here on
interpretation of the so-called \textbf{fixed effects} (the main
predictors), as opposed to the participant and item random
effects.\sidenote{\footnotesize Participant means are estimated to have a standard
  deviation of 0.23 (in log-odds) while items have a standard deviation
  of 0.27. These indicate that both of our random effects capture
  meaningful variation.} Table~\ref{tbl-models-sgf-model-print} shows
the coefficients. Again, let's walk through each.

\begin{itemize}
\item
  The \textbf{intercept} (control condition estimate) is
  \(\hat{\beta} = 0.80\), 95\% CI \([0.42, 1.18]\), \(z = 4.16\),
  \(p < .001\). This estimate reflects that the log-odds of a correct
  response for an average-age participant in the control condition is
  0.8, which corresponds to a probability of 0.69. If we look at
  Figure~\ref{fig-models-sgf-plot}), that estimate makes sense: 0.69
  seems close to the average for the control condition.
\item
  The \textbf{age effect} estimate is \(\hat{\beta} = 0.55\), 95\% CI
  \([0.21, 0.88]\), \(z = 3.19\), \(p = .001\). This means there is a
  slight decrease in the log-odds of a correct response for older
  children in the control condition. Again, looking at
  Figure~\ref{fig-models-sgf-plot}, this estimate is interpretable: we
  see a small decline in the probability of a correct response for the
  oldest age group.
\item
  The key experimental condition estimate then is . This estimate means
  that the log-odds of a correct response for an average-age participant
  in the experimental condition is the sum of the estimates for the
  control (intercept) and the experimental conditions: 0.8 + -2.26,
  which corresponds to a probability of 0.19. Grounding our
  interpretation in Figure~\ref{fig-models-sgf-plot}, this estimate
  corresponds to the average value for the experimental condition.
\item
  Finally, the \textbf{interaction} of age and condition is . This
  positive coefficient reflects that with every year of age, the
  difference between control and experimental conditions grows.
\end{itemize}

In sum, this model suggests that there was a substantial difference in
performance between experimental and control conditions, in turn
supporting the hypothesis that children in the sampled age group can
perform pragmatic inferences above chance.

This example illustrates the ``default saturated model'' framework that
we recommend -- the idea that a single regression model corresponding to
the design of the experiment can yield an interpretable estimate of the
causal effect of interest, even in the presence of other sources of
variation.

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{when-does-it-makes-sense-to-include-covariates-in-a-model}{%
\section*{When does it makes sense to include covariates in a
model?}\label{when-does-it-makes-sense-to-include-covariates-in-a-model}}
\addcontentsline{toc}{section}{When does it makes sense to include
covariates in a model?}

\markright{When does it makes sense to include covariates in a model?}

Let's come back to one piece of advice that we gave above about making a
``default'' model of an experiment: not including covariates. This
advice can seem surprising. Many demographic factors are of interest to
psychologists and other behavioral scientists, and in observational
studies these factors will almost always be related to important life
outcomes. So why not put them into our experimental models? After all,
we did include age in our worked example above!

Well, if you have one or at most a small handful of covariates that you
believe are meaningfully related to the outcome, you \emph{can} plan in
advance to put them in your model. If you think that your effect is
likely to be \textbf{moderated} a specific demographic characteristic --
as we did with age in our developmental example above -- then this
inclusion can be quite useful.

Further, including covariates can increase the precision of your
estimates by reducing ``noise'' in your outcome, if you hypothesize that
they interact. What's surprising though is how \emph{little} this
adjustment does to increase your overall precision unless the
correlation between covariate and outcome is very strong.

Figure~\ref{fig-models-precision} shows the relationship between
estimation error and the inclusion of covariates via a simple
simulation. Only when the correlation between covariate and outcome
(e.g., age and tea rating is greater than \(r=0.6\) to \(r=0.8\) does
this adjustment really help.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{007-models_files/figure-html/models-precision-1.png}

}

\caption{\label{fig-models-precision}Decreases in estimation error due
to adjusting for covariates, plotted by the N participants in each group
and the correlation between the covariate and the outcome.}

\end{figure}

That said, there are quite a few reasons not to include covariates --
motivating our recommendation to skip them in your default model unless
you have very strong theory-based expectations for either (A) a
correlation with the outcome or (B) a strong moderation relationship.

The first is simply because we don't need to. Because randomization cuts
causal links, our experimental estimate is an unbiased estimate of the
causal effect of interest (at least for large samples). We are
guaranteed that, in the limit of many different experiments, even though
people with different ages will be in the different tea tasting
conditions, this source of variation will be averaged out. Actually,
including unnecessary covariates into models (slightly) decreases the
probability that the model can detect a true effect (that is, it
decreases statistical precision and power). Just by chance covariates
can ``soak up'' variation in the outcome, leaving less to be accounted
for by the true effect!

The second reason is that you can actually compromise your causal
inference by including some covariates, particularly those that are
collected \emph{after} randomization. The logic of randomization is that
you cut all causal links between features of the sample and the
condition manipulation. But you can ``uncut'' these links by accident by
adding variables into your model that are related to group status. This
problem is generically called \textbf{conditioning on post-treatment
variables} and a full discussion of is out of the scope of this book,
but it's something to avoid (and read up on if you're worried about it;
Montgomery, Nyhan, and Torres
(\protect\hyperlink{ref-montgomery2018}{2018})).

Finally, one of the standard justifications for adding covariates --
because your groups are unbalanced -- is actually ill-founded as well.
People often talk about ``unhappy randomization'': you randomize to the
different tea-tasting groups, for example, but then it turns out the
mean age is a bit different between groups. Then you do a t-test or some
other statistical test and find out that you actually have a significant
age difference. This practice makes no sense! Because you randomized,
you know that the difference in ages occurred by chance. Further,
incidental demographic differences between groups are unlikely to be
important unless that characteristic is highly correlated with the
outcome (see above). Instead, if the sample size is small enough that
meaningfully large incidental differences could arise in important
confounders, then it is preferable to stratify randomization on that
confounder at the outset (and then also to include that covariate in
modeling).

So these are our options: if a covariate is known to be very strongly
related to our outcome, we can include it in our default model.
Otherwise, we avoid a lot of trouble by leaving covariates out.

\end{tcolorbox}

\hypertarget{robustness-checks-and-the-multiverse}{%
\subsection{Robustness checks and the
multiverse}\label{robustness-checks-and-the-multiverse}}

Using the NHST statistical testing approach that has been common in the
psychology literature, even a simple two factor experimental design
affords a host of different \(t\)-tests and ANOVAs,\sidenote{\footnotesize Although we
  don't cover this point here, ANOVAs are also a special case of
  regression.} offering many opportunities for \(p\)-hacking and
selective reporting. We've been advocating here instead for a ``default
model'' approach in which you pre-plan and pre-register a single
regression model that captures the planned features of your experimental
design including manipulations and sources of clustering. This approach
can help you to navigate some of the complexity of data analysis by
having a standard approach that you take in almost every case.

Not every dataset will be amenable to this approach, however. For
complex experimental designs or unusual measures, sometimes it can be
hard to figure out how to specify or fit the default saturated model.
And especially in these cases, the choice of model can make a big
difference to the magnitude of the reported effect. To quantify
variability in effect size due to model choice, ``Many Analysts''
projects have asked a set of teams to approach a dataset using different
analysis methods. The consistent result from these projects has been
that there is substantial variability in outcomes depending on what
approach is taken (\protect\hyperlink{ref-silberzahn2018}{Silberzahn et
al. 2018}; \protect\hyperlink{ref-botvinik-nezer2020}{Botvinik-Nezer et
al. 2020}).

\textbf{Robustness analysis} (also sometimes called ``sensitivity
analysis'' or ``multiverse analysis'', which sounds cooler) is a
technique for addressing the possibility that an individual analysis
over- or under-estimates a particular effect by chance
(\protect\hyperlink{ref-steegen2016}{Steegen et al. 2016}). The general
idea is that analysts explore a space of different possible analyses. In
its simplest form, alternative model specifications can be reported in a
supplement; more sophisticated versions of the idea call for averaging
estimates across a range of possible specifications and reporting this
average as the primary effect estimate.

The details of this kind of analysis will vary depending on what you are
worried about your model being sensitive to. One analyst might be
concerned about the effects of adding different covariates; another
might be using a Bayesian framework and be concerned about sensitivity
to particular prior values. If you get similar results across many
different specifications, you can sleep better at night. The primary
principle to take home is a bit of humility about our models. Any given
model is likely wrong in some of its details. Investigating the
sensitivity of your estimates to the details of your model specification
is a good idea.

\hypertarget{chapter-summary-models}{%
\section{Chapter summary: Models}\label{chapter-summary-models}}

In the last three chapters, we have spelled out a framework for data
analysis that focuses on our key experimental goal: a measurement of a
particular causal effect. We began with basic techniques for estimating
effects and making inferences about how these effects estimated from a
sample can be generalized to a population. This chapter showed how these
ideas naturally give rise to the idea of making models of data, which
allow estimation of effects in more complex designs. Simple regression
models, which are formally identical to other inference methods in the
most basic case, can be extended with the generalized linear model as
well as with mixed effects models. Finally, we ended with some guidance
on how to build a ``default model'' -- an (often pre-registered)
regression model that maps onto your experimental design and provides
the primary estimate of your key causal effect.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Choose a paper that you have read for your research and take a look at
  the statistical analysis. Does the reporting focus more on hypothesis
  testing or on estimating effect sizes?
\item
  We focused here on the linear model as a tool for building models,
  contrasting this perspective with the common ``statistical testing''
  mindset. But -- here's the mind-blowing thing -- most of those
  statistical tests are special cases of the linear model anyway. Take a
  look at this extended meditation on the equivalences between tests and
  models:
  \url{https://lindeloev.github.io/tests-as-linear/\#9_teaching_materials_and_a_course_outline}.
  If the paper you chose for question 1 used tests, could their tests be
  easily translated to models? How would the use of a model-based
  perspective change the results section of the paper?
\item
  Take a look at this cool visualization of hierarchical (mixed effect)
  models: \url{http://mfviz.com/hierarchical-models/}. In your own
  research, what are the most common units that group together your
  observations?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\item
  An opinionated practical guide to regression modeling and data
  description: Gelman, A., Hill, J., \& Vehtari, A. (2020).
  \emph{Regression and other stories}. Cambridge University Press. Free
  online at \url{https://avehtari.github.io/ROS-Examples/}.
\item
  A more in-depth introduction to the process of developing Bayesian
  models of data that allow for estimation and inference in complex
  datasets: McElreath, R. (2020). \emph{Statistical rethinking: A
  Bayesian course with examples in R and Stan}. Chapman and Hall/CRC.
  Free materials available at
  \url{https://xcelab.net/rm/statistical-rethinking/}.
\end{itemize}

\end{tcolorbox}

\leavevmode\vadjust pre{\hypertarget{planning}{}}%
\part{Planning}

\hypertarget{bibliography-9}{%
\section*{References}\label{bibliography-9}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-9}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-barr2013}{}}%
Barr, Dale J, Roger Levy, Christoph Scheepers, and Harry J Tily. 2013.
{``Random Effects Structure for Confirmatory Hypothesis Testing: Keep It
Maximal.''} \emph{Journal of Memory and Language} 68 (3): 255--78.

\leavevmode\vadjust pre{\hypertarget{ref-bates2014}{}}%
Bates, Douglas, Martin Mchler, Ben Bolker, and Steve Walker. 2014.
{``Fitting Linear Mixed-Effects Models Using Lme4.''} \emph{arXiv
Preprint arXiv:1406.5823}.

\leavevmode\vadjust pre{\hypertarget{ref-bie2021fitting}{}}%
Bie, Ruofan, Sebastien Haneuse, Nathan Huey, Jonathan Schildcrout, and
Glen McGee. 2021. {``Fitting Marginal Models in Small Samples: A
Simulation Study of Marginalized Multilevel Models and Generalized
Estimating Equations.''} \emph{Statistics in Medicine} 40 (24):
5298--5312.

\leavevmode\vadjust pre{\hypertarget{ref-botvinik-nezer2020}{}}%
Botvinik-Nezer, Rotem, Felix Holzmeister, Colin F Camerer, Anna Dreber,
Juergen Huber, Magnus Johannesson, Michael Kirchler, et al. 2020.
{``Variability in the Analysis of a Single Neuroimaging Dataset by Many
Teams.''} \emph{Nature} 582 (7810): 84--88.

\leavevmode\vadjust pre{\hypertarget{ref-davis2010}{}}%
Davis, Matthew J. 2010. {``Contrast Coding in Multiple Regression
Analysis: Strengths, Weaknesses, and Utility of Popular Coding
Structures.''} \emph{Journal of Data Science} 8 (1): 61--73.

\leavevmode\vadjust pre{\hypertarget{ref-galton1877}{}}%
Galton, Francis. 1877. {``Typical Laws of Heredity.''} In. Royal
Institution of Great Britain.

\leavevmode\vadjust pre{\hypertarget{ref-mancl2001covariance}{}}%
Mancl, Lloyd A, and Timothy A DeRouen. 2001. {``A Covariance Estimator
for GEE with Improved Small-Sample Properties.''} \emph{Biometrics} 57
(1): 126--34.

\leavevmode\vadjust pre{\hypertarget{ref-mcelreath2018}{}}%
McElreath, Richard. 2018. \emph{Statistical Rethinking: A Bayesian
Course with Examples in r and Stan}. Chapman; Hall/CRC.

\leavevmode\vadjust pre{\hypertarget{ref-montgomery2018}{}}%
Montgomery, Jacob M, Brendan Nyhan, and Michelle Torres. 2018. {``How
Conditioning on Posttreatment Variables Can Ruin Your Experiment and
What to Do about It.''} \emph{Am. J. Pol. Sci.} 62 (3): 760--75.

\leavevmode\vadjust pre{\hypertarget{ref-silberzahn2018}{}}%
Silberzahn, Raphael, Eric L Uhlmann, Daniel P Martin, Pasquale Anselmi,
Frederik Aust, Eli Awtrey, tpn Bahnk, et al. 2018. {``Many Analysts,
One Data Set: Making Transparent How Variations in Analytic Choices
Affect Results.''} \emph{Advances in Methods and Practices in
Psychological Science} 1 (3): 337--56.

\leavevmode\vadjust pre{\hypertarget{ref-steegen2016}{}}%
Steegen, Sara, Francis Tuerlinckx, Andrew Gelman, and Wolf Vanpaemel.
2016. {``Increasing Transparency Through a Multiverse Analysis.''}
\emph{Perspectives on Psychological Science} 11 (5): 702--12.
\url{https://doi.org/10.1177/1745691616658637}.

\leavevmode\vadjust pre{\hypertarget{ref-stiller2015}{}}%
Stiller, Alex J, Noah D Goodman, and Michael C Frank. 2015. {``Ad-Hoc
Implicature in Preschool Children.''} \emph{Language Learning and
Development} 11 (2): 176--90.

\end{CSLReferences}

\hypertarget{sec-measurement}{%
\chapter{Measurement}\label{sec-measurement}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Discuss the reliability and validity of psychological measures
\item
  Reason about tradeoffs between different measures and measure types
\item
  Identify the characteristics of well-constructed survey questions
\item
  Articulate risks of measurement flexibility and the costs and benefits
  of multiple measures
\end{itemize}

\end{tcolorbox}

Throughout the history of science, advances in measurement have gone
hand in hand with advances in knowledge.\sidenote{\footnotesize As such, measurement
  is a perennially controversial topic in philosophy of science. For an
  overview of competing frameworks, see Tal
  (\protect\hyperlink{ref-sep-measurement-science}{2020}) or Maul,
  Irribarra, and Wilson
  (\protect\hyperlink{ref-maul2016philosophical}{2016}), which focuses
  specifically on measurement in psychology.} Telescopes revolutionized
astronomy, microscopes revolutionized biology, and patch clamping
revolutionized physiology. But measurement isn't easy. Even the humble
thermometer, allowing reliable measurement of temperature, required
centuries of painstaking effort to perfect
(\protect\hyperlink{ref-chang2004inventing}{Chang 2004}). Psychology and
the behavioral sciences are no different -- we need reliable instruments
to measure the things we care about. In this next section of the book,
we're going to discuss the challenges of measurement in psychology, and
the properties that distinguish good instruments from bad.

What does it mean to measure something? Intuitively, we know that a
ruler measures the quantity of length, and a scale measures the quantity
of weight (\protect\hyperlink{ref-kisch1965scales}{Kisch 1965}). But
what does it mean to measure a psychological construct -- a hypothesized
theoretical quantity inside the head?

First, not every measure is equally precise. This point is obvious when
you think about physical measurement instruments: a caliper will give
you a much more precise estimate of the thickness of a small object than
a ruler will. One way to see that the measurement is more precise is by
repeating it a bunch of times. The measurements from the caliper will
likely be more similar to one another, reflecting the fact that the
amount of error in each individual measurement is smaller. We can do the
same thing with a psychological measurement -- repeat and assess
variation -- though as we'll see below it's a little trickier.
Measurement instruments that have less error are called more
\textbf{reliable} instruments.\sidenote{\footnotesize Is reliability the same as
  \textbf{precision}? Yes, more or less. Confusingly, different fields
  call these concepts different things (there's a helpful table of these
  names in \protect\hyperlink{ref-brandmaier2018}{Brandmaier et al.
  2018}). Here we'll talk about reliability as a property of instruments
  specifically while using the term precision to talk about the
  measurements themselves.}

Second, as we discussed in \textbf{?@sec-theories}, psychological
measurements do not directly reflect the theoretical constructs of
interest, quantities like happiness, intelligence, or language
processing ability. We have to measure an observable behavior -- our
operationalization of the construct -- and then make an argument about
how the measure relates to the construct of interest. This is an
argument for the \textbf{validity} of measurements from the
instrument.\sidenote{\footnotesize We are also going to talk in \textbf{?@sec-design}
  about the validity of manipulations. The way you identify a causal
  effect on some measure is by operationalizing some construct as well.
  To identify causal effects, we must link a particular construct of
  interest to something we can concretely manipulate in an experiment,
  like the stimuli or instructions.}

These two concepts, reliability and validity, provide a conceptual
toolkit for assessing a psychological measurement and how well it serves
the researcher's goal.

\begin{tcolorbox}[colframe=.blue, title=\faMicroscope \enspace Case study]

\hypertarget{a-reliable-and-valid-measure-of-childrens-vocabulary}{%
\section*{A reliable and valid measure of children's
vocabulary}\label{a-reliable-and-valid-measure-of-childrens-vocabulary}}
\addcontentsline{toc}{section}{A reliable and valid measure of
children's vocabulary}

\markright{A reliable and valid measure of children's vocabulary}

Anyone who has worked with little children, or had children of their
own, can attest to how variable their early language is. Some children
speak clearly and produce long sentences from an early age, while others
struggle; this variation appears to be linked to later school outcomes
(\protect\hyperlink{ref-marchman2008}{Marchman and Fernald 2008}). Thus,
there are many reasons why you'd want to make precise measurements of
children's early language ability as a latent construct of interest.

Because bringing children into a lab can be expensive, one popular
alternative option for measuring child language is the MacArthur Bates
Communicative Development Inventory (CDI for short), a form which asks
parents to mark words that their child says or understands. CDI forms
are basically long checklists of words. But is parent report a reliable
or valid measure of children's early language?

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{images/measurement/psycho-cors2.png}

}

\caption{\label{fig-measurement-psycho-cors}Longitudinal correlations
between a child's score on one administration of the CDI and another one
several months later. From Frank et al.
(\protect\hyperlink{ref-frank2021}{2021}).}

\end{figure}

As we'll see below, one way to measure the reliability of the CDI to
compute the correlation between two different administrations of the
form for the same child. Unfortunately, this analysis has one issue: the
longer you wait between observations the more the child has changed!
Figure~\ref{fig-measurement-psycho-cors} displays these correlations for
two CDIs, showing how correlations start off high and drop off as the
gap between observations increases
(\protect\hyperlink{ref-frank2021}{Frank et al. 2021}).

Given that CDI forms are relatively reliable instruments, are they
valid? That is, do they really measure the construct of interest, namely
children's early language ability? Bornstein and Haynes
(\protect\hyperlink{ref-bornstein1998}{1998}) collected many different
measures of children's language -- including the ELI (an early CDI form)
and other ``gold standard'' measures like transcribed samples of
children's speech. CDI scores were highly correlated with all the
different measures, suggesting that the CDI was a valid measure of the
construct.

The combination of reliability and validity evidence suggests that CDIs
are a useful (and relatively inexpensive source) of data about
children's early language, and indeed they have become one of the most
common assessments for this age group!

\end{tcolorbox}

\hypertarget{reliability}{%
\section{Reliability}\label{reliability}}

\begin{marginfigure}

{\centering \includegraphics{images/measurement/reliability-validity.png}

}

\caption{\label{fig-measurement-brandmaier}Reliability and validity
visualized. The reliability of an instrument is its expected precision.
The bias of measurements from an instrument also provide a metaphor for
its validity.}

\end{marginfigure}

Reliability is a way of describing the extent to which a measure yields
signal relative to noise. Intuitively, if there's less noise, then there
will be more similarity between different measurements of the same
quantity, illustrated in Figure~\ref{fig-measurement-brandmaier} as a
tighter grouping of points on the bulls-eye. But how do we measure
signal and noise?

\hypertarget{measurement-scales}{%
\subsection{Measurement scales}\label{measurement-scales}}

\begin{marginfigure}

{\centering \includegraphics{images/measurement/cv.png}

}

\caption{\label{fig-measurement-cv}Computing the coefficient of
variation (CV).}

\end{marginfigure}

In the physical sciences, it's common to measure the precision of an
instrument by quantifying its coefficient of variation
(\protect\hyperlink{ref-brandmaier2018}{Brandmaier et al. 2018}):

\[CV = \frac{\sigma_w}{\mu_w}\] \noindent where \(\sigma_w\) is the
standard deviation of the measurements within an individual and
\(\mu_w\) is the mean of those measurements
(Figure~\ref{fig-measurement-cv}).

Imagine we measure the height of a person five times, resulting in
measurements of 171cm, 172cm, 171cm, 173cm, and 172cm. These are the
combination of the person's true height (we assume they have one!) and
some \textbf{measurement error}. Now we can use these measurements to
compute the coefficient of variation, which is 0.005, suggesting very
limited variability relative to the overall quantity being measured. Why
can't we just do this same thing with psychological measurements?

\footnotesize

\hypertarget{tbl-stevens}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.1190}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3214}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.3095}}@{}}
\caption{\label{tbl-stevens}Stevens
(\protect\hyperlink{ref-stevens1946}{1946}) table of scale types and
their associated operations and statistics.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scale
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Operations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Statistics
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Scale
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Definition
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Operations
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Statistics
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Nominal & Unordered list & Equality & Mode \\
Ordinal & Ordered list & Greater than or less than & Median \\
Interval & Numerical & Equality of intervals & Mean, SD \\
Ratio & Numerical with zero & Equality of ratios & Coefficient of
variation \\
\end{longtable}

\normalsize

Thinking about this question takes us on a detour through the different
kinds of measurement scales used in psychological research
(\protect\hyperlink{ref-stevens1946}{Stevens 1946}). The height
measurements in our example are on what is known as a \textbf{ratio}
scale: a scale in which numerical measurements are equally spaced and on
which there is a true zero point. These scales are common for physical
quantities but somewhat less frequent in psychology (with reaction times
as a notable exception). More common are \textbf{interval} scales, in
which there is no true zero point. For example, IQ (and other
standardized scores) are intended to capture interval variation on some
dimension but 0 is meaningless -- an IQ of 0 does not correspond to any
particular interpretation.\sidenote{\footnotesize It can actually be shown in a
  suitably rigorous sense that ratio and interval scales (and another
  lying in between) are the \emph{only} scales possible for the real
  numbers (\protect\hyperlink{ref-narens1986measurement}{Narens and Luce
  1986}).}

\textbf{Ordinal} scales are also commonly used. These are scales that
are ordered but are not necessarily spaced equally. For example, levels
of educational achievement (``Elementary'', ``High school'', ``Some
college'', ``College'', ``Graduate school'') are ordered, but there is
no sense in which ``High school'' is as far from ``Elementary'' as
``Graduate school'' is from ``College.'' The last type in Stevens'
hierarchy is \textbf{nominal} scales, in which no ordering is possible
either. For example, race is an unordered scale in which multiple
categories are present but there is no inherent ordering of these
categories. The full hierarchy is presented in Table~\ref{tbl-stevens}.

Critically, different summary measures work for each scale type. If you
have an unordered list like a list of options for a question about race
on a survey, you can present the modal response (the most likely one).
It doesn't even make sense to think about what the median was -- there's
no ordering! For ordered levels of education, a median is possible but
you can't compute a mean. And for interval variables like ``number of
correct answers on a math test'' you can compute a mean and a standard
deviation.\sidenote{\footnotesize You might be tempted to think that ``number of
  correct answers'' is a ratio variable -- but is zero really
  meaningful? Does it truly correspond to ``no math knowledge'' or is it
  just a stand-in for ``less math knowledge than this test requires''?}

Now we're ready to answer our initial question about why we can't
quantify reliability using the coefficient of variation. Unless you have
a ratio scale with a true zero, you can't compute a coefficient of
variation. Think about it for IQ scores: currently, by convention,
standardized IQ scores are set to have a mean of 100. If we tested
someone multiple times and found the standard deviation of their test
scores was 4 points, then we could estimate the precision of their
measurements as ``CV'' of 4/100 = .04. But since IQ of 0 isn't
meaningful, we could just set the mean IQ for the population to 200. Our
test would be the same, and so the CV would be 4/200 = .02. On that
logic we just doubled the precision of our measurements by rescaling the
test! That doesn't make any sense.

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{early-controversies-over-psychological-measurement}{%
\section*{Early controversies over psychological
measurement}\label{early-controversies-over-psychological-measurement}}
\addcontentsline{toc}{section}{Early controversies over psychological
measurement}

\markright{Early controversies over psychological measurement}

\begin{quote}
``Psychology cannot attain the certainty and exactness of the physical
sciences, unless it rests on a foundation of {[}\ldots{]} measurement''
(\protect\hyperlink{ref-cattel1890mental}{Cattel 1890}).
\end{quote}

It is no coincidence that the founders of experimental psychology were
obsessed with measurement
(\protect\hyperlink{ref-heidelberger2004nature}{Heidelberger 2004}). It
was viewed as the primary obstacle facing psychology on its road to
becoming a legitimate quantitative science. For example, one of the
final pieces written by Hermann von Helmholtz (Wilhelm Wundt's doctoral
advisor), was a 1887 philosophical treatise entitled ``Zahlen und
Messen'' (``Counting and Measuring''; see Darrigol
(\protect\hyperlink{ref-darrigol2003number}{2003})). In the same year,
Fechner (\protect\hyperlink{ref-fechner1987my}{1987}) explicitly
grappled with the foundations of measurement in ``Uber die psychischen
Massprincipien'' (``On Psychic Measurement Principles'').

Many of the early debates over measurement revolved around the emerging
area of \emph{psychophysics}, the problem of relating objective,
physical stimuli (e.g.~light or sound or pressure) to the subjective
sensations they produce in the mind. For example, Fechner
(\protect\hyperlink{ref-fechner1860elemente}{1860}) was interested in a
quantity called the ``just noticeable difference'', the smallest change
in a stimulus that can be discriminated by our senses. He argued for a
lawful (logarithmic) relationship: a logarithmic change in the intensity
of, say, brightness corresponded to a linear change in the intensity
people reported (up to some constant). In other words, sensation was
\emph{measurable} via instruments like just noticeable difference.

It may be surprising to modern ears that the basic claim of
measurability was controversial, even if the precise form of the
psychophysical function would continue to be debated. But this claim led
to a deeply rancorous debate, culminating with the so-called Ferguson
Committee, formed by the British Association for the Advancement of
Science in 1932 to investigate whether such psychophysical procedures
could count as quantitative `measurements' of anything at all
(\protect\hyperlink{ref-moscati2018measuring}{Moscati 2018}). It was
unable to reach a conclusion, with physicists and psychologists
deadlocked:

\begin{quote}
Having found that individual sensations have an order, they {[}some
psychologists{]} assume that they are \emph{measurable}. Having
travestied physical measurement in order to justify that assumption,
they assume that their sensation intensities will be related to stimuli
by numerical laws {[}\ldots{]} which, if they mean anything, are
certainly false. (\protect\hyperlink{ref-ferguson1940}{Ferguson and
Tucker 1940})
\end{quote}

The heart of the disagreement was rooted in the classical definition of
quantity requiring strictly \emph{additive} structure. An attribute was
only considered measurable in light of a meaningful concatenation
operation. For example, weight was a measurable attribute because
putting a bag of three rocks on a scale yields the same number as
putting each of the three rocks on separate scales and then summing up
those numbers (in philosophy of science, attributes with this
concatenation property are known as ``extensive'' attributes, as opposed
to ``intensive'' ones.) Norman Campbell, one of the most prominent
members of the Ferguson Committee, had recently defined
\emph{fundamental} measurement in this way (e.g.~see
\protect\hyperlink{ref-campbell1928account}{Campbell 1928}), contrasting
it with \emph{derived measurement}, which involved computing some
function based on one or mroe fundamental measures. According to the
physicists on the Ferguson Committee, measuring mental sensations was
impossible because they could never be grounded in any
\emph{fundamental} scale with this kind of additive operation. It just
didn't make sense to break up people's sensations into parts the way we
would weights or lengths: they didn't come in ``amounts'' or
``quantities'' that could be combined
(\protect\hyperlink{ref-cattell1962relational}{Cattell 1962}). Even the
intuitive additive logic of Donders
(\protect\hyperlink{ref-donders1969}{1868/1969})'s ``method of
subtraction'' for measuring the speed of mental processes was viewed
skeptically on the same grounds by the time of the committee (e.g., in
an early textbook, Woodworth
(\protect\hyperlink{ref-woodworth1938}{1938}) claimed ``we cannot break
up the reaction into successive acts and obtain the time for each
act.'')

The primary target of the Ferguson Committee's investigation was the
psychologist S. S. Stevens, who had claimed to measure the sensation of
loudness using psychophysical instruments. Exiled from classical
frameworks of measurement, he went about developing an alternative
``operational'' framework (\protect\hyperlink{ref-stevens1946}{Stevens
1946}), where the classical ratio scale recognized by physicists was
only one of several ways of assigning numbers to things (see
@Table~\ref{tbl-stevens} below). Stevens' framework quickly spread,
leading to an explosion of proposed measures. However, operationalism
remains controversial outside psychology
(\protect\hyperlink{ref-michell1999measurement}{Michell 1999}). The most
extreme version of Steven's stance (``measurement is the assignment of
numerals to objects or events according to rule'') permits researchers
to \emph{define} constructs operationally in terms of a measure
(\protect\hyperlink{ref-hardcastle1995ss}{Hardcastle 1995}). For
example, one may say that the construct of intelligence is simply
\emph{whatever it is} that IQ measures. It is then left up to the
researcher to decide which scale type their proposed measure should
belong to.

In \textbf{?@sec-theories}, we outlined a somewhat different view,
closer to a kind of constructive realism
(\protect\hyperlink{ref-giere2004models}{Giere 2004};
\protect\hyperlink{ref-putnam1999threefold}{Putnam 2000}). Psychological
constructs like happiness are taken to exist independent of any given
operationalization, putting us on firmer ground to debate the pros and
cons associated with different ways of measuring the same construct. In
other words, we are not free to assign numbers however we like. Whether
a particular construct or quantity \emph{is measurable} on a particular
scale should be treated as an empirical question.

The next major breakthrough in measurement theory emerged with the birth
of mathematical psychology in the 1960s, which aimed to put
psychological measurement on more rigorous foundations. This effort
culminated in the three-volume Foundations of Measurement series
(\protect\hyperlink{ref-krantz2006additive}{Krantz et al. 1971};
\protect\hyperlink{ref-suppes2007foundations}{Suppes et al. 1989};
\protect\hyperlink{ref-luce2007foundations}{Robert Duncan Luce et al.
1990}), which has become the canonical text for every psychology student
seeking to understand measurement in the non-physical sciences. One of
the key breakthroughs was to shift the burden from measuring (additive)
constructs themselves to measuring (additive) \emph{effects} of
constructs in conjunction with one another:

\begin{quote}
When no natural concatenation operation exists, one should try to
discover a way to measure factors and responses such that the `effects'
of different factors are additive.
(\protect\hyperlink{ref-luce1964simultaneous}{R. Duncan Luce and Tukey
1964}).
\end{quote}

This modern viewpoint broadly informs the view we describe here.

\end{tcolorbox}

\hypertarget{measuring-reliability}{%
\subsection{Measuring reliability}\label{measuring-reliability}}

So then how do we measure signal and noise when we don't have a true
zero? We can still look at the variation between repeated measurement,
but rather than comparing that variation between measurements to the
mean, we can compare it to some other kind of variation, for example,
variation between people. In what follows, we'll discuss reliability on
interval scales, but many of the same tools have been developed for
ordinal and nominal scales.

Imagine that you are developing an instrument to measure some cognitive
ability. We assume that every participant has a true ability, \(t\),
just the same way that they have a true height in the example above.
Every time we measure this true ability with our instrument, however, it
gets messed up by some measurement error. Let's specify that error is
normally distributed with a mean of zero -- so it doesn't \textbf{bias}
the measurements, it just adds noise. The result is our observed score,
\(o\).\sidenote{\footnotesize The approach we use to introduce this set of ideas is
  called \textbf{classical test theory}. There are other -- more modern
  -- alternative approaches, but CTT (as it's called) is a good starting
  point for thinking through the concepts.}

Taking this approach, we could define a relative version of the
coefficient of variation. The idea is that the reliability of a
measurement is the amount of variance attributable to the true score
variance (signal), rather than the observed score variance (which
includes noise). If \(\sigma^2_t\) is the variance of the true scores
and \(\sigma^2_o\) is the variance of the observed scores, then this
ratio is

\[
R = \frac{\sigma^2_t}{\sigma^2_o}.
\] When noise is high, then the denominator is going to be big and \(R\)
will go down to 0; when noise is low, the numerator and the denominator
will be almost the same and \(R\) will approach 1.

This all sounds great, except for one problem: we can't compute
reliability using this formula without knowing true ability scores and
their variance. But if we knew those, we wouldn't need to measure
anything at all!

There are two main approaches to computing reliability from data. Each
of them makes an assumption that lets you circumvent the fundamental
issue, which is that you only have access to observed scores and not
true scores. Let's think these through in the context of a math test.

\begin{marginfigure}

{\centering \includegraphics{images/measurement/trt.png}

}

\caption{\label{fig-measurement-trt}Computing test-retest reliability.}

\end{marginfigure}

\textbf{Test-retest reliability}. Imagine you have two parallel versions
of your math test that are the same difficulty. Hence, you think a
student's score on either one will reflect the same true score, modulo
some noise. In that case, you can use these two sets of observed scores
(\(o_1\) and \(o_2\)) to compute the reliability of the instrument by
simply computing the correlation between them (\(\rho_{o_1, o_2}\)). The
logic is that, if both variants reflect the same true score, then the
shared variance (\textbf{covariance}) between them is just
\(\sigma^2_t\), the true score variance, which is the variable that we
wanted but didn't have. Test-retest reliability is thus a very
convenient way to measure reliability
(Figure~\ref{fig-measurement-trt}).

\textbf{Internal reliability}. If you don't have two parallel versions
of the test, or you can't give the test twice for whatever reason, then
you have another option. Assuming you have multiple questions on your
math test (which is a good idea!), then you can split the test in pieces
and treat the scores from each of these sub-parts as parallel versions.
The simplest way to do this is to split the instrument in half and
compute the correlation between participants' scores on the two halves
-- this quantity is called \textbf{split half reliability}.\sidenote{\footnotesize The
  problem is that each half is\ldots{} half as long as the original
  instrument. To get around this, there is a correction called the
  Spearman-Brown correction that can be applied to estimate the expected
  correlation for the full-length instrument. You also want to make sure
  that the test doesn't get harder from the beginning to the end. If it
  does, you may want to use the even-numbered and odd-numbered questions
  as the two parallel versions.}

Another method for computing the internal reliability (the
\textbf{consistency} of a test) is to treat each test item as a
sub-instrument and compute the average split-half correlation over all
splits. This method yields the statistic \textbf{Cronbach's} \(\alpha\)
(``alpha''). \(\alpha\) is a widely reported statistic, but it is also
widely misinterpreted (\protect\hyperlink{ref-sijtsma2009}{Sijtsma
2009}). First, it is actually a lower bound on reliability rather than a
good estimate of reliability itself. And second, it is often
misinterpreted as evidence that an instrument yields scores that are
``internally consistent,'' which it does not; it's not an accurate
summary of dimensionality. \(\alpha\) is a standard statistic, but it
should be used with caution. We don't recommend it.

One final note: these tools often get used for observers' ratings of the
same stimulus (\textbf{inter-rater} or \textbf{inter-annotator
reliability}), say for example when you have two coders rate how
aggressive a person seems in a video. The most common measure of
inter-annotator agreement is a categorical measure called
\textbf{Cohen's \(\kappa\)} (``kappa''), for categorical agreement, but
you can use ICCs for continuous data as well as many other measures.

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{reliability-paradoxes}{%
\section*{Reliability paradoxes!}\label{reliability-paradoxes}}
\addcontentsline{toc}{section}{Reliability paradoxes!}

\markright{Reliability paradoxes!}

There's a major issue with calculating reliabilities using the
approaches we described here: because reliability is defined as a ratio
of two measures of variation, it will always be relative to the
variation in the sample. So if a sample has less variability,
reliability will decrease!

Let's think about the CDI data in our case study, which showed high
test-retest reliability. Now imagine we restricted our sample to only 16
-- 18 month-olds (our prior sample had 16 -- 30-month-olds) with low
maternal education. Within this more restricted subset, overall
vocabularies would be lower and more similar to one another, and so the
average amount of change \emph{within} a child would be larger relative
to the differences \emph{between} children. That would make our
test-retest reliability score go \emph{down}, even though we would be
computing it on a subset of the exact same data.

That doesn't sound so bad. But we can construct a much more worrisome
version of the same problem. Say we are very sloppy in our
administration of the CDI and create lots of between-participants
variability, perhaps by giving different instructions to different
families. This practice will actually \emph{increase} our estimate of
split-half reliability. While the within-participant variability will
remain the same, the between-participant variability will go up! You
could call this a ``reliability paradox'' -- sloppier data collection
can actually lead to higher reliabilities.

More generally, when we think about reliability, we need to be sensitive
to the sources of variability we're quantifying reliability over -- both
the numerator and the denominator. If we're computing split-half
reliabilities, typically we're looking at variability across test
questions (from some question bank) vs.~across individuals (from some
population). Both of these sampling decisions affect reliability -- if
the population is more variable \emph{or} the questions are less
variable, we'll get higher reliability.

\end{tcolorbox}

\hypertarget{practical-advice-for-computing-reliability}{%
\subsection{Practical advice for computing
reliability}\label{practical-advice-for-computing-reliability}}

If you don't know the reliability of your measures for an experiment,
you risk wasting your and your participants' time. Ignorance is not
bliss. A higher reliability measure will lead to more precise
measurements of a causal effect of interest and hence smaller required
sample sizes.\sidenote{\footnotesize Low-reliability measures also limit your ability
  to detect correlations between measurements. One of us spent several
  fruitless months in graduate school running dozens of participants
  through batteries of language processing tasks and correlating the
  results across tasks. This exercise was a waste of time because most
  of the tasks were of such low reliability that, even had they been
  highly correlated with another task, this relationship would have been
  almost impossible to detect without a huge sample size. One rule of
  thumb that's helpful for individual difference designs of this sort is
  that the maximal correlation that can be observed between two
  variables \(x\) and \(y\) is the square root of the product of their
  reliabilities: \(\sqrt{r_x r_y}\) . So if you have two measures that
  are reliable at .25, the maximal measured correlation between them is
  .25 as well! This kind of method is now frequently used in cognitive
  neuroscience (and other fields as well) to compute the so-called
  \textbf{noise ceiling} for a measure: the maximum amount of signal
  that in principle \emph{could} be predicted
  (\protect\hyperlink{ref-lage-castellanos2019}{Lage-Castellanos et al.
  2019}).}

Test-retest reliability is generally the most conservative practical
measure of reliability. Test-retest estimates include not only
measurement error but also participants' state variation across
different testing sessions and variance due to differences between
versions of your instrument. These real-world quantities are absent from
internal reliability estimates, which may make you erroneously think
that there is more signal present in your instrument than there
is.\sidenote{\footnotesize Even though \(\alpha\) is a theoretical lower bound on
  reliability, in practice, test-retest accuracy often ends up lower
  than \(\alpha\) because it incorporates all these other sources of
  variation.} It's hard work to measure test-retest reliability
estimates, in part because you need two different versions of a test (to
avoid memory effects). If you plan on using an instrument more than once
or twice, though, it will likely be worthwhile!

Finally, if you have multiple measurement items as part of your
instrument, make sure you evaluate how they contribute to the
reliability of the instrument. Perhaps you have several questions in a
survey that you'd like to use to measure the same construct; perhaps
multiple experimental vignettes that vary in content or difficulty. Some
of these items may not contribute to your instrument's reliability --
and some may even detract. At a bare minimum, you should always
visualize the distribution of responses across items to scan for
\textbf{floor and ceiling effects} -- when items always yield responses
bunched at the bottom or top of the scale, limiting their usefulness --
and take a look at whether there are particular items on which items do
not relate to the others. If you are thinking about developing an
instrument that you use repeatedly, it may be useful to use more
sophisticated psychometric models to estimate the dimensionality of
responses on your instrument as well as the properties of the individual
items.\sidenote{\footnotesize If your items have binary answers, like test questions,
  then \textbf{item response theory} is a good place to start
  (\protect\hyperlink{ref-embretson2013}{Embretson and Reise 2013}). If
  your items are more like ratings on a continuous (interval or ratio)
  scale, then you may want to look at factor analysis and related
  methods (\protect\hyperlink{ref-furr2021}{Furr 2021}).}

\hypertarget{validity}{%
\section{Validity}\label{validity}}

In \textbf{?@sec-theories}, we talked about the process of theory
building as a process of describing the relationships between
constructs. But for the theory to be tested, the constructs must be
measured so that you can test the relationships between them!
Measurement and measure construction is therefore intimately related to
theory construction, and the notion of validity is central.\sidenote{\footnotesize Some
  authors have treated ``validity'' as a broader notion that can
  include, for example, statistical issues
  (\protect\hyperlink{ref-shadish2002}{Shadish, Cook, and Campbell
  2002}). The sense of validity that we are interested in here is a bit
  more specific. We focus on \textbf{construct validity}, the
  relationship between the measure and the construct.}

A valid instrument measures the construct of interest. In
Figure~\ref{fig-measurement-brandmaier}), invalidity is pictured as bias
-- the holes in the target are tightly grouped but in the wrong
place.\sidenote{\footnotesize This metaphor is a good rough guide but it doesn't
  distinguish an instrument that is systematically biased (for example,
  by estimating scores too low for one group) and one that is invalid
  (because it measures the wrong construct.} How can you tell if a
measure is valid, given that the construct of interest is unobserved?
There is no single test of the validity of a measure
(\protect\hyperlink{ref-cronbach1955}{Cronbach and Meehl 1955}). Rather,
the measure is valid if there is evidence that fits into the broader
theory as it relates to the specific construct it is supposed to be
measuring. For example, it should be strongly related to other measures
of the construct, but not as related to measures of different
constructs.

How do you establish that a measure fits into the broader theory?
Measurement validity is typically established via an argument that calls
on different sources of support (\protect\hyperlink{ref-kane1992}{Kane
1992}). Here are some of the ways that you might support the
relationship between a measure and a construct:

\begin{itemize}
\tightlist
\item
  \textbf{Face validity}: The measure looks like the construct, such
  that intuitively it is reasonable that it measures the construct. Face
  validity is a relatively weak source of evidence for validity, since
  it relies primarily on pre-theoretic intuitions rather than any
  quantitative assessment. For example, reaction time is typically
  correlated with intelligence test results (e.g.,
  \protect\hyperlink{ref-jensen1979}{Jensen and Munro 1979}), but does
  not appear to be a face-valid measure of intelligence in that simply
  being fast doesn't accord with our intuition about what it means to be
  intelligent!
\item
  \textbf{Ecological validity}: The measure relates to the context of
  people's lives. For example, a rating of a child's behavioral
  self-control in the classroom is a more ecologically valid measure of
  executive function than a reaction-time task administered in a lab
  context. Ecological validity arguments can be made on the basis of the
  experimental task, the stimuli, and the general setting of the
  experiment (\protect\hyperlink{ref-schmuckler2001}{Schmuckler 2001}).
  Researchers differ in how much weight they assign to ecological
  validity based on their goals and their theoretical orientation.
\item
  \textbf{Internal validity}: Usually used negatively. A ``challenge to
  internal validity'' is a description of a case where the measure is
  administered in such a way as to weaken the relationship between
  measure and construct. For example, if later items on a math test
  showed lower performance due to test-taker's fatigue rather than lower
  knowledge of the concepts, the test might have an internal validity
  issue.\sidenote{\footnotesize Often this concept is described as being relevant to
    the validity of a \emph{manipulation} also, e.g.~when the
    manipulation of the construct is confounded and some other
    psychological variable is manipulated as well. We discuss internal
    validity further in \textbf{?@sec-design}}
\item
  \textbf{Convergent validity}: The classic strategy for showing
  validity is to show that a measure relates (usually, correlates) with
  other putative measures of the same construct. When these
  relationships are measured concurrently, this is sometimes called
  \textbf{concurrent validity}. As we mentioned in
  \textbf{?@sec-theories}, self-reports of happiness relate to
  independent ratings by friends and family, suggesting that both
  measure the same underlying construct
  (\protect\hyperlink{ref-sandvik1993}{Sandvik, Diener, and Seidlitz
  1993}).\sidenote{\footnotesize This idea of convergent validity relates to the idea
    of holism we described in \textbf{?@sec-theories}. A measure is
    valid if it relates to other valid measures, which themselves are
    only valid if the first one is! The measures are valid because the
    theory works, and the theory works because the measures are valid.
    This circularity is a difficult but perhaps unavoidable part of
    constructing psychological theories (see the above Depth Box on the
    history of measurement). We don't ever have an objective starting
    point for the study of the human mind.}
\item
  \textbf{Predictive validity}. If the measure predicts other later
  measures of the construct, or related outcomes that might be of
  broader significance. Predictive validity is often used in lifespan
  and developmental studies where it is particularly prized for a
  measure to be able to predict meaningful life outcomes such as
  educational success in the future. For example, classroom self-control
  ratings (among other measures) appear strongly predictive of later
  life health and wealth outcomes
  (\protect\hyperlink{ref-moffitt2011}{Moffitt et al. 2011}).
\item
  \textbf{Divergent validity}. If the measure can be shown to be
  distinct from measure(s) of a different construct, this evidence can
  help establish that the measure is specifically linked to the target
  construct. For example, measures of happiness (specifically, life
  satisfaction) can be distinguished from measures of optimism as well
  as both positive and negative affect, suggesting that these are
  distinct constructs (\protect\hyperlink{ref-lucas1996}{Lucas, Diener,
  and Suh 1996}).
\end{itemize}

\hypertarget{validity-arguments-in-practice}{%
\subsection{Validity arguments in
practice}\label{validity-arguments-in-practice}}

Let's take a look at how we might make an argument about the validity of
the CDI, the vocabulary instrument that we used for our case study.

First, the CDI is face valid -- it is clearly about early language
ability. In contrast, even though a child's height would likely be
correlated with their early language ability, we should be skeptical of
this measure due to its lack of face validity. In addition, the CDI
shows good convergent and predictive validity. Concurrently, the CDI
correlates well with evidence from transcripts of children's actual
speech and from standardized language assessments (as discussed in the
case study above). And predictively, CDI scores at age 2 actually relate
to reading scores during elementary school
(\protect\hyperlink{ref-marchman2008}{Marchman and Fernald 2008}).

On the other hand, users of the CDI must avoid challenges to the
internal validity of the data they collect. For example, some CDI data
are compromised by confusing instructions or poor data collection
processes (\protect\hyperlink{ref-frank2021}{Frank et al. 2021}).
Further, advocates and critics of the CDI argue about its ecological
validity. There is something quite ecologically valid about asking
parents and caregivers -- who are experts on their own child -- to
report on their child's abilities. On the other hand, the actual
experience of filling out a structured form estimating language ability
might be more familiar to some families from high education backgrounds
than it would be for others from lower education backgrounds. Thus, a
critic could reasonably say that comparisons of CDI scores across
socioeconomic strata would be an invalid usage.

\hypertarget{avoid-questionable-measurement-practices}{%
\subsection{Avoid questionable measurement
practices!}\label{avoid-questionable-measurement-practices}}

Experimentalists sometimes have a tendency to make up ad hoc measures on
the fly. It's fine to invent new measures, but the next step is to think
about what evidence there is that it's valid!
Table~\ref{tbl-flake-questions} gives a set of questions to guide
thoughtful reporting of measurement practices (adapted from
\protect\hyperlink{ref-flake2020}{Flake and Fried 2020}).

\footnotesize
\renewcommand{\arraystretch}{2}

\hypertarget{tbl-flake-questions}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.4000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6000}}@{}}
\caption{\label{tbl-flake-questions}Questions about measurement that
every reseacher should answer in their paper. Adapted from Flake and
Fried (\protect\hyperlink{ref-flake2020}{2020}).}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Question
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Information to Report
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Question
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Information to Report
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
What is your construct? & Define construct, describe theory and
research. \\
What measure did you use to operationalize your construct? & Describe
measure and justify operationalization. \\
Did you select your measure from the literature or create it from
scratch? & Justify measure selection and review evidence on reliability
and validity (or disclose the lack of such evidence). \\
Did you modify your measure during the process? & Describe and justify
any modifications; note whether they occurred before or after data
collection. \\
How did you quantify your measure? & Describe decisions underlying the
calculation of scores on the measure; note whether these were
established before or after data collection and whether they are based
on standards from previous literature. \\
\end{longtable}

\renewcommand{\arraystretch}{1}
\normalsize

One big issue to be careful about is that researchers have been known to
modify their scales and their scale scoring practices (say, omitting
items from a survey or rescaling responses) after data collection. This
kind of post-hoc alteration of the measurement instrument can sometimes
be justified by features of the data, but it can also look a lot like
\(p\)-hacking! If researchers modify their measurement strategy after
seeing their data, this decision needs to be disclosed, and it may
undermine their statistical inferences.

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{talk-about-flexible-measurement}{%
\section*{Talk about flexible
measurement!}\label{talk-about-flexible-measurement}}
\addcontentsline{toc}{section}{Talk about flexible measurement!}

\markright{Talk about flexible measurement!}

The Competitive Reaction Time Task (CRTT) is a lab-based measure of
aggression. Participants are told that they are playing a reaction-time
game against another player and are asked to set the parameters of a
noise blast that will be played to their opponent. Unfortunately, in an
analysis of the literature using CRTT, Elson et al.
(\protect\hyperlink{ref-elson2014}{2014}) found that different papers
using the CRTT use dramatically different methods for scoring the task.
Sometimes the analysis focused on the volume of the noise blast and
sometimes it focused on the duration. Sometimes these scores were
transformed (via logarithms) or thresholded. Sometimes they were
combined into a single score. Elson was so worried by this flexibility,
he created a website, \url{https://flexiblemeasures.com}, to document
the variation he observed.

As of 2016, Elson had found 130 papers using the CRTT. And across these
papers, he documented an astonishing 157 quantification strategies. One
paper reported ten different strategies for extracting numbers from this
measure! More worrisome still, Elson and colleagues found that when they
tried out some of these strategies on their own data, different
strategies led to very different effect sizes and levels of statistical
significance. They could effectively make a finding appear bigger or
smaller depending on which scoring they chose.

Triangulating a construct through multiple pre-specified measurements
can be a good thing. But the issue with the CRTT analysis was that
changes in the measurement strategy appeared to be made in a \emph{post
hoc}, data-driven way so as to maximize the significance of the
experimental manipulation (just like the \emph{p}-hacking we discussed
in Chapters \textbf{?@sec-replication} and \textbf{?@sec-inference}).

This examination of the use of the CRTT measure has several
implications. First, and most troublingly, there may have been
undisclosed flexibility in the analysis of CRTT data across the
literature, with investigators taking advantage of the lack of
standardization to try many different analysis variants and report the
one most favorable to their own hypothesis. Second, it is unknown which
quantification of CRTT behavior is in fact most reliable and valid.
Since some of these variants are presumably better than others,
researchers are effectively ``leaving money on the table'' by using
suboptimal quantifications. As a consequence, if researchers adopt the
CRTT, they find much less guidance from the literature on what
quantification to adopt.

\end{tcolorbox}

\hypertarget{how-to-select-a-good-measure}{%
\section{How to select a good
measure?}\label{how-to-select-a-good-measure}}

Ideally you want a measure that is reliable and valid. How do you get
one? An important first principle is to use a pre-existing measure.
Perhaps someone else has done the hard work of compiling evidence on
reliability and validity, and in that case you will most likely want to
piggyback on that work. Standardized measures are typically broad in
their application and so the tendency can be to discard these because
they are not tailored for our studies specifically. But the benefits of
a standardized measure are substantial. Not only can you justify the
measure using the prior literature, you also have an important index of
population variability by comparing absolute scores to other
reports.\sidenote{\footnotesize Comparing absolute measurements is a really important
  trick for ``sanity-checking'' your data. If your measurements are very
  different than the ones in the paper you're following up (for example,
  if reaction times are much longer or shorter, or if accuracies on a
  test are much higher or lower), that may be a signal that something
  has gone wrong.}

If you don't use someone else's measure, you'll need to make one up
yourself. Most experimenters go down this route at some point, but if
you do, remember that you will need to figure out how to estimate its
reliability and also how to make an argument for its validity!

We can assign numbers to almost anything people do. We could run an
experiment on children's exploratory play and count the number of times
they interact with another child (\protect\hyperlink{ref-ross1989}{Ross
and Lollis 1989}), or run an experiment on aggression where we quantify
the amount of hot sauce participants serve
(\protect\hyperlink{ref-lieberman1999}{Lieberman et al. 1999}). Yet most
of the time we choose from a relatively small set of operational
variables: asking survey questions, collecting choices and reaction
times, and measuring physiological variables like eye-movements. Besides
following these conventions, how do we choose the right measurement type
for a particular experiment?

There's no hard and fast rule about what aspect of behavior to measure,
but here we will focus on two dimensions that can help us organize the
broad space of possible measure targets.\sidenote{\footnotesize Some authors
  differentiate between ``self-report'' and ``observational'' measures.
  This distinction seems simple on its face, but actually gets kind of
  complicated. Is a facial expression a ``self-report''? Language is not
  the only way that people communicate with one another -- many actions
  are intended to be communicative
  (\protect\hyperlink{ref-shafto2012}{Shafto, Goodman, and Frank 2012}).}
The first of these is the continuum between simple and complex
behaviors. The second is the focus on explicit, voluntary behaviors
vs.~implicit or involuntary behaviors.

\hypertarget{simple-vs.-complex-behaviors}{%
\subsection{Simple vs.~complex
behaviors}\label{simple-vs.-complex-behaviors}}

\begin{figure}

\sidecaption{\label{fig-measurement-considerations}Often choosing a
measure can be consolidated into a choice along a continuum from simple
measures that provide a small amount of information but are quick and
easy to repeat and those that provide much richer information but
require more time.}

{\centering \includegraphics{images/measurement/measure-considerations.png}

}

\end{figure}

Figure~\ref{fig-measurement-considerations} shows a continuum between
simple and complex behaviors. The simplest measurable behaviors tend to
be button presses, for example:

\begin{itemize}
\tightlist
\item
  pressing a key to advance to the next word in a word-by-word
  self-paced reading study
\item
  selecting ``yes'' or ``no'' in a lexical decision task
\item
  making a forced choice between different alternatives to indicate
  which has been seen before
\end{itemize}

These specific measures -- and many more like them -- are the bread and
butter of many cognitive psychology studies. Because they are quick and
easy to explain, these tasks can be repeated over many trials. They can
also be executed with a wider variety of populations including with
young children and sometimes even with non-human animals with
appropriate adaptation. (A further benefit of these paradigms is that
they can yield useful reaction time data, which we discuss further
below).

In contrast, a huge range of complex behaviors have been studied by
psychologists, including:

\begin{itemize}
\tightlist
\item
  open-ended verbal interviews
\item
  written expression, e.g.~via handwriting or writing style
\item
  body movements, including gestures, walking, or dance
\item
  drawing or artifact building
\end{itemize}

There are many reasons to study these kinds of behaviors. First, the
behaviors themselves may be examples of tasks of interest (e.g., studies
of drawing that seek to understand the origins of artistic expression).
Or, the behavior may stand in for other even more complex behaviors of
interest, as in studies of typing that use this behavior as a proxy for
lexical knowledge (\protect\hyperlink{ref-rumelhart1982}{Rumelhart and
Norman 1982}).

Complex behaviors typically afford a huge variety of different
measurement strategies. So any experiment that uses a particular
measurement of a complex behavior will typically need to do significant
work up front to justify the choice of that measurement strategy --
e.g., how to quantify dances or gestures or typing errors -- and provide
some assurance about its reliability. Further, it is often much more
difficult to have a participant repeat a complex behavior many times
under the same conditions. Imagine asking someone to draw hundreds of
sketches as opposed to pressing a key hundreds of times! Thus, the
choice of a complex behavior is often a choice to forego a large number
of simple trials for a small number of more complex trials.

Complex behaviors can be especially useful to study either at the
beginning or the end of a set of experiments. At the beginning of a set
of experiments, they can provide inspiration about the richness of the
target behavior and insight into the many factors that influence it. And
at the end of a set of experiments, they can provide an ecologically
valid measure to complement a reliable but more artificial, lab-based
behavior.

The more complex the behavior, however, the more it will vary across
individuals and the more environmental and situational factors will
affect it. These can be important parts of the phenomenon, but they can
also be nuisances that are difficult to get under experimental control.
Simple measures are typically easier to use and hence easier to deploy
repeatedly in a set of experiments where you iterate your manipulation
to test a causal theory.

\hypertarget{implicit-vs.-explicit-behaviors}{%
\subsection{Implicit vs.~explicit
behaviors}\label{implicit-vs.-explicit-behaviors}}

A second important dimension of organization for measures is the
difference between implicit and explicit measures. An explicit measure
provides a measurement of a behavior that a participant has conscious
awareness of -- for example, the answer to a question. In contrast,
implicit measures provide measurements of psychological processes that
participants are unable to report (or occasionally, unwilling
to).\sidenote{\footnotesize Implicit/explicit is likely more of a continuum, but one
  cut-point is whether the participants' behavior is considered
  intentional: that is, participants \emph{intend} to press a key to
  register a decision, but they likely do not intend to react in 300 as
  opposed to 350 milliseconds due to having seen a prime.} Implicit
measures, especially reaction time, have long been argued to reflect
internal psychological processes
(\protect\hyperlink{ref-donders1969}{Donders 1868/1969}). They also have
been proposed as measures of qualities such as racial bias that
participants may have motivation not to disclose
(\protect\hyperlink{ref-greenwald1998}{Greenwald, McGhee, and Schwartz
1998}). There are also of course a host of physiological measurements
available. Some of these measure eye-movements, heart rate, or skin
conductance, which can be linked to aspects of cognitive process. Others
reflect underlying brain activity via the signals associated with MRI,
MEG, NIRS, and EEG measurements. These methods are outside the scope of
this book, though we note that the measurement concerns we discuss here
definitely apply (e.g., \protect\hyperlink{ref-zuo2019}{Zuo, Xu, and
Milham 2019}).

Many tasks produce both accuracy and reaction time data. Often these
trade off with one another in a classic \textbf{speed-accuracy
tradeoff}: the faster participants respond, the less accurate they are.
For example, to investigate racial bias in policing, Payne
(\protect\hyperlink{ref-payne2001}{2001}) showed US college students a
series of pictures of tools and guns, proceeded by a prime of either a
White face or a Black face. In a first study, participants were faster
to identify weapons when primed by a Black face but had similar
accuracies. A second study added a response deadline to speed up
judgments: this manipulation resulted in equal reaction times across
conditions but greater errors in weapon identification after Black prime
faces. These studies likely revealed the same phenomenon -- some sort of
bias to associate Black faces with weapons -- but the design of the task
moved participants along a speed accuracy tradeoff, yielding effects on
different measures.\sidenote{\footnotesize One way of describing the information
  processing underlying this tradeoff is given by drift diffusion
  models, which allow joint analysis of accuracy and reaction time
  (\protect\hyperlink{ref-voss2013}{Voss, Nagler, and Lerche 2013}).
  Used appropriately, drift diffusion models can provide a way to remove
  speed-accuracy tradeoffs and extract more reliable signals from tasks
  where accuracy and reaction time are both measured (see Johnson et al.
  (\protect\hyperlink{ref-johnson2017}{2017}) for an example of DDM on a
  weapon-decision task).}

Simple, explicit behaviors are often a good starting point. Work using
these measures -- often the least ecologically valid -- can then be
enriched with implicit measures or measurements of more complex
behaviors.

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{survey-measures}{%
\section*{Survey measures}\label{survey-measures}}
\addcontentsline{toc}{section}{Survey measures}

\markright{Survey measures}

Sometimes the easiest way to elicit information from participants is
simply to ask. Survey questions are an important part of experimental
measurement, so we'll share a few best practices, primarily derived from
Krosnick and Presser (\protect\hyperlink{ref-krosnick2010}{2010}).

Treat survey questions as a conversation. The easier your items are to
understand, the better. Don't repeat variations on the same question
unless you want different answers! Try to make the order reasonable, for
example by grouping together questions about the same topic and moving
from more general to more specific questions. The more you include
``tricky'' items the more you invite tricky answers to straightforward
questions. One specific kind of tricky questions are ``check'' questions
that

Open-ended survey questions can be quite rich and informative,
especially when an appropriate coding (classification) scheme is
developed in advance and responses are categorized into a relatively
small number of types. On the other hand, they present practical
obstacles because they require coding (often by multiple coders to
ensure reliability of the coding). Further, they tend to yield nominal
data, which are often less useful for quantitative theorizing.
Open-ended questions are a useful tool to add nuance and color to the
interpretation of an experiment.

\begin{figure}[H]

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{images/measurement/likert.png}

}

\caption{\label{fig-measurement-likert}Likert scales based on survey
best practices: a bipolar opinion scale with seven points and a unipolar
frequency scale with five points. Both have all points labeled.}

\end{figure}

Especially given their ubiquity in commercial survey research,
\textbf{Likert scales} -- scales with a fixed number of ordered,
numerical response options -- are a simple and conventional way of
gathering data on attitude and judgment questions
(Figure~\ref{fig-measurement-likert}). Bipolar scales are those in which
the endpoints represent opposites, for example the continuum between
``strongly dislike'' and ``strongly like.'' Unipolar scales have one
neutral endpoint, like the continuum between ``no pain'' and ``very
intense pain.'' Survey best practices suggest that reliability is
maximized when bipolar scales have seven points and unipolar scales have
five. Labeling every point on the scale with verbal labels is preferable
to labeling only the endpoints.

One important question is whether to treat data from Likert scales as
ordinal or interval. It's extremely common (and convenient) to make the
assumption that Likert ratings are interval, allowing the use of
standard statistical tools like means, standard deviations, linear
regression, and the like. The risk in this practice comes from the
possibility that scale items are not evenly spaced -- for example, on a
scale labeled ``never'',``seldom'',
``occasionally'',``often'',``always,'' the distance from ``often'' to
``always'' may be larger than the distance from ``seldom'' to
``occasionally.''

In practice, you can choose to use regression variants that are
appropriate, e.g.~ordinal logistic regression and its variants, or they
can attempt to assess and mitigate the risks of treating the data as
interval. If you choose the second option, it's definitely a good idea
to look carefully at the raw distributions for individual items (see
\textbf{?@sec-viz}) to see if their distribution appears approximately
normal.

Recently some researchers have begun to use ``visual analog scales'' (or
sliders) as a solution. We don't recommend these -- the distribution of
the resulting data is often anchored at the starting point or endpoints
(\protect\hyperlink{ref-matejka2016}{Matejka et al. 2016}), and a
meta-analysis shows that are quite a bit lower than Likert scales in
reliability (\protect\hyperlink{ref-krosnick2010}{Krosnick and Presser
2010}).

It rarely helps matters to add a ``don't know'' or ``other'' option to
survey questions. These are some of a variety of practices that
encourage \textbf{satisficing}, where survey takers give answers that
are good enough but don't reflect substantial thought about the
question. Another behavior that results from satisficing is
``straight-lining'' -- that is, picking the same option for every
question. In general, the best way to prevent straight-lining is to make
surveys relatively short, engaging, and well-compensated. The practice
of ``reverse coding'' to make the expected answers to some questions
more negative can block straight-lining, but at the cost of making items
more confusing. Some obvious formatting options can reduce
straight-lining as well, for example placing scales further apart or on
subsequent (web) pages.

In sum, survey questions can be a helpful tool for eliciting graded
judgments about explicit questions. The best way to execute them well is
to try and make them as clear and easy to answer as possible.

\end{tcolorbox}

\hypertarget{the-temptation-to-measure-lots-of-things}{%
\section{The temptation to measure lots of
things}\label{the-temptation-to-measure-lots-of-things}}

If one measure is good, shouldn't two be better? Many experimenters add
multiple measurements to their experiments, reasoning that more data is
better than less. But that's not always true!

Deciding whether to include multiple measures is an aesthetic and
practical issue as well as a scientific one. Throughout this book we
have been advocating for a viewpoint in which experiments should be as
simple as possible. For us, the best experiment is one that shows that a
simple and valid manipulation affects a single, reliable and valid
measure.\sidenote{\footnotesize In an entertaining article called ``things I have
  learned (so far)'', Cohen (\protect\hyperlink{ref-cohen1990}{1990})
  quips that he leans so far in the direction of large numbers of
  observations and small numbers of measures, that some students think
  his perfect study has 10,000 participants and no measures.} If you are
tempted to include more than one measure, see if we can talk you out of
it.\sidenote{\footnotesize As usual, we want to qualify that we are only talking about
  randomized experiments here! In observational studies, often the point
  is to measure the associations between multiple measures so you
  typically \emph{have} to include more than one. Additionally, some of
  the authors of this book have advocated for measuring multiple
  outcomes in longitudinal observational studies, which could reduce
  investigator bias, encourage reporting null effects, enable comparison
  of effect sizes, and improve research efficiency
  (\protect\hyperlink{ref-vanderweele2020outcome}{VanderWeele, Mathur,
  and Chen 2020}). We've also done plenty of descriptive studies --
  these can be very valuable. In a descriptive context, often the goal
  is to include as many measures as possible so as to have a holistic
  picture of the phenomenon of interest.}

First, make sure that including more measures doesn't compromise each
individual measure. This can happen via fatigue or carryover effects.
For example, if a brief attitude manipulation is followed by multiple
questionnaire measures, it is a good bet that there is likely to be
``fade-out'' of the effect over time, so it won't have the same effect
on the first questionnaire as the last one. Further, even if a
manipulation has a long duration effect on participants, survey fatigue
may lead to less meaningful responses to later questions
(\protect\hyperlink{ref-herzog1981}{Herzog and Bachman 1981}).

Second, consider whether you have a strong prediction for each measure,
or whether you are simply looking for more ways to see an effect of your
manipulation. As we've discussed in \textbf{?@sec-theories}, we think of
an experiment as a ``bet.'' The more measures you add, the more bets you
are making but the less value you are putting on each. In essence, you
are ``hedging your bets'' and so the success of any one bet is less
convincing.

Third, if you include multiple measures in your experiment, you need to
think about how you will interpret inconsistent results. Imagine you
have experimental participants engage in a brief written reflection that
is hypothesized to affect a construct (vs a control writing exercise,
say listing meals). If you include two measures of the construct of
interest and one shows a larger effect, what will you conclude? It may
be tempting to assume that the one that shows a larger effect is the
``better measure'' but the logic is circular -- it's only better if the
manipulation affected the construct of interest, which is what you were
testing in the first place! Including multiple measures because you're
uncertain which one is more related to the construct indulges in this
circular logic, since the experiment often can't resolve the situation.
A much better move in this case is to do a preliminary study of the
reliability and validity of the two measures so as to be able to select
one as the experiment's primary endpoint.\sidenote{\footnotesize One caveat to this
  argument is that it can sometimes be useful to examine the effects of
  a manipulation on different measures because the measures are
  important. For example, you might be interested in whether an
  educational intervention increased grades \emph{and} decreased dropout
  rates. Both outcome measures are important and so it is useful to
  include both in your study.}

Finally, if you do include multiple measures, selective reporting of
significant or hypothesis-aligned measures becomes a real risk. For this
reason, preregistration and transparent reporting of all measures
becomes even more important.

There are some cases where more measures are better. The more expensive
the experiment, the less likely it is to be repeated to gather a new
measurement of the effects of the same manipulation. Thus, larger
studies present a stronger rationale for including multiple measures.
Clinical trials often involve interventions that can have effects on
many different measures; imagine a cancer treatment that might affect
mortality rates, quality of life, tumor growth rates, etc. Further, such
trials are extremely expensive and difficult to repeat. Thus, there is a
good reason for including more measures in such studies.

\hypertarget{chapter-summary-measurement}{%
\section{Chapter summary:
Measurement}\label{chapter-summary-measurement}}

In olden times, all the psychologists went to the same conferences and
worried about the same things. But then a split formed between different
groups. Educational psychologists and psychometricians thought a lot
about how different problems on tests had different measurement
properties. They began exploring how to select good and bad items, and
how to figure out people's ability abstracted away from specific items.
This research led to a profusion of interesting ideas about measurement
and modeling, but these ideas rarely percolated into day-to-day practice
in other areas of psychology. For example, cognitive psychologists
collected lots of trials and measured quantities of interest with high
precision, but worried less about measurement validity. Social
psychologists spent more time worrying about issues of ecological
validity in their experiments, but often used \emph{ad hoc} scales with
poor psychometric properties.

These sociological differences between fields has led to an unfortunate
divergence, where experimentalists often do not recognize the value of
the conceptual tools developed to aid measurement, and hence fail to
reason about the reliability and validity of their measures in ways that
can help them make better inferences. As we said in our discussion of
reliability, ignorance is not bliss. Much better to think these choices
through!

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Let's go back to our example on the relationship between money and
  happiness from \textbf{?@sec-experiments}. How many different kinds of
  measures of happiness can you come up with? Make a list with at least
  five.
\item
  Choose one of your measures of happiness and come up with a validation
  strategy for it, making reference to at least three different types of
  validity. What data collection would this validation effort require?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\item
  A classic textbook on psychometrics that introduces the concepts of
  reliability and validity in a simple and readable way: Furr, R. M.
  (2021). \emph{Psychometrics: an introduction}. SAGE publications.
\item
  A great primer on questionnaire design: Krosnick, J.A. (2018).
  Improving Question Design to Maximize Reliability and Validity. In:
  Vannette, D., Krosnick, J. (eds) The Palgrave Handbook of Survey
  Research. Palgrave Macmillan, Cham.
  \url{https://doi.org/10.1007/978-3-319-54395-6_13}.
\item
  Introduction to general issues in measurement and why they shouldn't
  be ignored: Flake, J. K., \& Fried, E. I. (2020). Measurement
  schmeasurement: Questionable measurement practices and how to avoid
  them. Advances in Methods and Practices in Psychological Science,
  3(4), 456-465. \url{https://doi.org/10.1177/2515245920952393}.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-11}{%
\section*{References}\label{bibliography-11}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-11}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-bornstein1998}{}}%
Bornstein, Marc H, and O Maurice Haynes. 1998. {``Vocabulary Competence
in Early Childhood: Measurement, Latent Construct, and Predictive
Validity.''} \emph{Child Development} 69 (3): 654--71.

\leavevmode\vadjust pre{\hypertarget{ref-brandmaier2018}{}}%
Brandmaier, Andreas M, Elisabeth Wenger, Nils C Bodammer, Simone Khn,
Naftali Raz, and Ulman Lindenberger. 2018. {``Assessing Reliability in
Neuroimaging Research Through Intra-Class Effect Decomposition
(ICED).''} \emph{Elife} 7: e35718.

\leavevmode\vadjust pre{\hypertarget{ref-campbell1928account}{}}%
Campbell, Norman Robert. 1928. \emph{An Account of the Principles of
Measurement and Calculation}. Longmans, Green; Company, Limited.

\leavevmode\vadjust pre{\hypertarget{ref-cattel1890mental}{}}%
Cattel, J McK. 1890. {``Mental Tests and Measurements.''} \emph{Mind}
15: 373--80.

\leavevmode\vadjust pre{\hypertarget{ref-cattell1962relational}{}}%
Cattell, Raymond B. 1962. {``The Relational Simplex Theory of Equal
Interval and Absolute Scaling.''} \emph{Acta Psychologica} 20: 139--58.

\leavevmode\vadjust pre{\hypertarget{ref-chang2004inventing}{}}%
Chang, Hasok. 2004. \emph{Inventing Temperature: Measurement and
Scientific Progress}. Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-cohen1990}{}}%
Cohen, Jacob. 1990. {``Things i Have Learned (so Far).''} \emph{American
Psychologist} 45: 1304--12.

\leavevmode\vadjust pre{\hypertarget{ref-cronbach1955}{}}%
Cronbach, L J, and P E Meehl. 1955. {``Construct Validity in
Psychological Tests.''} \emph{Psychol. Bull.} 52 (4): 281--302.

\leavevmode\vadjust pre{\hypertarget{ref-darrigol2003number}{}}%
Darrigol, Olivier. 2003. {``Number and Measure: Hermann von Helmholtz at
the Crossroads of Mathematics, Physics, and Psychology.''} \emph{Studies
in History and Philosophy of Science Part A} 34 (3): 515--73.

\leavevmode\vadjust pre{\hypertarget{ref-donders1969}{}}%
Donders, Franciscus Cornelis. 1868/1969. {``On the Speed of Mental
Processes.''} \emph{Acta Psychologica} 30 (1868/1969): 412--31.

\leavevmode\vadjust pre{\hypertarget{ref-elson2014}{}}%
Elson, Malte, M. Rohangis Mohseni, Johannes Breuer, Michael Scharkow,
and Thorsten Quandt. 2014. {``Press {CRTT} to Measure Aggressive
Behavior: The Unstandardized Use of the Competitive Reaction Time Task
in Aggression Research.''} \emph{Psychological Assessment} 26 (2):
419--32. \url{https://doi.org/10.1037/a0035569}.

\leavevmode\vadjust pre{\hypertarget{ref-embretson2013}{}}%
Embretson, Susan E, and Steven P Reise. 2013. \emph{Item Response
Theory}. Psychology Press.

\leavevmode\vadjust pre{\hypertarget{ref-fechner1860elemente}{}}%
Fechner, Gustav Theodor. 1860. \emph{Elemente Der Psychophysik}. Vol. 2.
Breitkopf u. H{}rtel.

\leavevmode\vadjust pre{\hypertarget{ref-fechner1987my}{}}%
---------. 1987. {``My Own Viewpoint on Mental Measurement (1887).''}
\emph{Psychological Research} 49 (4): 213--19.

\leavevmode\vadjust pre{\hypertarget{ref-ferguson1940}{}}%
Ferguson, Myers, A., and W. S. Tucker. 1940. {``Quantitative Estimates
of Sensory Events, Final Report.''} \emph{Report of the British
Association for the Advancement of Science}, 331--49.

\leavevmode\vadjust pre{\hypertarget{ref-flake2020}{}}%
Flake, Jessica Kay, and Eiko I Fried. 2020. {``Measurement
Schmeasurement: Questionable Measurement Practices and How to Avoid
Them.''} \emph{Advances in Methods and Practices in Psychological
Science} 3 (4): 456--65.

\leavevmode\vadjust pre{\hypertarget{ref-frank2021}{}}%
Frank, Michael C, Mika Braginsky, Daniel Yurovsky, and Virginia A
Marchman. 2021. \emph{Variability and Consistency in Early Language
Learning: The Wordbank Project}. MIT Press.

\leavevmode\vadjust pre{\hypertarget{ref-furr2021}{}}%
Furr, R Michael. 2021. \emph{Psychometrics: An Introduction}. SAGE
publications.

\leavevmode\vadjust pre{\hypertarget{ref-giere2004models}{}}%
Giere, Ronald N. 2004. {``How Models Are Used to Represent Reality.''}

\leavevmode\vadjust pre{\hypertarget{ref-greenwald1998}{}}%
Greenwald, Anthony G, Debbie E McGhee, and Jordan LK Schwartz. 1998.
{``Measuring Individual Differences in Implicit Cognition: The Implicit
Association Test.''} \emph{Journal of Personality and Social Psychology}
74 (6): 1464.

\leavevmode\vadjust pre{\hypertarget{ref-hardcastle1995ss}{}}%
Hardcastle, Gary L. 1995. {``SS Stevens and the Origins of
Operationism.''} \emph{Philosophy of Science}, 404--24.

\leavevmode\vadjust pre{\hypertarget{ref-heidelberger2004nature}{}}%
Heidelberger, Michael. 2004. \emph{Nature from Within: Gustav Theodor
Fechner and His Psychophysical Worldview}. University of Pittsburgh Pre.

\leavevmode\vadjust pre{\hypertarget{ref-herzog1981}{}}%
Herzog, A Regula, and Jerald G Bachman. 1981. {``Effects of
Questionnaire Length on Response Quality.''} \emph{Public Opinion
Quarterly} 45 (4): 549--59.

\leavevmode\vadjust pre{\hypertarget{ref-jensen1979}{}}%
Jensen, Arthur R, and Ella Munro. 1979. {``Reaction Time, Movement Time,
and Intelligence.''} \emph{Intelligence} 3 (2): 121--26.

\leavevmode\vadjust pre{\hypertarget{ref-johnson2017}{}}%
Johnson, David J, Christopher J Hopwood, Joseph Cesario, and Timothy J
Pleskac. 2017. {``Advancing Research on Cognitive Processes in Social
and Personality Psychology: A Hierarchical Drift Diffusion Model
Primer.''} \emph{Social Psychological and Personality Science} 8 (4):
413--23.

\leavevmode\vadjust pre{\hypertarget{ref-kane1992}{}}%
Kane, Michael T. 1992. {``An Argument-Based Approach to Validity.''}
\emph{Psychological Bulletin} 112 (3): 527.

\leavevmode\vadjust pre{\hypertarget{ref-kisch1965scales}{}}%
Kisch, B. 1965. \emph{Scales and Weights: A Historical Outline}. Yale
Studies in the History of Science and Medicine. Yale University Press.

\leavevmode\vadjust pre{\hypertarget{ref-krantz2006additive}{}}%
Krantz, David H, R Duncan Luce, Patrick Suppes, and Amos Tversky. 1971.
\emph{Foundations of Measurement i: Additive and Polynomial
Representations}. Courier Corporation.

\leavevmode\vadjust pre{\hypertarget{ref-krosnick2010}{}}%
Krosnick, Jon A, and Stanley Presser. 2010. {``Question and
Questionnaire Design.''} \emph{Handbook of Survey Research}, 263.

\leavevmode\vadjust pre{\hypertarget{ref-lage-castellanos2019}{}}%
Lage-Castellanos, Agustin, Giancarlo Valente, Elia Formisano, and
Federico De Martino. 2019. {``Methods for Computing the Maximum
Performance of Computational Models of fMRI Responses.''} \emph{PLoS
Computational Biology} 15 (3): e1006397.

\leavevmode\vadjust pre{\hypertarget{ref-lieberman1999}{}}%
Lieberman, Joel D, Sheldon Solomon, Jeff Greenberg, and Holly A
McGregor. 1999. {``A Hot New Way to Measure Aggression: Hot Sauce
Allocation.''} \emph{Aggressive Behavior: Official Journal of the
International Society for Research on Aggression} 25 (5): 331--48.

\leavevmode\vadjust pre{\hypertarget{ref-lucas1996}{}}%
Lucas, Richard E, Ed Diener, and Eunkook Suh. 1996. {``Discriminant
Validity of Well-Being Measures.''} \emph{Journal of Personality and
Social Psychology} 71 (3): 616.

\leavevmode\vadjust pre{\hypertarget{ref-luce1964simultaneous}{}}%
Luce, R Duncan, and John W Tukey. 1964. {``Simultaneous Conjoint
Measurement: A New Type of Fundamental Measurement.''} \emph{Journal of
Mathematical Psychology} 1 (1): 1--27.

\leavevmode\vadjust pre{\hypertarget{ref-luce2007foundations}{}}%
Luce, Robert Duncan, David H Krantz, Patrick Suppes, and Amos Tversky.
1990. \emph{Foundations of Measurement III: Representation,
Axiomatization, and Invariance}. Courier Corporation.

\leavevmode\vadjust pre{\hypertarget{ref-marchman2008}{}}%
Marchman, Virginia A, and Anne Fernald. 2008. {``Speed of Word
Recognition and Vocabulary Knowledge in Infancy Predict Cognitive and
Language Outcomes in Later Childhood.''} \emph{Developmental Science} 11
(3): F9--16.

\leavevmode\vadjust pre{\hypertarget{ref-matejka2016}{}}%
Matejka, Justin, Michael Glueck, Tovi Grossman, and George Fitzmaurice.
2016. {``The Effect of Visual Appearance on the Performance of
Continuous Sliders and Visual Analogue Scales.''} In \emph{Proceedings
of the 2016 CHI Conference on Human Factors in Computing Systems},
5421--32.

\leavevmode\vadjust pre{\hypertarget{ref-maul2016philosophical}{}}%
Maul, Andrew, David Torres Irribarra, and Mark Wilson. 2016. {``On the
Philosophical Foundations of Psychological Measurement.''}
\emph{Measurement} 79: 311--20.

\leavevmode\vadjust pre{\hypertarget{ref-michell1999measurement}{}}%
Michell, Joel. 1999. \emph{Measurement in Psychology: A Critical History
of a Methodological Concept}. Vol. 53. Cambridge University Press.

\leavevmode\vadjust pre{\hypertarget{ref-moffitt2011}{}}%
Moffitt, Terrie E, Louise Arseneault, Daniel Belsky, Nigel Dickson,
Robert J Hancox, HonaLee Harrington, Renate Houts, et al. 2011. {``A
Gradient of Childhood Self-Control Predicts Health, Wealth, and Public
Safety.''} \emph{Proceedings of the National Academy of Sciences} 108
(7): 2693--98.

\leavevmode\vadjust pre{\hypertarget{ref-moscati2018measuring}{}}%
Moscati, Ivan. 2018. \emph{Measuring Utility: From the Marginal
Revolution to Behavioral Economics}. Oxford University Press.

\leavevmode\vadjust pre{\hypertarget{ref-narens1986measurement}{}}%
Narens, Louis, and R Duncan Luce. 1986. {``Measurement: The Theory of
Numerical Assignments.''} \emph{Psychological Bulletin} 99 (2): 166.

\leavevmode\vadjust pre{\hypertarget{ref-payne2001}{}}%
Payne, B Keith. 2001. {``Prejudice and Perception: The Role of Automatic
and Controlled Processes in Misperceiving a Weapon.''} \emph{Journal of
Personality and Social Psychology} 81 (2): 181.

\leavevmode\vadjust pre{\hypertarget{ref-putnam1999threefold}{}}%
Putnam, Hilary. 2000. \emph{The Threefold Cord: Mind, Body, and World}.
Columbia Univ. Press.

\leavevmode\vadjust pre{\hypertarget{ref-ross1989}{}}%
Ross, Hildy S, and Susan P Lollis. 1989. {``A Social Relations Analysis
of Toddler Peer Relationships.''} \emph{Child Development}, 1082--91.

\leavevmode\vadjust pre{\hypertarget{ref-rumelhart1982}{}}%
Rumelhart, David E, and Donald A Norman. 1982. {``Simulating a Skilled
Typist: A Study of Skilled Cognitive-Motor Performance.''}
\emph{Cognitive Science} 6 (1): 1--36.

\leavevmode\vadjust pre{\hypertarget{ref-sandvik1993}{}}%
Sandvik, Ed, Ed Diener, and Larry Seidlitz. 1993. {``Subjective
Well-Being: The Convergence and Stability of Self-Report and
Non-Self-Report Measures.''} \emph{Journal of Personality} 61 (3):
317--42.

\leavevmode\vadjust pre{\hypertarget{ref-schmuckler2001}{}}%
Schmuckler, Mark A. 2001. {``What Is Ecological Validity? A Dimensional
Analysis.''} \emph{Infancy} 2 (4): 419--36.

\leavevmode\vadjust pre{\hypertarget{ref-shadish2002}{}}%
Shadish, William, Thomas D Cook, and Donald Thomas and Campbell. 2002.
\emph{Experimental and Quasi-Experimental Designs for Generalized Causal
Inference}. Boston, MA: Houghton Mifflin.

\leavevmode\vadjust pre{\hypertarget{ref-shafto2012}{}}%
Shafto, Patrick, Noah D Goodman, and Michael C Frank. 2012. {``Learning
from Others: The Consequences of Psychological Reasoning for Human
Learning.''} \emph{Perspectives on Psychological Science} 7 (4):
341--51.

\leavevmode\vadjust pre{\hypertarget{ref-sijtsma2009}{}}%
Sijtsma, Klaas. 2009. {``On the Use, the Misuse, and the Very Limited
Usefulness of Cronbach's Alpha.''} \emph{Psychometrika} 74 (1): 107.

\leavevmode\vadjust pre{\hypertarget{ref-stevens1946}{}}%
Stevens, S S. 1946. {``On the Theory of Scales of Measurement.''}
\emph{Science} 103 (2684): 677--80.

\leavevmode\vadjust pre{\hypertarget{ref-suppes2007foundations}{}}%
Suppes, Patrick, David H Krantz, Robert Duncan Luce, and Amos Tversky.
1989. \emph{Foundations of Measurement II: Geometrical, Threshold, and
Probabilistic Representations}. Courier Corporation.

\leavevmode\vadjust pre{\hypertarget{ref-sep-measurement-science}{}}%
Tal, Eran. 2020. {``{Measurement in Science}.''} In \emph{The {Stanford}
Encyclopedia of Philosophy}, edited by Edward N. Zalta, {F}all 2020.
\url{https://plato.stanford.edu/archives/fall2020/entries/measurement-science/};
Metaphysics Research Lab, Stanford University.

\leavevmode\vadjust pre{\hypertarget{ref-vanderweele2020outcome}{}}%
VanderWeele, Tyler J, Maya B Mathur, and Ying Chen. 2020.
{``Outcome-Wide Longitudinal Designs for Causal Inference: A New
Template for Empirical Studies.''} \emph{Statistical Science} 35 (3):
437--66.

\leavevmode\vadjust pre{\hypertarget{ref-voss2013}{}}%
Voss, Andreas, Markus Nagler, and Veronika Lerche. 2013. {``Diffusion
Models in Experimental Psychology.''} \emph{Experimental Psychology} 60
(6): 385--402. \url{https://doi.org/10.1027/1618-3169/a000218}.

\leavevmode\vadjust pre{\hypertarget{ref-woodworth1938}{}}%
Woodworth, R. S. 1938. {``Experimental Psychology.''}

\leavevmode\vadjust pre{\hypertarget{ref-zuo2019}{}}%
Zuo, Xi-Nian, Ting Xu, and Michael Peter Milham. 2019. {``Harnessing
Reliability for Neuroscience Research.''} \emph{Nature Human Behaviour}
3 (8): 768--71. \url{https://doi.org/10.1038/s41562-019-0655-x}.

\end{CSLReferences}

\hypertarget{sec-design}{%
\chapter{Design}\label{sec-design}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Describe key elements of experimental design
\item
  Define randomization and counterbalancing strategies for removing
  confounds
\item
  Discuss strategies to design experiments that are appropriate to the
  populations of interest
\end{itemize}

\end{tcolorbox}

The key thesis of our book is that experiments should be designed to
yield precise and unbiased measurements of a causal effect. But the
causal effect of what? The manipulation! In an experiment we manipulate
(intervene on) some aspect of the world and measure the effects of that
manipulation. We then compare that measurement to a situation where the
intervention has not occurred.

We refer to different intervention states as \textbf{conditions} of the
experiment. The most common experimental design is the comparison
between a \textbf{control} condition, in which the intervention is not
performed, and an \textbf{experimental} (sometimes called
\textbf{treatment}) condition in which the intervention is performed.

But many other experimental designs are possible. In more complex
experiments, manipulations along different dimensions (sometimes called
\textbf{factors} in this context) can be combined. In the first part of
the chapter, we'll introduce some common experimental designs and the
vocabulary for describing them.

A good experimental measure must be a valid measure of the construct of
interest. The same is true for a manipulation -- it must validly relate
to the causal effect of interest. In the second part of the chapter,
we'll discuss issues of \textbf{manipulation validity}, including both
issues of ecological validity and \textbf{confounding}. We'll talk about
how practices like \textbf{randomization} and \textbf{counterbalancing}
can help remove nuisance confounds.\sidenote{\footnotesize This section will draw on
  our introduction to causal inference in \textbf{?@sec-experiments}, so
  if you haven't read that, now's the time.}

To preview our general take-home points from this chapter: we think that
your default experiment should manipulate one or two factors -- usually
not more -- and should manipulate those factors continuously and
within-participants. Although such designs are not always possible, they
are typically the most likely to yield precise estimates of a particular
effect that can be used to constrain future theorizing. We'll start by
considering a case study in which a subtle confound led to difficulties
interpreting an experimental result.

\begin{tcolorbox}[colframe=.blue, title=\faMicroscope \enspace Case study]

\hypertarget{automatic-theory-of-mind}{%
\section*{Automatic theory of mind?}\label{automatic-theory-of-mind}}
\addcontentsline{toc}{section}{Automatic theory of mind?}

\markright{Automatic theory of mind?}

In an early version of our course, student Desmond Ong set out to
replicate a thought-provoking finding: both infants and adults seemed to
show evidence of tracking other agents' belief state, even when it was
irrelevant to the task at hand
(\protect\hyperlink{ref-kovacs2010}{Kovcs, Tgls, and Endress 2010}).
In the paradigm, an animated Smurf character would watch as a
self-propelled ball came in and out from behind a screen. At the end of
the video, the screen would swing down and the participant had to
respond whether the ball was present or absent. Reaction time for this
decision was the key dependent variable.

The experimental design investigated two factors: whether the
participant believed the ball was present or absent (P+/P-) and whether
the animated agent \emph{would have believed} the ball was present or
absent (A+/A-) based on what it saw. The result was four conditions:
P+/A+, P+/A-, P-/A+, and P-/A-. (We could call this a \textbf{fully
crossed} design because each level of one factor was presented with each
level of the other).

\begin{figure}[H]

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{images/design/kovacs-original.png}

}

\caption{\label{fig-design-kovacs-original}Original data from Kovcs,
Tgls, and Endress (\protect\hyperlink{ref-kovacs2010}{2010}). Error
bars show 95\% confidence intervals.}

\end{figure}

Both the original experiments and the replication that Desmond ran
showed a significant effect of the agent's beliefs on participants'
reaction times, suggesting that what the -- totally irrelevant -- agent
thought about the ball was leading them to react more or less quickly to
the presence of the ball. Figure~\ref{fig-design-kovacs-original} shows
the original data (N=24). But although both studies showed an effect of
agent belief, the replication and several variations also showed a
crossover \textbf{interaction} of participant and agent belief. The
participants were slower when the agents \emph{and} the participants
believed that the ball was behind the screen
(Figure~\ref{fig-design-kovacs-replication}). That finding wasn't
consistent with the theory that tracking inconsistent beliefs slowed
down reaction times. If participants were tracking their own beliefs
about the ball \emph{and} the agent's, they should have been fastest in
the P+/A+ condition, not slower.

\begin{figure}[H]

{\centering \includegraphics{images/design/kovacs-replication.png}

}

\caption{\label{fig-design-kovacs-replication}Data from a series of
replications of Kovcs, Tgls, and Endress
(\protect\hyperlink{ref-kovacs2010}{2010}), including versions on the
web (Experiments 1a and 1b) and in lab (Experiment 1c), as well as
several variations on the format of responding (Experiments 2 and 3;
2AFC = two alternative forced choice) and an experiment where a large
wall kept the agent from seeing the ball at all (Experiment 4). ``Hits''
and ``CRs'' panels refer to different subsets of trials where
participants responded ``present'' when the ball was present and
``absent'' when the ball was absent. Error bars are 95\% confidence
intervals.}

\end{figure}

A collaborative team working on this paradigm identified a key issue
(\protect\hyperlink{ref-phillips2015}{Phillips et al. 2015}). There was
a \textbf{confound} in the experimental design -- another factor that
varied across conditions besides the target factors. In other words,
something was changing between conditions other than the agent's and
participant's belief states. The confound was an attention check
(discussed further in \textbf{?@sec-collection}): participants had to
press a key when the agent left the scene to show that they were paying
attention. This attention check appeared a few seconds later in the
videos for the P+/A+ and P-/A- trials -- the ones that yielded the slow
reaction times -- than it did for the other two. When the attention
check was removed or when its timing was equalized across conditions,
reaction time effects were eliminated, suggesting that the original
pattern of findings may have been due to the confound.

If the standard for replication is significance of particular
statistical tests at \emph{p\textless.05}, then this experiment
replicated successfully. But the effect estimates were inconsistent with
the proposed theoretical explanation. A finding can be replicable
without providing support for the underlying theory!

There's an important caveat to this story. The followup work \emph{only}
revealed that there was a confound in one particular experimental
operationalization, and did not provide evidence against automatic
theory of mind in general. Indeed, others have suggested that different
versions of this paradigm \emph{do} reveal evidence for theory of mind
processing once the confound is eliminated
(\protect\hyperlink{ref-el-kaddouri2020}{El Kaddouri et al. 2020}).

\end{tcolorbox}

\hypertarget{experimental-designs}{%
\section{Experimental designs}\label{experimental-designs}}

Experimental designs are fundamental to many fields; unfortunately the
terminology used to describe them can vary, which can get quite
confusing! Here we will mostly describe an experiment as a relationship
between some manipulation(s), in which participants are randomly
assigned to experimental conditions to estimate effects on some measure.
Factors are the dimensions along which manipulations vary. For example,
in our case study above, the two factors were participant belief and
agent belief. One alternative terminology it's good to be familiar with
is the terms we used in Chapters \textbf{?@sec-estimation} -
\textbf{?@sec-models}, which are often used in econometrics and
statistics: the \textbf{treatment} (manipulation) and the
\textbf{outcome} (measure).\sidenote{\footnotesize Terminology here is hard. In
  psychology people sometimes say there's an \textbf{independent
  variable} (the manipulation, which is causally prior and hence
  ``independent'' of other causal influences) and a \textbf{dependent
  variable} (the measure, which causally depends on the manipulation, or
  so we hypothesize). We find this terminology to be hard to remember
  because the terms are so different from the actual concepts being
  described.}

In this section, we'll discuss some key dimensions on which experiments
vary: 1) how many factors they incorporate and how these factors are
crossed, 2) how many conditions and measures are given to each
participant, and 3) whether manipulations have discrete levels or fall
on a continuous scale.

\hypertarget{a-two-factor-experiment}{%
\subsection{A two-factor experiment}\label{a-two-factor-experiment}}

The classical ``design of experiments'' framework has as its goal to
separate observed variability in the dependent measure into 1)
variability due to the manipulation(s) and (2) other variability,
including measurement error and participant-level variation. This
framework maps nicely onto the statistical framework described in
Chapters \textbf{?@sec-estimation} -- \textbf{?@sec-models}. In essence,
this framework models the distribution of the measure using the
condition structure of our experiment as the predictor.

Different experimental designs will allow us to estimate condition
effects more and less effectively. Recall in \textbf{?@sec-estimation},
we estimated the effect of our manipulation by a simple subtraction:
\(\beta = \theta_{T} - \theta_{C}\) (where \(\beta\) is the effect
estimate, and \(\theta\)s indicate the estimates for each condition,
treatment \(T\) and control \(C\)). This logic works just fine also if
there are two distinct treatments in a three condition experiment: each
treatment can be compared to control separately. For treatment 1,
\(\beta_{T_1} = \theta_{T_2} - \theta_{C}\) and
\(\beta_{T_2} = \theta_{T_2} - \theta_{C}\). That logic is going to get
more complicated if we have more than one distinct factor of interest,
though. Let's look at a simple example.

\begin{marginfigure}

{\centering \includegraphics{images/design/young2007-design2.png}

}

\caption{\label{fig-design-young-design}The 2x2 crossed design used in
Young et al. (\protect\hyperlink{ref-young2007}{2007})}

\end{marginfigure}

Let's look at an example. Young et al.
(\protect\hyperlink{ref-young2007}{2007}) were interested in how moral
judgments depend on both the beliefs of actors and the outcomes of their
actions. They presented participants with vignettes in which they
learned, for example, that Grace visits a chemical factory with her
friend and goes to the coffee break room, where she sees a white powder
that she puts in her friend's coffee. They then manipulated both Grace's
\emph{beliefs} and the \emph{outcomes} of her action following the
schema in Figure~\ref{fig-design-young-design}. Participants (N=10) used
a four-point Likert scale to rate whether the actions were morally
forbidden (1) or permissible (4).

Young et al.'s design has two factors -- belief and outcome -- each with
two levels (neutral and negative, noted as \(B\) and \(-B\) for belief
and \(0\) and \(-O\) for outcome).\sidenote{\footnotesize Neither of these is
  necessarily a ``control'' condition: the goal is simply to compare
  these two levels of the factor -- negative and neutral -- to estimate
  the effect due to the factor.} These factors are \textbf{fully
crossed}: each level of each factor is combined with each level of each
other. That means that we can estimate a number of effects of interest.
The experimental data are shown in Figure~\ref{fig-design-young-data}.

\begin{figure}

\sidecaption{\label{fig-design-young-data}Moral permissability as a
function of belief and outcome. Results from Young et al.
(\protect\hyperlink{ref-young2007}{2007}), annotated with the estimated
effects. Simple effects measure differences between the individual
conditions and the neutral belief, neutral outcome condition. The
interaction measures the difference between the predicted sum of the two
simple effects and the actual observed data for the negative belief,
negative outcome condition.}

{\centering \includegraphics{images/design/young2007-data2.png}

}

\end{figure}

This fully-crossed design makes it easy for us to estimate quantities of
interest. Let's say that our \textbf{reference} group (equivalent to the
control group for now) is neutral belief, neutral outcome. Now it's easy
to use the same kind of subtraction we did before to estimate particular
effects we care about. For example, we can look at the effect of
negative belief in the case of a neutral outcome:
\(\beta_{-B,O} = \theta_{-B,O} - \theta_{B,O}\). The effect of a
negative outcome is computed similarly as
\(\beta_{B,-O} = \theta_{B,-O} - \theta_{B,O}\).

But now there is a complexity: these two \textbf{simple effects}
(effects of one variable at a particular level of another variable)
together suggest that the combined effect \(\beta_{-B,-O}\) should be
equal to the sum of \(\beta_{-B,O}\) and \(\beta_{B,-O}\).\sidenote{\footnotesize If
  you're interested, you can also compute the \textbf{average} or
  \textbf{main} effect of a particular factor via the same subtractive
  logic. For example, the average effect of negative belief (\(-B\))
  vs.~a neutral belief (\(B\)) can be computed as
  \(\beta_{-B} = \frac{(\theta_{-O, -B} + \theta_{O, -B}) - (\theta_{-O, B} + \theta_{O, B})}{2}\).}
As we can see from the graph, that's not right. If it were, the negative
belief, negative outcome condition would be below the minimum possible
rating! Instead, we observe an \textbf{interaction} effect (sometimes
called a \textbf{two-way interaction} when there are two factors): The
effect when both factors are present is different than the sum of the
two simple effects. To capture this effect, we need an interaction term
\(\beta_{-B,-O}\).\sidenote{\footnotesize If you're reading carefully, you might be
  thinking that this all sounds like we're talking about the analysis of
  variance (ANOVA), not about experimental design per se. These two
  topics are actually the same topic! The question is how to design an
  experiment so that these statistical models can be used to estimate
  particular effects -- and combinations of effects -- that we care
  about. In case you missed it, we discuss modeling interactions in a
  regression framework in \textbf{?@sec-models}.} In other words, the
effect of negative beliefs (intent) on subjective moral permissibility
depends on whether the action caused harm. Critically, without a
fully-crossed design, we can't estimate this interaction and we would
have made an incorrect prediction.

\hypertarget{generalized-factorial-designs}{%
\subsection{Generalized factorial
designs}\label{generalized-factorial-designs}}

Young et al.'s design, in which there are two factors with two levels
each, is called a \textbf{2x2 design} (pronounced ``two by two''). 2x2
designs are incredibly common and useful, but they are only one of an
infinite variety of such designs that can be constructed.

Say we added a third factor to Young et al.'s design such that Grace
either feels neutral towards her friend or is angry on that day. If we
fully crossed this third affective factor with the other two (belief and
outcome), we'd have a 2x2x2 design. This design would have eight
conditions: \((A, B, O)\), \((A, B, -O)\), \((A, -B, O)\),
\((A, -B, -O)\), \((-A, B, O)\), \((-A, B, -O)\), \((-A, -B, O)\),
\((-A, -B, -O)\). These conditions would in turn allow us to estimate
both two-way and three-way interactions, enumerated in
Table~\ref{tbl-three-way}.

\footnotesize

\hypertarget{tbl-three-way}{}
\begin{longtable}[]{@{}ll@{}}
\caption{\label{tbl-three-way}Possible effects in a hypothetical 2x2x2
experimental design with affect, belief, and outcome as
factors.}\tabularnewline
\toprule\noalign{}
Effect & Term Type \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Effect & Term Type \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Affect & Main effect \\
Belief & Main effect \\
Outcome & Main effect \\
Affect X Belief & 2-way interaction \\
Affect X Outcome & 2-way interaction \\
Belief X Outcome & 2-way interaction \\
Affect X Belief X Outcome & 3-way interaction \\
\end{longtable}

\normalsize

Three-way interactions are hard to think about! The affect X belief X
outcome interaction tells you about the difference in moral
permissibility that's due to all three factors being present as opposed
to what you'd predict on the basis of your estimates of the two-way
interactions. In addition to being hard to think about, higher order
interactions tend to be hard to estimate, because estimating them
accurately requires you to have a stable estimate of all of the
lower-order interactions
(\protect\hyperlink{ref-mcclelland1993}{McClelland and Judd 1993}). For
this reason, we recommend against experimental designs that rely on
higher-order interactions unless you are in a situation where you both
have strong predictions about these interactions and are confident in
your ability to estimate them appropriately.

Things can get even more complicated. If you have three factors with two
levels each, as in the example above (Table~\ref{tbl-three-way}), you
can estimate 7 total effects of interest. But if you have \emph{four}
factors with two levels each, you get 15. Four factors with \emph{three}
levels each gets you a horrifying 80 different effects!\sidenote{\footnotesize The
  general formula for \(N\) factors with \(M\) levels each is \(M^N-1\).}
This way lies madness, at least from the perspective of estimating and
interpreting individual effects in a reasonable sample size. Again, we
suggest starting with one- and two-factor designs. There is a lot to be
learned from simple designs that follow good measurement and sampling
practices.

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{estimation-strategies-for-generalized-factorial-designs}{%
\section*{Estimation strategies for generalized factorial
designs}\label{estimation-strategies-for-generalized-factorial-designs}}
\addcontentsline{toc}{section}{Estimation strategies for generalized
factorial designs}

\markright{Estimation strategies for generalized factorial designs}

So what should you do if you really do care about four or more factors
-- in the sense that you want to estimate their effects and include them
in your theory? The simplest strategy is to start your research off by
measuring them independently in a series of single-factor experiments.
This kind of setup is natural when there is a single reference level for
each factor of interest, and such experiments can provide a basis for
judging which factors are most important for your outcome and hence
which should be prioritized for experiments to estimate interactions.

On the other hand, sometimes there is no reference level for a factor.
For example, in the Kovcs, Tgls, and Endress
(\protect\hyperlink{ref-kovacs2010}{2010}) paradigm, it's not clear
whether a positive or negative belief is the reference level. That's not
a problem in a fully-crossed design like theirs, but this situation can
pose a problem if you have more than two such factors. Ideally you would
want to run independent experiments, but you have to choose some level
for all of the other variables -- you can't just assume that one level
is ``neutral.''

One solution that lets you compute main effects but not interactions is
called a \textbf{Latin square}. Latin squares are a good solution for
three-factor designs, which is the level at which a fully-crossed design
typically gets overwhelming. A Latin square is an \(n x n\) matrix in
which each number occurs exactly once in each row and column,
e.g.~\[\begin{bmatrix} 
    1 & 2 & 3 \\
    2 & 3 & 1\\
    3 & 1 & 2 \\
    \end{bmatrix}\]

This Latin square for \(n=3\) gives the solution for how to balance
factors across a 3x3x3 experiment. The row number is one factor, the
column number is the second factor, and the number in the cell is the
third factor. So one condition would be (1,1,1), the first level of all
factors, shown in the upper left cell. Another would be (3,3,2), the
lower right cell. Although a fully-crossed design would require 27 cells
to be run, the Latin square has only nine. Critically, the combinations
of factors are balanced across the nine cells so that the average effect
of each level of the three factors can be estimated.\sidenote{\footnotesize You can
  check and see that no interactions can be estimated, because no factor
  co-occurs with two different levels of another factor.}

There are also fancier methods available. For example, the literature on
\textbf{optimal experiment design} contains methods for choosing the
most informative sequence of experiments to run in order to estimate the
parameters in a model that can include many factors and their
interactions (\protect\hyperlink{ref-myung2009}{Myung and Pitt 2009}).
Going down this road typically means having an implemented computational
theory of your domain, but it can be a very productive strategy for
exploring a complex experimental space with many factors.

\end{tcolorbox}

\hypertarget{between--vs.-within-participant-designs}{%
\subsection{Between- vs.~within-participant
designs}\label{between--vs.-within-participant-designs}}

\begin{marginfigure}

{\centering \includegraphics{images/design/between2.png}

}

\caption{\label{fig-design-between}A between-participants design.}

\end{marginfigure}

Once you have a sense of the factor or factors you would like to
manipulate in your experiment, the next step is to consider how these
will be presented to participants, and how that presentation will
interact with your measurements. The biggest decision to be made is
whether each participant will experience only one level of a factor -- a
\textbf{between-participants} design -- or whether they will experience
multiple levels -- a \textbf{within-participants} design.
Figure~\ref{fig-design-between} shows a very simple example of
between-participants design with four participants (two assigned to each
condition), while Figure~\ref{fig-design-within} shows a
within-participants version of the same design.\sidenote{\footnotesize The
  within-participants design is counterbalanced for the order of the
  conditions; we cover the issue of counterbalancing below.}

\begin{figure}

\sidecaption{\label{fig-design-within}A within-participants design,
counterbalanced for order.}

{\centering \includegraphics{images/design/within2.png}

}

\end{figure}

Because people are very variable, the decision whether to measure a
particular factor between- or within-participants is consequential.
Imagine we're estimating our treatment effect as before, simply by
computing
\(\widehat{\beta} = \widehat{\theta}_{T} - \widehat{\theta}_{C}\) with
each of these estimates from different populations of participants. In
this scenario, our estimate \(\widehat{\beta}\) contains three
components: 1) the true differences between \(\theta_{T}\) and
\(\theta_{C}\), 2) sampling-related variation in which participants from
the population ended up in the samples for the two conditions, and 3)
measurement error. Component \#2 is present because any two samples of
participants from a population will differ in their average on a measure
-- this is precisely the kind of sampling variation we saw in the null
distributions in \textbf{?@sec-inference}.

When our experimental design is within-participants, component \#2 is
not present, because participants in both conditions are sampled from
the \emph{same} population. If we get unlucky and all of our
participants are lower than the population mean on our measure, then
that unluckiness affects our conditions equally. The consequences for
choosing an appropriate sample size are fairly extreme:
Between-participants designs typically require between two and eight
times as many participants as within-participants designs!\sidenote{\footnotesize If
  you want to estimate how big an advantage you get from
  within-participants data collection, you need to know how correlated
  (reliable) your observations are.
  \href{https://daniellakens.blogspot.com/2016/11/why-within-subject-designs-require-less.html}{One
  analysis of this issue} suggests that the key relationship is that
  \(N_{within} = N_{between} (1-\rho) /2\) where \(\rho\) is the
  correlation between the measurement of the two conditions within
  individuals. The more correlated they are, the smaller your
  within-participants \(N\).}

Given these advantages, why would you consider using a
between-participants design? A within-participants design is simply not
possible for all experiments. For example, consider a medical
intervention -- say, a new surgical procedure that is being compared to
an established one. Patients cannot receive two different procedures,
and so no within-participant comparison is possible.

Most manipulations in the behavioral sciences are not so extreme, but it
still may be impractical or inadvisable to deliver multiple conditions.
Greenwald (\protect\hyperlink{ref-greenwald1976}{1976}) distinguishes
three types of undesirable effects:\sidenote{\footnotesize We tend to think of all of
  these as being forms of carry-over effect, and sometimes use this as a
  catch-all description. Some people also use the picturesque
  description
  \href{https://statmodeling.stat.columbia.edu/2017/11/25/poisoning-well-within-person-design-whats-risk/}{``poisoning
  the well''} -- earlier conditions ``ruin'' the data for later
  conditions.}

\begin{itemize}
\tightlist
\item
  \textbf{Practice effects} occur when administering the measure or the
  treatment will lead to change Imagine a curriculum intervention for
  teaching a math concept -- it would be hard to convince a school to
  teach the same topic to students twice, and the effect of the second
  round of teaching would likely be quite different than the first!
\item
  \textbf{Sensitization effects} occur when seeing two versions of an
  intervention mean that you might respond differently to the second
  than the first because you have compared them and noticed the
  contrast. Consider a study on room lighting -- if the experimenters
  are constantly changing the lighting, participants may become aware
  that lighting is the focus of the study!
\item
  \textbf{Carry-over effects} refer to the case where one treatment
  might have a longer-lasting effect than the measurement period. For
  example, imagine a study in which one treatment was to make
  participants frustrated with an impossible puzzle; if a second
  condition were given after this first one, participants might still be
  frustrated, leading to spill-over of effects between conditions.
\end{itemize}

All of these issues can lead to real concerns with respect to
within-participant designs. But the desire for effect estimates that are
completely unbiased by these concerns may lead to the overuse of
between-participant designs (\protect\hyperlink{ref-gelman2017}{Gelman
2017}). As we mentioned above, between-participant designs come at a
major cost in terms of power and precision.

An alternative approach is to acknowledge the possibility of carry-over
type effects and seek to mitigate them. First, you can make sure that
the order of condition is randomized or balanced (see below); and
second, you can analyze carryover effects these within your statistical
model (for example by estimating the interaction of condition and
order).\sidenote{\footnotesize Even when one factor must be varied between
  participants, it is often still possible to vary others within
  subjects, leading to a \textbf{mixed} design in which some factors are
  between and others within.}

We summarize the state of affairs from our perspective in
Figure~\ref{fig-design-between-within}. We think that within-participant
designs should be preferred whenever possible. This conclusion is also
consistent with meta-research we've done on replications from our
course: across 176 student replications, the use of a within-subjects
design was the strongest correlate of a successful replication
(\protect\hyperlink{ref-boyce2023}{Boyce, Mathur, and Frank 2023}).

\begin{figure}

\sidecaption{\label{fig-design-between-within}Pros and cons of between-
vs.~within-participant designs. We recommend within-participant designs
when possible.}

{\centering \includegraphics{images/design/between-within.png}

}

\end{figure}

\hypertarget{repeated-measurements-and-experimental-items}{%
\subsection{Repeated measurements and experimental
items}\label{repeated-measurements-and-experimental-items}}

We just discussed decision-making about whether to administer multiple
\emph{manipulations} to a single participant. An exactly analogous
decision comes up for \emph{measures}! And our take-home will be
similar: unless there are specific difficulties that come up, it's
usually a very good idea to take multiple measurements (experimental
\textbf{trials}) from each participant in each condition.

You can create a between-participants design where you administer your
manipulation and then measure multiple times. This scenario is pictured
in Figure~\ref{fig-design-rm-between}). Sometimes this works quite well.
For example, imagine a transcranial magnetic stimulation (TMS)
experiment: participants receive neural stimulation for a period of
time, targeted at a particular region. Then they perform some
measurement task repeatedly until it wears off. The more times they
perform the measurement task, the better the estimate of whatever effect
(when compared to a control of TMS to another region, say).

\begin{figure}

\sidecaption{\label{fig-design-rm-between}A between-participants,
repeated-measures design.}

{\centering \includegraphics{images/design/rm-between2.png}

}

\end{figure}

Sometimes this design is called a \textbf{repeated measures} design, but
terminology here is tricky again. The term ``repeated measures'' refers
to any experiment where each participant is measured more than once,
including both between-participants \emph{and} within-participants
designs.\sidenote{\footnotesize We're talking about multiple trials with the same
  measure, not multiple distinct measures. As we discussed in
  \textbf{?@sec-measurement}, we tend to be against measuring lots of
  different things in a single experiment -- in part because of the
  concerns that we're articulating in this chapter: if you have time,
  it's better to make more precise measures of what you care about most.
  Measuring one thing well is hard enough. Much better to measure one
  thing well than many things badly.} Our advice is \emph{both} to use
within-participants designs \emph{and} to get multiple measurements from
each participant.

Why? In the last subsection, we described how variability in our
estimates in a between-participants design depend on three components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  true condition differences,
\item
  sampling variation between conditions, and
\item
  measurement error
\end{enumerate}

Within-participants designs are good because they don't include source
\#2. Repeated measurements reduce source \#3: The more times you
measure, the lower your measurement error -- leading to greater measure
reliability!

There are problems with repeating the same measure many times, however.
Some measures can't be repeated without altering the response. To take
an obvious example, we can't give the exact same math problem twice and
get two useful measurements of mathematical ability! The typical
solution to this problem is to create multiple \textbf{items}. In the
case of a math assessment, you create multiple problems that you believe
test the same concept but have different numbers or other superficial
characteristics.

Using multiple items for measurement is good for two reasons. First, it
reduces measurement error by allowing responses to be combined across
items. But second, it increases the generalizability of the measurement.
An effect that is consistent across many different items is more likely
to be an effect that can be generalized to a whole class of stimuli --
in precisely the same way that the use of multiple participants can
license generalizations across a population of people
(\protect\hyperlink{ref-clark1973}{Clark 1973}).

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{stimulus-specific-effects}{%
\section*{Stimulus-specific effects}\label{stimulus-specific-effects}}
\addcontentsline{toc}{section}{Stimulus-specific effects}

\markright{Stimulus-specific effects}

Imagine you're a psycholinguist who has the hypothesis that nouns are
processed faster than verbs. You run an experiment where you pick out
ten verbs and ten nouns, then measure a large sample of participants'
reading time for each of these. You find strong evidence for the
predicted effect and publish a paper on your claim. The only problem is
that, at the same time, someone else has done exactly the same study --
with different nouns and verbs -- and published a paper making the
opposite claim. When this happens, it is possible that each effect is
driven by the specific experimental items that were chosen, rather than
a generalization that is true of nouns and verbs in general
(\protect\hyperlink{ref-clark1973}{Clark 1973}).

The problem of generalization from sample to population is not new -- as
we discussed in \textbf{?@sec-inference}, we are constantly making this
kind of inference with the samples of people that participate in our
experiments. Our classic statistical techniques are designed to quantify
our ability to generalize from a sample of participants to a population,
so we recognize that a very small sample size leads to a weak
generalization. The exact same issue comes up with \emph{items}: a very
small sample of experimental items leads to a weak generalization to the
population of items.

Item effects are kind of like accidentally finding a group of ten people
whose left toes are longer than their right ones. If you continued to
measure the same group's toes, you could continue to replicate the
difference in length. But that doesn't mean it's true of the population
as a whole.

This kind of \textbf{stimulus generalizability} problem comes up across
many different areas of psychology. In one example, hundreds of papers
were written about a phenomenon called the ``risky shift'' -- in which
groups deliberating about a decision would produce riskier decisions
than individuals. Unfortunately, this phenomenon appeared to be
completely driven by the specific choice of vignettes that groups
deliberated about, with some stories producing a risky shift and others
producing a more conservative shift
(\protect\hyperlink{ref-westfall2015}{Westfall, Judd, and Kenny 2015}).

Another example comes from the memory literature, where a classic paper
by Baddeley, Thomson, and Buchanan
(\protect\hyperlink{ref-baddeley1975}{1975}) suggested that words that
take longer to pronounce (``tycoon'' or ``morphine'') would be
remembered worse than words that took a shorter amount of time
(``ember'' or ``wicket'') even when they had the same number of
syllables. This effect also appears to be driven by the specific sets of
words chosen in the original paper. It's very replicable with that
particular stimulus set but not generalizable across other sets
(\protect\hyperlink{ref-lovatt2000}{Lovatt, Avons, and Masterson 2000}).

The implication of these examples is clear: experimenters need to take
care in both their experimental design and analysis to avoid
overgeneralizing from their stimuli to a broader construct. Three
primary steps can help experimenters avoid this pitfall:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To maximize generality, use samples of experimental items -- words,
  pictures, or vignettes -- that are comparable in size to your samples
  of participants.
\item
  When replicating an experiment, consider taking a new sample of items
  as well as a new sample of participants. It's more work to draft new
  items, but it will lead to more robust conclusions.
\item
  When experimental items are sampled random from a broader population,
  use a statistical model that includes this sampling process (e.g.,
  mixed effects models with random intercepts for items from
  \textbf{?@sec-models}).
\end{enumerate}

\end{tcolorbox}

One variation on the repeated measures, between-participants design is a
specific version where the measure is administered both before (pre-)
and after (post-) intervention, as in Figure~\ref{fig-design-pre-post}).
This design is sometimes known as a \textbf{pre-post} design. It is
extremely common in cases where the intervention is larger-scale and
harder to give within-participants, such as in a field experiment where
a policy or curriculum is given to one sample and not to another. The
pre measurements can be used to subtract participant-level variability
out and recover a more precise estimate of the treatment effect. Recall
that our treatment effect in a pure between participants design is
\(\beta = \theta_{T} - \theta_{C}\). In a pre-post design, we can do
better by computing
\(\beta = (\theta_{T_{post}} - \theta_{T_{pre}}) - (\theta_{C_{post}} - \theta_{C_{pre}}\).
This equation says ``how much more did the treatment group go up than
the control group?\sidenote{\footnotesize This estimate is sometimes called a
  ``difference in differences'' and is very widely used in the field of
  econometrics, both in experimental and quasi-experimental cases
  (\protect\hyperlink{ref-cunningham2021}{Cunningham 2021}).}

\begin{figure}

\sidecaption{\label{fig-design-pre-post}A between-participants, pre-post
design.}

{\centering \includegraphics{images/design/pre-post2.png}

}

\end{figure}

In sum, within-participants, repeated measurement designs are the bread
and butter of most research in perception, psychophysics, and cognitive
psychology. When both manipulations and measures can be repeated, these
designs afford high measurement precision even with small sample sizes;
they are recommended whenever possible.

\hypertarget{discrete-and-continuous-experimental-manipulations}{%
\subsection{Discrete and continuous experimental
manipulations}\label{discrete-and-continuous-experimental-manipulations}}

Most experimental designs in psychology use discrete condition
manipulations: treatment vs.~control. In our view, this decision often
leads to a lost opportunity relative to a more continuous manipulation
of the strength of the treatment. The goal of an experiment is to
estimate a causal effect; ideally, this estimate can be generalized to
other contexts and used as a basis for theory. Measuring not just one
effect but instead a \textbf{dose-response} relationship -- how the
measure changes as the strength of the manipulation is changed -- has a
number of benefits in helping to achieve this goal.

Many manipulations can be \textbf{titrated} -- that is, their strength
can be varied continuously -- with a little creativity on the part of an
experimenter. A curriculum intervention can be applied at different
levels of intensity, perhaps by changing the number of sessions in which
it is taught. For a priming manipulation, the frequency or duration of
prime stimuli can be varied. Two stimuli can be morphed continuously so
that categorization boundaries can be examined.\sidenote{\footnotesize These methods
  are extremely common in perception and psychophysics research, in part
  because the dimensions being studied are often continuous in nature.
  It would be basically impossible to estimate a participant's visual
  contrast sensitivity \emph{without} continuously manipulating the
  contrast of the stimulus!}

\begin{figure}

\sidecaption{\label{fig-design-dose-schema}Three schematic designs.
(left) Control and treatment are two levels of a nominal variable.
(middle) Control is compared to ordered levels of a treatment. (right)
Treatment level is an interval or ratio variable such that points can be
connected and a parametric curve can be extrapolated.}

{\centering \includegraphics{images/design/dose-response2.png}

}

\end{figure}

Dose-response designs are useful because they provide insight into the
shape of the function mapping your manipulation to your measure. Knowing
this shape can inform your theoretical understanding! Consider the
examples given in Figure~\ref{fig-design-dose-schema}. If you only have
two conditions in your experiment, then the most you can say about the
relationship between your manipulation and your measure is that it
produces an effect of a particular magnitude; in essence, you are
assuming that condition is a nominal variable. If you have multiple
ordered levels of treatment, you can start to speculate about the nature
of the relationship between treatment and effect magnitude. But if you
can measure the strength of your treatment, then you can begin to
describe the nature of the relationship between the strength of
treatment and strength of effect via a parametric function (e.g., a
linear regression, a sigmoid, or other function.\sidenote{\footnotesize These
  assumptions are theory-laden, of course -- the choice of a linear
  function or a sigmoid is not necessary: nothing guarantees that
  simple, smooth, or monotonic functions are the right ones. The
  important point is that choosing a function makes explicit your
  assumptions about the nature of the treatment-effect relationship.}
These parametric functions can in turn allow you to generalize from your
experiment, making predictions about what would happen under
intervention conditions that you didn't measure directly!

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{tradeoffs-associated-with-titrated-designs}{%
\section*{Tradeoffs associated with titrated
designs}\label{tradeoffs-associated-with-titrated-designs}}
\addcontentsline{toc}{section}{Tradeoffs associated with titrated
designs}

\markright{Tradeoffs associated with titrated designs}

Like adults, babies like to look at more interesting, complex stimuli.
But do they uniformly prefer complex stimuli, or do they search for
stimuli at an appropriate level of complexity for their processing
abilities? To test this hypothesis, Brennan, Ames, and Moore
(\protect\hyperlink{ref-brennan1966}{1966}) exposed infants in three
different age groups (3, 8, and 14 weeks, N=30) to black and white
checkerboard stimuli with three different levels of complexity (2x2,
8x8, and 24x24).

Their findings are plotted in \textbf{?@fig-design-dose-ex}: the
youngest infants preferred the simplest stimuli, while infants at an
intermediate age preferred stimuli of intermediate complexity, and the
oldest infants preferred the most complex stimuli. These findings help
to motivate the theory that infants attend preferentially to stimuli
that provide appropriate learning input for their processing ability
(\protect\hyperlink{ref-kidd2012}{Kidd, Piantadosi, and Aslin 2012}).

\begin{figure}[H]

{\centering \includegraphics{009-design_files/figure-pdf/design-dose-ex-1.png}

}

\caption{Infants' looking time, plotted by stimulus complexity and
infant age. Data from Brennan et al., 1966. Standard errors were
unavailable.}

\end{figure}

If your goal is simply to detect whether an effect is zero or non-zero,
then dose-response designs do not achieve the maximum statistical power.
For example, if Brennan, Ames, and Moore
(\protect\hyperlink{ref-brennan1966}{1966}) simply wanted to achieve
maximal statistical power, they probably should have only tested two age
groups and two levels of complexity (say, 3 and 14 week infants and 2x2
and 24x24 checkerboards). That would have been enough to show an
interaction of complexity and age, and their greater resources devoted
to these four (as opposed to nine) conditions would mean more precise
estimates of each. But their findings would be less clearly supportive
of the view that infants prefer stimuli that are appropriate to their
processing ability, because no group would have preferred an
intermediate level of complexity (as the 9-week-olds apparently did). By
seeking to measure intermediate conditions, they provided a stronger
test of their theory.

\end{tcolorbox}

\hypertarget{choosing-your-manipulation}{%
\section{Choosing your manipulation}\label{choosing-your-manipulation}}

In the previous section, we reviewed a host of common experimental
designs. These designs provide a palette of common options for combining
manipulations and measures. But your choice must be predicated on the
specific manipulation you are interested in! In this section, we discuss
considerations for experimenters as they design their manipulation.

In \textbf{?@sec-measurement}, we talked about \emph{measurement}
validity, but the idea of validity concept can be applied to
manipulations as well as measures. In particular, a manipulation is
valid if it corresponds to the construct that the experimenter intends
to intervene on. In this context, \emph{internal} validity threats to
manipulations tend to refer to cases where factors in the experimental
design keep the intended manipulation from actually intervening on the
construct of interest. In contrast, \emph{external} validity threats to
manipulations tend to be cases where the manipulation simply doesn't
line up well with the construct of interest.

\hypertarget{internal-validity-threats-confounding}{%
\subsection{Internal validity threats:
Confounding}\label{internal-validity-threats-confounding}}

First and foremost, manipulations must actually manipulate the construct
whose causal effect is being estimated. If they \emph{actually}
manipulate something else instead, they are \textbf{confounded}. This
term is used widely in psychology, but it's worth revisiting what it
means. An \textbf{experimental confound} is a variable that is created
in the course of the experimental design that is both causally related
to the predictor and potentially also related to the outcome. As such,
it is a threat to \textbf{internal validity}.

Let's go back to our discussion of causal inference in
\textbf{?@sec-experiments}. Our goal was to use a randomized experiment
to estimate the causal effect of money on happiness. But just giving
people money is a big intervention that involves contact with
researchers -- contact alone can lead to an experimental effect even if
your manipulation fails. For that reason, many studies that provide
money to participants either give a small amount of money or a large
amount of money. This design keeps researcher contact consistent in both
conditions, implying that the difference in outcomes between these two
conditions should be due to the amount of money received (unless there
are other confounds!).

Suppose you were designing an experiment of this sort and you wanted to
follow our advice and use a within-participants design. You could
measure happiness, give participants \$100, wait a month and measure
happiness again, give participants \$1000, wait a month, and then
measure happiness for the third time. The trouble is, this design has an
obvious experimental confound (Figure~\ref{fig-design-money1}): the
order of the monetary gifts. Maybe happiness just went up more over
time, irrespective of getting the second gift.

\begin{marginfigure}

{\centering \includegraphics{images/design/money1.png}

}

\caption{\label{fig-design-money1}Confounding order and condition
assignment means that you can't make an inference about the link between
money and happiness.}

\end{marginfigure}

If you think your experimental design might have a confound, you should
think about ways to remove it. For example, \textbf{counterbalancing}
order across participants is a very safe choice. Some participants get
\$100 first and others get \$1000 first. That way, you are guaranteed
that the order of conditions will have no effect of the confound on your
average effect. The effect of this counterbalancing is that it ``snips''
the causal dependency between condition assignment and later time. We
notate this on our causal diagram with a scissors icon
(Figure~\ref{fig-design-money2}).\sidenote{\footnotesize In practice, counterbalancing
  is like adding an additional factor to your factorial design! But
  because the factor is a \textbf{nuisance factor} -- basically, one we
  don't care about -- we don't discuss it as a true condition
  manipulation. Despite that, it's a good practice to check for effects
  of these sorts of nuisance factors in your preliminary analysis. Even
  though your average effect won't be biased by it, it introduces
  variation that you might want to understand to interpret other effects
  and plan news studies.} Time can still have an effect on happiness,
but the effect is independent from the effect of condition and hence
your experiment can still yield an unbiased estimate of the condition
effect.

\begin{marginfigure}

{\centering \includegraphics{images/design/money2.png}

}

\caption{\label{fig-design-money2}Confounding between a specific
condition and the time at which it's administered can be removed by
counterbalancing or randomization of order.}

\end{marginfigure}

Counterbalancing gets trickier when you have too many levels on a
variable or multiple confounding variables. In that case, it may not be
possible to do a full counterbalance so that all combinations of these
factors are seen by equal numbers of participants. You may have to rely
on partial counterbalancing schemes or Latin square designs (see Depth
box above; in this case, the Latin squares are used to create orderings
of stimuli such that the position of each treatment in the order is
controlled across two other confounding variables).

Another option, especially useful for such tricky cases is
\textbf{randomization}, that is, choosing which level of a nuisance
variable to administer to the participant via a random choice.
Randomization is increasingly common now that many experimental
interventions are delivered by software. If you \emph{can} randomize
experimental confounds, you probably should. The only time you really
get in trouble with randomization is when you have a large number of
options, a small number of participants, or some combination of the two.
Then you can end up with unbalanced levels of the randomized factors.
Averaging across many experiments, a lack of balance will come out in
the wash, but in a single experiment, it can lead to unfortunate bias in
numbers.

A good approach to thinking through your experimental design is to walk
through the experiment step by step and think about potential confounds.
For each of these confounds, consider how it might be removed via
counterbalancing or randomization. As our case study shows, confounds
are not always obvious, especially in complex paradigms. There is no
sure-fire way to ensure that you have spotted every one -- sometimes the
best way to avoid them is simply to present your candidate design to a
skeptical friend.

\hypertarget{internal-validity-threats-placebo-demand-and-expectancy}{%
\subsection{Internal validity threats: Placebo, demand, and
expectancy}\label{internal-validity-threats-placebo-demand-and-expectancy}}

A second class of important threats to internal validity comes from
cases where the research design is confounded by factors related to how
the manipulation is administered, or even \emph{that} a manipulation is
administered. In some cases, these create confounds that can be
controlled; in others they must simply be understood and guarded
against. Rosnow and Rosenthal (\protect\hyperlink{ref-rosnow1997}{1997})
called these ``artifacts'': systematic errors related to research
\emph{on} people, conducted \emph{by} people.

A \textbf{placebo} effect is a positive effect on the measure that comes
as a result of participants' expectations about a treatment in the
context of research study. The classic example of a placebo is medical:
giving an inactive sugar pill as a ``treatment'' leads some patients to
report a reduction in whatever symptom they are being treated for.
Placebo effects are a major concern in medical research as well as a
fixture in experimental designs in medicine
(\protect\hyperlink{ref-benedetti2020}{Benedetti 2020}). The key insight
is that treatments must not simply be compared to a baseline of no
treatment but rather to a baseline in which the psychological aspects of
treatment are present but the ``active ingredient'' is not. In the terms
we have been using, the experience of receiving a treatment (independent
of the content of the treatment) is a confounding factor when you simply
compare treatment to no treatment conditions.

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{brain-training}{%
\section*{Brain training?}\label{brain-training}}
\addcontentsline{toc}{section}{Brain training?}

\markright{Brain training?}

Can doing challenging cognitive tasks make you smarter? In the late
2000s and early 2010s, a large industry for ``brain training'' emerged.
Companies like Lumos Labs, CogMed, BrainHQ, and CogniFit offered games
-- often modeled on cognitive psychology tasks -- that claimed to lead
to broad gains in memory, attention, and problem solving.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{images/design/jaeggi.png}

}

\caption{\label{fig-design-jaeggi}The primary outcome graph for Jaeggi
et al. (\protect\hyperlink{ref-jaeggi2008}{2008}).}

\end{figure}

These companies were basing their claims in part on a scientific
literature reporting that concerted training on difficult cognitive
tasks could lead to benefits that \textbf{transferred} to other
cognitive domains. Among the most influential of these was a study by
Jaeggi et al. (\protect\hyperlink{ref-jaeggi2008}{2008}). They conducted
four experiments in which participants (N=70 across the studies) were
assigned to either working memory training via a difficult working
memory task (the ``dual N-back'') or a no-training control, with
training varying from 8 days all the way to 19 days.

The finding from this study excited a tremendous amount of interest
because they reported not only gains in performance on the specific
training task but also on a general intelligence task that the
participants had trained on. While the control group's scores on these
tasks improved, presumably just from being tested twice, there was a
condition by time (pre- vs.~post) interaction such that the scores of
the trained groups (consolidated across all four training experiments)
grew significantly more over the training period
(Figure~\ref{fig-design-jaeggi}). These results were interpreted as
supporting transfer -- whereby training on one task leads to broader
gains -- a key goal for ``brain training.''

\begin{figure}[H]

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{images/design/jaeggi-disaggregated.png}

}

\caption{\label{fig-design-jaeggi-disagg}The four sub-experiments of
Jaeggi et al. (\protect\hyperlink{ref-jaeggi2008}{2008}), now
disaggregated. Panels show 8- (A), 12- (B), 17- (C), and 19-session (D)
studies. Note the different scales and measures. RAPM = Raven's Advanced
Progressive Matrices; BOMAT = Bochumer Matrizentest. From Redick et al.
(\protect\hyperlink{ref-redick2013}{2013}).}

\end{figure}

Careful readers of the original paper noticed signs of analytic
flexibility (as discussed in Chapters \textbf{?@sec-replication} and
\textbf{?@sec-inference}), however. For example, the key statistical
model was fit to dataset created by post-hoc consolidation of
experiments, which yielded \(p = .025\) on the key interaction
(\protect\hyperlink{ref-redick2013}{Redick et al. 2013}). When data were
disaggregated, it was clear that the measures and effects had differed
in each of the different sub-experiments
(Figure~\ref{fig-design-jaeggi-disagg}).

Several replications by the same group addressed some of these issues,
but still failed to show convincing evidence of transfer. In particular,
there was no comparison to an \textbf{active control group} in which
participants did some kind of alternative activity for the same amount
of time (\protect\hyperlink{ref-simons2016}{Simons et al. 2016}). Such a
comparison is critical because a comparison to a \textbf{passive control
group} (a group that does no intervention) confounds participants'
general effort and involvement in the study with the specific training
being used. Successful transfer compared to passive control could be the
result of participants' involvement, expectations, or motivation rather
than brain training per se.

A careful replication of the training study (N=74) with an active
control group and a wide range of outcome measures failed to find any
transfer effects from working-memory training
(\protect\hyperlink{ref-redick2013}{Redick et al. 2013}). A
meta-analysis of 23 studies concluded that their findings cast doubt on
working memory training for increasing cognitive functioning
(\protect\hyperlink{ref-melby-lervag2013}{Melby-Lervg and Hulme 2013}).
In one convincing and broad test of the cognitive transfer theory, a BBC
show (``Bang Goes The Theory'') encouraged its listeners to participate
in a six week online brain training study. More than 11,000 listeners
completed the pre- and post-tests and at least two training sessions.
Neither focused training of planning and reasoning nor broader training
on memory, attention and mathematics led to transfer to untrained tasks.

Placebo effects are one plausible explanation for some positive findings
in the brain training literature. Foroughi et al.
(\protect\hyperlink{ref-foroughi2016}{2016}) recruited participants to
participate via two different advertisements. The first advertised that
``numerous studies have shown working memory training can increase fluid
intelligence'' (``placebo treatment'' group) while the second simply
offered experimental credits (control group). After a single training
session, the placebo treatment group showed significant improvements to
their matrix reasoning abilities. Participants in the placebo treatment
group realized gains from training out of proportion with any they could
have realized through training. Further, those participants who
responded to the placebo treatment ad tended to endorse statements about
the malleability of intelligence, suggesting that they might have been
especially likely to self-select into the intervention.

Summarizing the voluminous literature on brain training, Simons et al.
(\protect\hyperlink{ref-simons2016}{2016}) wrote that ``Despite
marketing claims from brain-training companies of `proven
benefits'\ldots{} we find the evidence of benefits from cognitive brain
training to be `inadequate.'\,''

\end{tcolorbox}

If placebo effects reflect what participants expect from a treatment
then \textbf{demand characteristics} reflect what participants think
\emph{experimenters} want and their desire to help the experimenters
achieve that goal (\protect\hyperlink{ref-orne1962}{Orne 1962}). Demand
characteristics are often raised as a reason for avoiding
within-participants designs -- if participants become alert to the
presence of an intervention, they may then respond in a way that they
believe is helpful to the experimenter. Typical tools for controlling or
identifying demand characteristics include using a cover story to mask
the purpose of an experiment, using a debriefing procedure to probe
whether participants typically guessed the purpose of an experiment, and
(perhaps most effectively) creating a control condition with similar
demand characteristics but missing a key component of the experimental
intervention.\sidenote{\footnotesize If you use a cover story to mask the purpose of
  your experiment, it's worth thinking about whether you are using
  deception, which can raise ethical issues (see Chapter
  -\textbf{?@sec-ethics}). Certainly you should be sure to debrief
  participants about the true function of the experiment!}

The final entry into this list of internal validity threats is
\textbf{experimenter expectancy effects}, where the experimenter's
behavior biases participants in a way that results in the appearance of
condition differences where no true difference exists. The classic
example of such effects comes from the animal learning literature and
the story of Clever Hans. Clever Hans was a horse who appeared able to
do arithmetic by tapping out solutions with his hoof. On deeper
investigation, it became apparent that he was being cued by his
trainer's posture (apparently without the trainer's knowledge) to stop
tapping when the desired answer was reached. The horse knew nothing
about math, but the experimenter's expectations were altering the
horse's behavior across different conditions.

In any experiment delivered by human experimenters who know what
condition they are delivering, condition differences can result from
experimenters imparting their expectations.
Figure~\ref{fig-design-rosenthal} shows the results of a meta-analysis
estimating the size of expectancy effects across a range of domains. The
magnitudes are shocking. There is no question that experimenter
expectancy is sufficient to ``create'' many interesting phenomena
artifactually if we are not on guard against it. The mechanisms of
expectancy are an interesting research topic in their own right; in many
cases expectancies appear to be communicated non-verbally in much the
same way that Clever Hans learned
(\protect\hyperlink{ref-rosnow1997}{Rosnow and Rosenthal 1997}).

\begin{figure}

\sidecaption{\label{fig-design-rosenthal}Magnitudes of expectancy
effects. From Rosenthal (\protect\hyperlink{ref-rosenthal1994}{1994}).}

{\centering \includegraphics{images/design/rosenthal2004.png}

}

\end{figure}

In medical research, the gold standard is an experimental design where
neither patients nor experimenters know which condition the patients are
in.\sidenote{\footnotesize These are commonly referred to as \textbf{double blind}
  designs (though the term \textbf{masked} is now often preferred).}
Results from other designs are treated with suspicion because of their
vulnerability to demand and expectancy effects. In psychology, the most
common modern protection against experimenter expectancy is the delivery
of interventions by a computer platform that can give instructions in a
coherent and uniform way across conditions.

In the case of interventions that must be delivered by experimenters,
ideally experimenters should be unaware of which condition they are
delivering. On the other hand, the logistics of maintaining experimenter
ignorance can be quite complicated in psychology. For this reason, many
researchers opt for lesser degrees of control, for example, choosing to
standardize delivery of an intervention via a script. These designs are
sometimes necessary for practical reasons but should be scrutinized
closely. ``How can you rule out experimenter expectancy effects?'' is an
uncomfortable question that should be asked more frequently in seminars
and paper reviews.

\hypertarget{external-validity-of-manipulations}{%
\subsection{External validity of
manipulations}\label{external-validity-of-manipulations}}

The goal of a specific experimental manipulation is to operationalize a
particular causal relationship of interest. Just as the relationship
between measure and construct can be more or less valid, so too can the
relationship between manipulation and construct. How can you tell? Just
like in the case of measures, there's no one royal road to validity. You
need to make a validity argument (\protect\hyperlink{ref-kane1992}{Kane
1992}).\sidenote{\footnotesize One caveat is that the validity of a manipulation
  incorporates the validity of the manipulation \emph{and} the measure.
  You can't really have a good estimate of a causal effect if the
  measurement is invalid.}

For testing the effect of money on happiness, our manipulation was to
give participants \$1000. This manipulation is clearly face valid. But
how often do people just receive a windfall of cash, versus getting a
raise at work or inheriting money from a relative? Is the effect caused
by \emph{having} the money, or \emph{receiving} the money with no
strings attached? We might have to do more experiments to figure out
what aspect of the money manipulation was most important. Even in
straightforward cases like this one, we need to be careful about the
breadth of the claims we make.

Sometimes validity arguments are made based on the success of the
manipulation in producing some change in the measurement. In the the
implicit theory of mind case study we began with, the stimulus contained
an animated Smurf character, and the argument was that participants took
the Smurf's beliefs into account in making their judgments. This
stimulus choice seems surprising -- not only would participants have to
track the implicit beliefs of other \emph{people}, they would also have
to be tracking the beliefs of depictions of non-human, animated
characters. On the other hand, based on the success of the manipulation,
the authors made an \emph{a fortiori} argument: if people track even an
animated Smurf's beliefs, then they \emph{must} be tracking the beliefs
of real humans.

Let's look at one last example to think more about manipulation
validity. Walton and Cohen (\protect\hyperlink{ref-walton2011}{2011})
conducted a short intervention in which college students (N=92) read
about social belonging and the challenges of the transition to college
and then reframed their own experiences using these ideas. This
intervention led to long-lasting changes in grades and well-being. While
the intervention undoubtedly had a basis in theory, part of our
understanding of the validity of the intervention comes from its
efficacy: sense of belonging \emph{must} be a powerful factor if
intervening on it causes such big changes in the outcome
measures.\sidenote{\footnotesize On the other hand, if the manipulation \emph{doesn't}
  produce a change in your measure, maybe the manipulation is invalid,
  but the construct still exists. Sense of belonging could still be
  important even if my particular intervention failed to alter it!} The
only danger is when the argument becomes circular -- a theory is correct
because the intervention yielded a success, and the intervention is
presumed to be valid because of the theory. The way out of this circle
is through replication and generalization of the intervention. If the
intervention repeatably produces the outcome, as has been shown in
replications of the sense of belonging intervention
(\protect\hyperlink{ref-walton2020}{Walton, Brady, and Crum 2020}), then
the manipulation becomes an intriguing target for future theories. The
next step in such a research program is to understand the limitations of
such interventions (sometimes called \textbf{boundary conditions}).

\hypertarget{summary-experimental-design}{%
\section{Summary: Experimental
design}\label{summary-experimental-design}}

In this chapter, we started by examining some common experimental
designs that allow us to measure effects associated with one or more
manipulations. Our advice, in brief, was: ``keep it simple!'' The
failure mode of many experiments is that they contain too many
manipulations, and these manipulations are measured with too little
precision.

Start with just a single manipulation, and measure it carefully. Ideally
this measurement should be done via a within-participants design unless
the manipulation is completely incompatible with this design. And if
this design can incorporate a dose-response manipulation, it is more
likely to provide a basis for quantitative theorizing.

How do you ensure that your manipulation is valid? A careful
experimenter needs to consider possible confounds and ensure that these
are controlled or randomized. They must also consider other artifacts
including placebo, demand, and expectancy effects. Finally, they must
begin thinking about the relation of their manipulation to the broader
theoretical construct whose causal role they hope to test.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Choose a classic study in your area of psychology. Analyze the design
  choices: how many factors were manipulated? How many measures were
  taken? Did it use a within-participants or between-participants
  design? Were measures repeated? Can you justify these choices with
  respect to trade-offs (e.g., carry-over effects, fatigue, etc.)?
\item
  Consider the same study. Design an alternative version that varies one
  of these design parameters (e.g., drops a manipulation or measure,
  changes within- to between-participants, etc.). What are the pros and
  cons of this change? Do you think your design improves on the
  original?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\tightlist
\item
  Much of this material is covered in more depth in the classic text on
  research methods: Rosenthal, R. \& Rosnow, R. L. 2008.
  \emph{Essentials of Behavioral Research: Methods and Data Analysis}.
  Third Edition. New York: McGraw-Hill.
  \url{http://dx.doi.org/10.34944/dspace/66}.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-12}{%
\section*{References}\label{bibliography-12}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-12}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-baddeley1975}{}}%
Baddeley, Alan D, Neil Thomson, and Mary Buchanan. 1975. {``Word Length
and the Structure of Short-Term Memory.''} \emph{Journal of Verbal
Learning and Verbal Behavior} 14 (6): 575--89.

\leavevmode\vadjust pre{\hypertarget{ref-benedetti2020}{}}%
Benedetti, Fabrizio. 2020. \emph{Placebo Effects}. Oxford University
Press.

\leavevmode\vadjust pre{\hypertarget{ref-boyce2023}{}}%
Boyce, Veronica, Maya Mathur, and Michael C Frank. 2023. {``Eleven Years
of Student Replication Projects Provide Evidence on the Correlates of
Replicability in Psychology.''}

\leavevmode\vadjust pre{\hypertarget{ref-brennan1966}{}}%
Brennan, Wendy M, Elinor W Ames, and Ronald W Moore. 1966. {``Age
Differences in Infants' Attention to Patterns of Different
Complexities.''} \emph{Science} 151 (3708): 354--56.

\leavevmode\vadjust pre{\hypertarget{ref-clark1973}{}}%
Clark, Herbert H. 1973. {``The Language-as-Fixed-Effect Fallacy: A
Critique of Language Statistics in Psychological Research.''}
\emph{Journal of Verbal Learning and Verbal Behavior} 12 (4): 335--59.

\leavevmode\vadjust pre{\hypertarget{ref-cunningham2021}{}}%
Cunningham, Scott. 2021. \emph{Causal Inference}. Yale University Press.

\leavevmode\vadjust pre{\hypertarget{ref-el-kaddouri2020}{}}%
El Kaddouri, Rachida, Lara Bardi, Diana De Bremaeker, Marcel Brass, and
Roeljan Wiersema. 2020. {``Measuring Spontaneous Mentalizing with a Ball
Detection Task: Putting the Attention-Check Hypothesis by Phillips and
Colleagues (2015) to the Test.''} \emph{PSYCHOLOGICAL
RESEARCH-PSYCHOLOGISCHE FORSCHUNG} 84 (6): 1749--57.

\leavevmode\vadjust pre{\hypertarget{ref-foroughi2016}{}}%
Foroughi, Cyrus K, Samuel S Monfort, Martin Paczynski, Patrick E
McKnight, and PM Greenwood. 2016. {``Placebo Effects in Cognitive
Training.''} \emph{Proceedings of the National Academy of Sciences} 113
(27): 7470--74.

\leavevmode\vadjust pre{\hypertarget{ref-gelman2017}{}}%
Gelman, Andrew. 2017.
\url{https://statmodeling.stat.columbia.edu/2017/11/25/poisoning-well-within-person-design-whats-risk/}.

\leavevmode\vadjust pre{\hypertarget{ref-greenwald1976}{}}%
Greenwald, Anthony G. 1976. {``Within-Subjects Designs: To Use or Not to
Use?''} \emph{Psychological Bulletin} 83 (2): 314.

\leavevmode\vadjust pre{\hypertarget{ref-jaeggi2008}{}}%
Jaeggi, Susanne M, Martin Buschkuehl, John Jonides, and Walter J Perrig.
2008. {``Improving Fluid Intelligence with Training on Working
Memory.''} \emph{Proceedings of the National Academy of Sciences} 105
(19): 6829--33.

\leavevmode\vadjust pre{\hypertarget{ref-kane1992}{}}%
Kane, Michael T. 1992. {``An Argument-Based Approach to Validity.''}
\emph{Psychological Bulletin} 112 (3): 527.

\leavevmode\vadjust pre{\hypertarget{ref-kidd2012}{}}%
Kidd, Celeste, Steven T Piantadosi, and Richard N Aslin. 2012. {``The
Goldilocks Effect: Human Infants Allocate Attention to Visual Sequences
That Are Neither Too Simple nor Too Complex.''} \emph{PloS One} 7 (5):
e36399.

\leavevmode\vadjust pre{\hypertarget{ref-kovacs2010}{}}%
Kovcs, gnes Melinda, Ern Tgls, and Ansgar Denis Endress. 2010.
{``The Social Sense: Susceptibility to Others' Beliefs in Human Infants
and Adults.''} \emph{Science} 330 (6012): 1830--34.

\leavevmode\vadjust pre{\hypertarget{ref-lovatt2000}{}}%
Lovatt, Peter, Steve E Avons, and Jackie Masterson. 2000. {``The
Word-Length Effect and Disyllabic Words.''} \emph{The Quarterly Journal
of Experimental Psychology: Section A} 53 (1): 1--22.

\leavevmode\vadjust pre{\hypertarget{ref-mcclelland1993}{}}%
McClelland, Gary H, and Charles M Judd. 1993. {``Statistical
Difficulties of Detecting Interactions and Moderator Effects.''}
\emph{Psychological Bulletin} 114 (2): 376.

\leavevmode\vadjust pre{\hypertarget{ref-melby-lervag2013}{}}%
Melby-Lervg, Monica, and Charles Hulme. 2013. {``Is Working Memory
Training Effective? A Meta-Analytic Review.''} \emph{Developmental
Psychology} 49 (2): 270.

\leavevmode\vadjust pre{\hypertarget{ref-myung2009}{}}%
Myung, Jay I, and Mark A Pitt. 2009. {``Optimal Experimental Design for
Model Discrimination.''} \emph{Psychological Review} 116 (3): 499.

\leavevmode\vadjust pre{\hypertarget{ref-orne1962}{}}%
Orne, Martin T. 1962. {``On the Social Psychology of the Psychological
Experiment: With Particular Reference to Demand Characteristics and
Their Implications.''} \emph{American Psychologist} 17 (11): 776.

\leavevmode\vadjust pre{\hypertarget{ref-phillips2015}{}}%
Phillips, Jonathan, Desmond C Ong, Andrew DR Surtees, Yijing Xin,
Samantha Williams, Rebecca Saxe, and Michael C Frank. 2015. {``A Second
Look at Automatic Theory of Mind: Reconsidering Kov{}cs, t{}gl{}s,
and Endress (2010).''} \emph{Psychological Science} 26 (9): 1353--67.

\leavevmode\vadjust pre{\hypertarget{ref-redick2013}{}}%
Redick, Thomas S, Zach Shipstead, Tyler L Harrison, Kenny L Hicks, David
E Fried, David Z Hambrick, Michael J Kane, and Randall W Engle. 2013.
{``No Evidence of Intelligence Improvement After Working Memory
Training: A Randomized, Placebo-Controlled Study.''} \emph{Journal of
Experimental Psychology: General} 142 (2): 359.

\leavevmode\vadjust pre{\hypertarget{ref-rosenthal1994}{}}%
Rosenthal, Robert. 1994. {``Interpersonal Expectancy Effects: A 30-Year
Perspective.''} \emph{Current Directions in Psychological Science} 3
(6): 176--79.

\leavevmode\vadjust pre{\hypertarget{ref-rosnow1997}{}}%
Rosnow, Ralph, and Robert Rosenthal. 1997. \emph{People Studying People:
Artifacts and Ethics in Behavioral Research}. WH Freeman.

\leavevmode\vadjust pre{\hypertarget{ref-simons2016}{}}%
Simons, Daniel J, Walter R Boot, Neil Charness, Susan E Gathercole,
Christopher F Chabris, David Z Hambrick, and Elizabeth AL Stine-Morrow.
2016. {``Do {`Brain-Training'} Programs Work?''} \emph{Psychological
Science in the Public Interest} 17 (3): 103--86.

\leavevmode\vadjust pre{\hypertarget{ref-walton2020}{}}%
Walton, Gregory M, Shannon T Brady, and AJ Crum. 2020. {``The
Social-Belonging Intervention.''} \emph{Handbook of Wise Interventions:
How Social Psychology Can Help People Change}, 36--62.

\leavevmode\vadjust pre{\hypertarget{ref-walton2011}{}}%
Walton, Gregory M, and Geoffrey L Cohen. 2011. {``A Brief
Social-Belonging Intervention Improves Academic and Health Outcomes of
Minority Students.''} \emph{Science} 331 (6023): 1447--51.

\leavevmode\vadjust pre{\hypertarget{ref-westfall2015}{}}%
Westfall, Jacob, Charles M Judd, and David A Kenny. 2015. {``Replicating
Studies in Which Samples of Participants Respond to Samples of
Stimuli.''} \emph{Perspectives on Psychological Science} 10 (3):
390--99.

\leavevmode\vadjust pre{\hypertarget{ref-young2007}{}}%
Young, Liane, Fiery Cushman, Marc Hauser, and Rebecca Saxe. 2007. {``The
Neural Basis of the Interaction Between Theory of Mind and Moral
Judgment.''} \emph{Proceedings of the National Academy of Sciences} 104
(20): 8235--40.

\end{CSLReferences}

\hypertarget{sec-sampling}{%
\chapter{Sampling}\label{sec-sampling}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Discuss sampling theory and stratified sampling
\item
  Reason about the limitations of different samples, especially
  convenience samples
\item
  Consider sampling biases and how they affect your inferences
\item
  Learn how to choose and justify an appropriate sample size for your
  experiment
\end{itemize}

\end{tcolorbox}

As we keep reminding you, experiments are designed to yield measurements
of a causal effect. But a causal effect of what, and for whom? These are
questions that are often given surprisingly little air time in our
papers. Titles in our top journals read ``Daxy thinking promotes
fribbles,'' ``Doing fonzy improves smoodling,'' or ``Blicket practice
produces more foozles than smonkers.''\sidenote{\footnotesize Titles changed to
  protect the original authors. These researchers might very well have
  said more specific things in the text of their paper.} Each of these
uses \textbf{generic language} to state a claim that is implied to be
generally true (\protect\hyperlink{ref-dejesus2019}{DeJesus et al.
2019}),\sidenote{\footnotesize Generic language is a fascinating linguistic
  phenomenon. When we say things like ``mosquitoes transmit malaria,''
  we don't mean that \emph{all} mosquitoes do it, only something like
  ``it's a valid and diagnostic generalization about mosquitoes in
  contrast to other relevant insects or other creatures that they are
  spreaders of malaria'' (\protect\hyperlink{ref-tessler2019}{Tessler
  and Goodman 2019}).} but for each of these, we could reasonably ask
``doing fonzy improves smoodling \emph{for whom}?'' Is it everyone? Or a
particular set of people? And similarly, we might want to ask
``\emph{how much} and \emph{what kind} of fonzy reading?'' These are
questions about the \textbf{generalizability of research}.

We wouldn't let the authors get away with a fully universal version of
their claim: ``Doing {[}\emph{any}{]} fonzy improves smoodling
{[}\emph{for anyone}{]}.'' The non-generic version states a
generalization that goes way beyond the evidence we actually have. But
it seems that we are often OK with authors \emph{implying} (with generic
language) that their findings generalize broadly. Imagine for a second
what the completely specific version of one of these titles might look
like: ``Reading one particular selection of fonzy for fifteen minutes in
the lab improved 36 college students' smoodling scores on a
questionnaire.'' This paper sounds pretty narrow in its applicability!

We've already run into generalizability in our treatment of statistical
estimation and inference. When we estimated a particular quantity (say,
the effect of fonzy), we did so in our own sample. But we then used
inferential tools to reason about how the estimate in this
\textbf{sample} related to the parameter in the \textbf{population} as a
whole. How do we link up these \emph{statistical} tools for
generalization to the \emph{scientific} questions we have about the
generalizability of our findings? That's the question of this chapter.

A key set of decisions in experiment planning is what population to
sample from and how to sample. We'll start by talking about the basics
of \textbf{sampling theory}: different ways of sampling and the
generalizations they do and don't license. The second section of the
chapter will then deal with \textbf{sampling biases} that can compromise
our effect estimates. A final set of key decisions is about
\textbf{sample size} planning. In the third part of the chapter we'll
address this issue, starting with classic \textbf{power analysis} but
then introducing several other ways that an experimenter can plan and
justify their sample size.

\begin{tcolorbox}[colframe=.blue, title=\faMicroscope \enspace Case study]

\hypertarget{is-everyone-bad-at-describing-smells}{%
\section*{Is everyone bad at describing
smells?}\label{is-everyone-bad-at-describing-smells}}
\addcontentsline{toc}{section}{Is everyone bad at describing smells?}

\markright{Is everyone bad at describing smells?}

Since Darwin, scientists have assumed that smell is a vestigial sense in
humans -- one that we don't even bother to encode in language. In
English we don't even have consistent words for odors. We can say
something is ``stinky,'' ``fragrant'', or maybe ``musty,'' but beyond
these, most of our words for smells are about the \emph{source} of the
smell, not the qualities of it. Bananas, roses, and skunks all have
distinctive smells, but we don't have any vocabulary for naming what is
common or uncommon about them. And when we make up ad-hoc vocabulary,
it's typically quite inconsistent
(\protect\hyperlink{ref-majid2014}{Majid and Burenhult 2014}). The same
situation applies across many languages.

So, would it be a good generalization about human beings -- all people
-- that olfaction as a sense is de-emphasized relative to, say, vision?
This inference has a classic sample-to-population structure. Within
several samples of participants using widely-spoken languages, we
observe limited and inconsistent vocabulary for smells, as well as poor
discrimination. We use these samples to license an inference to the
population -- in this case, the entire human population.

\begin{figure}[H]

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{images/sampling/majid2014.png}

}

\caption{\label{fig-sampling-majid2014}Data from Majid and Burenhult
(\protect\hyperlink{ref-majid2014}{2014}) on the consistency of color
and odor naming in English and Jahai speakers. Higher values indicate
more consistent descriptions. Pie charts indicate the type of language
being used. Error bars show standard deviation.}

\end{figure}

But these inferences about the universal lack of olfactory vocabulary
are likely based on choosing non-representative samples! Multiple
hunter-gatherer groups appear to have large vocabularies for consistent
smell description. For example, the Jahai, a hunter-gatherer group on
the Malay Peninsula, have a vocabulary that includes at least twelve
words for distinct odors, for example /cs/, which names odors with a
``stinging smell'' like gasoline, smoke, or bat droppings. When Jahai
speakers are asked to name odors, they produce shorter and much more
consistent descriptions than English speakers -- in fact, their smell
descriptions were as consistent as their color descriptions
(Figure~\ref{fig-sampling-majid2014}). Further studies implicate the
hunter-gatherer lifestyle as a factor: while several hunter-gatherer
groups show good odor naming, nearby horticulturalist groups don't
(\protect\hyperlink{ref-majid2018}{Majid and Kruspe 2018}).

Generalizations about humans are tricky. If you want to estimate the
average odor naming ability, you could take a random sample of humans
and evaluate their odor naming. Most of the individuals in the sample
would likely speak English, Mandarin, Hindi, or Spanish. Almost
certainly, none of them would speak Jahai, which is spoken by only a
little more than a thousand people and is listed as
\href{https://www.ethnologue.com/language/jhi}{Threatened} by
Ethnologue. Your estimate of low odor naming stability might be a good
guess for the \emph{majority} of the world's population, but would tell
you little about the Jahai.

On the other hand, it's more complicated to jump from a statistical
generalization about average ability to a richer claim, like ``humans
have low olfactory naming ability.'' Such claims about universal aspects
of the human experience require much more care and much stronger
evidence (\protect\hyperlink{ref-piantadosi2014}{Piantadosi and Gibson
2014}). From a sampling perspective, human behavior and cognition show
immense and complex \textbf{heterogeneity} -- variability of individuals
and variability across clusters. Put simply, if we want to know what
people in general are like, we have to think carefully about which
people we include in our studies.

\end{tcolorbox}

\hypertarget{sampling-theory}{%
\section{Sampling theory}\label{sampling-theory}}

The basic idea of sampling is simple: you want to estimate some
measurement for a large or infinite population by measuring a sample
from that population.\sidenote{\footnotesize There are some tools for dealing with
  estimation in smaller populations where your sample is a substantial
  fraction of the population (e.g., a survey of your department where
  you get responses from half of the students). We won't discuss those
  here; our focus is on generalizing to large populations of humans.}
Sampling strategies are split into two categories. \textbf{Probability
sampling} strategies are those in which each member of the population
has some known, pre-specified probability of being selected to be in the
sample -- think, ``generalizing to Japanese people by picking randomly
from a list of everyone in Japan.'' \textbf{Non-probability sampling}
covers strategies in which probabilities are unknown or shifting, or in
which some members of the population could never be included in the
sample -- think, ``generalizing to Germans by sending a survey to a
German email list and asking people to forward the email to their
family.''

\hypertarget{classical-probability-sampling}{%
\subsection{Classical probability
sampling}\label{classical-probability-sampling}}

In classical sampling theory there is some \textbf{sampling frame}
containing every member of the population -- think of a giant list with
every adult human's name in it. Then we use some kind of
\textbf{sampling strategy}, maybe at the simplest just a completely
random choice, to select \(N\) humans from that sample frame, and then
we include them in our experiment. This scenario is the one that informs
all of our statistical results about how sample means converge to the
population mean (as in \textbf{?@sec-inference}).

Unfortunately, we very rarely do sampling of this sort in psychological
research. Gathering true probability samples from the large populations
that we'd like to generalize to is far too difficult and expensive.
Consider the problems involved in doing some experiment with a sample of
\emph{all adult humans}, or even \emph{adult English-speaking humans who
are located in the United States}. As soon as you start to think about
what it would take to collect a probability sample of this kind of
population, the complexities get overwhelming. How will you find their
names -- what if they aren't in the phone book? How will you contact
them -- what if they don't have email? How will they do your experiment
-- what if they don't have an up-to-date web browser? What if they don't
want to participate at all?

Instead, the vast majority of psychology research has been conducted
with \textbf{convenience samples}: non-probability samples that feature
individuals who can be recruited easily, such as college undergraduates
or workers on crowdsourcing platforms like Amazon Mechanical Turk (see
\textbf{?@sec-collection}). We'll turn to these below.

For survey research, on the other hand -- think of election polling --
there are many sophisticated techniques for dealing with sampling;
although this field is still imperfect, it has advanced considerably in
trying to predict complex and dynamic behaviors. One of the basic ideas
is the construction of \textbf{representative samples}: samples that
resemble the population in their representation of one or several
sociodemographic characteristics like gender, income, race and
ethnicity, age, or political orientation.

Representative samples can be constructed by probability sampling, but
they can also be constructed through non-probability methods like
recruiting quotas of individuals from different groups via various
different convenience methods. These methods are critical for much
social science research, but they have been used less frequently in
experimental psychology research and aren't necessarily a critical part
of the beginning experimentalist's toolkit.\sidenote{\footnotesize Readers can come up
  with counter-examples of recent studies that focus on representative
  sampling, but our guess is that they will prove the rule more
  generally. For example, a recent study tested the generality of growth
  mindset interventions for US high school students using a national
  sample (\protect\hyperlink{ref-yeager2019}{Yeager et al. 2019}). This
  large-scale study sampled more than 100 high schools from a sampling
  frame of all registered high schools in the US, then randomly assigned
  students within schools that agreed to participate. They then checked
  that the schools that agreed to participate were representative of the
  broader population of schools. This study is great stuff, but we hope
  you agree that if you find yourself in this kind of situation --
  planning a multi-investigator 5 year consortium study on a national
  sample -- you might want to consult with a statistician and not use an
  introductory book like this one.}

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{representative-samples-and-stratified-sampling}{%
\section*{Representative samples and stratified
sampling}\label{representative-samples-and-stratified-sampling}}
\addcontentsline{toc}{section}{Representative samples and stratified
sampling}

\markright{Representative samples and stratified sampling}

\textbf{Stratified sampling} is a cool method that can help you get more
precise estimates of your experimental effect, if you think it varies
across some grouping in your sample. Imagine you're interested in a
particular measure in a population -- say, attitudes towards tea
drinking across US adults -- but you think that this measure will vary
with one or more characteristics such as whether the adults are
frequent, infrequent, or non-coffee drinkers. Even worse, your measure
might be more variable within one group: perhaps most frequent and
infrequent coffee drinkers feel OK about tea, but as a group non-coffee
drinkers tend to hate it (most don't drink any caffeinated beverages).

A simple random sample from this heterogeneous population \emph{will}
yield statistical estimates that converge asymptotically to the correct
population average for tea-drinking attitudes. But it will do so more
slowly than ideal because any given sample may over- or under-sample
non-drinkers just by chance. In a small sample, if you happen to get too
many non-coffee drinkers, your estimate of attitudes will be biased
downward; if you happen to get too few, you will be biased upwards. All
of this will come out in the wash eventually, but any individual sample
(especially a small one) will be noisier than ideal.

But, if you know the proportion of frequent, infrequent, or non-coffee
drinkers in the population, you can perform stratified sampling within
those subpopulations to ensure that your sample is representative along
this dimension (\protect\hyperlink{ref-neyman1992}{Neyman 1992}). This
situation is pictured in Figure~\ref{fig-sampling-stratified}, which
shows how a particular sampling frame can be broken up into groups for
stratified sampling. The result is a sample that matches the population
proportions on a particular characteristic. In contrast, a simple random
sample can over- or under-sample the subgroups by chance.

\begin{figure}[H]

{\centering \includegraphics{images/sampling/stratified-sample2.png}

}

\caption{\label{fig-sampling-stratified}Illustration of stratified
sampling. The left panel shows the sampling frame. The upper frames show
the sampling frame stratified by a participant characteristic and a
stratified sample. The lower frame shows a simple random sample, which
happens to omit one group completely by chance.}

\end{figure}

Stratified sampling can lead to substantial gains in the precision of
your estimate. These gains are most prominent when either the groups
differ a lot in their mean or when they differ a lot in their variance.
There are several important refinements of stratified sampling in case
you think these methods are important for your problem. In particular,
\textbf{optimal sampling} can help you figure out how to over-sample
groups with higher variance. On the other hand, if the characteristic on
which you stratify participants doesn't relate to your outcome at all,
then estimates from stratified sampling converge just as fast as random
sampling (though it's a bit more of a pain to implement).

Figure~\ref{fig-sampling-stratified-sim} shows a simulation of the
scenario in Figure~\ref{fig-sampling-stratified}, in which each coffee
preference group has a different tea attitude mean, and the smallest
group has the biggest variance. Although the numbers here are invented,
it's clear that estimation error is much smaller in the stratified group
and estimation error declines much more quickly as samples get larger.

\begin{figure}[H]

{\centering \includegraphics{010-sampling_files/figure-pdf/fig-sampling-stratified-sim-1.png}

}

\caption{\label{fig-sampling-stratified-sim}Simulation showing the
potential benefits of stratification. Each dot is an estimated mean for
a sample of a particular size, sampled randomly or with stratification.
Red points show the mean and standard deviation of sample estimates.}

\end{figure}

Stratification is everywhere, and it's useful even in convenience
samples. For example, researchers who are interested in development
typically stratify their samples across ages (e.g., recruiting equal
numbers of two- and three-year-olds for a study of preschoolers). You
can estimate developmental change in a pure random sample, but you are
guaranteed good coverage of the range of interest when you stratify.

If you have an outcome that you think varies with a particular
characteristic, it's not a bad idea to consider stratification. But
don't go overboard -- you can drive yourself to distraction finding the
last left-handed non-binary coffee drinker to complete your sample.
Focus on stratifying when you know the measure varies with the
characteristic of interest.

\end{tcolorbox}

\hypertarget{convenience-samples-generalizability-and-the-weird-problem}{%
\section{Convenience samples, generalizability, and the WEIRD
problem}\label{convenience-samples-generalizability-and-the-weird-problem}}

Now let's go back to the question of generalizability. How generalizable
are the experimental effect estimates that we obtain in experiments that
are conducted exclusively with convenience samples? We'll start by
laying out the worst version of the problem of generalizability in
experimental psychology. We'll then try to pull back from the brink and
discuss some reasons why we might not want to be in despair despite some
of the true generalizability issues that plague the psychology
literature.

\hypertarget{the-worst-version-of-the-problem}{%
\subsection{The worst version of the
problem}\label{the-worst-version-of-the-problem}}

Psychology is the study of the human mind. But from a sampling theory
standpoint, not a single estimate in the published literature is based
on a simple random sample from the human population. And the situation
is worse than that. Here are three of the most severe issues that have
been raised regarding the generalizability of psychology research.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Convenience samples}. Almost all research in experimental
  psychology is performed with convenience samples. This issue has led
  to the remark that ``the existing science of human behavior is largely
  the science of the behavior of sophomores'' (McNemar, 1946, quoted in
  \protect\hyperlink{ref-rosenthal1984}{Rosenthal and Rosnow 1984}). The
  samples we have easy access to just don't represent the populations we
  want to describe! At some point there was a social media account
  devoted to finding biology papers that made big claims about curing
  diseases and appending the qualifier ``in mice'' to them. We might
  consider whether we need to do the same to psychology papers. Would
  ``Doing fonzy improves smoodling \emph{in sophomore college
  undergraduates in the Western US}'' make it into a top journal?
\item
  \textbf{The WEIRD problem}. Not only are the convenience samples that
  we study not representative of the local or national contexts in which
  they are recruited, those local and national contexts are also
  unrepresentative of the broad range of human experiences. Henrich,
  Heine, and Norenzayan (\protect\hyperlink{ref-henrich2010}{2010})
  coined the term WEIRD (Western, Educated, Industrialized, Rich, and
  Democratic) to sum up some of the ways that typical participants in
  psychology experiments differ from other humans. The vast
  over-representation of WEIRD participants in the literature has led
  some researchers to suggest that published results simply reflect
  ``WEIRD psychology'' -- a small and idiosyncratic part of a much
  broader universe of human psychology.\sidenote{\footnotesize The term WEIRD has been
    very useful in drawing attention to the lack of representation of
    the breadth of human experiences in experimental psychology. But one
    negative consequence of this idea has been the response that what we
    need to do as a field is to sample more ``non-WEIRD'' people. It is
    not helpful to suggest that every culture outside the WEIRD moniker
    is the same (\protect\hyperlink{ref-syed2020}{Syed and Kathawalla
    2020})! A better starting point is to consider the way that cultural
    variation might guide our choices about sampling.}
\item
  \textbf{The item sampling issue}. As we discussed in
  \textbf{?@sec-models} and -\textbf{?@sec-design}, we're typically not
  just trying to generalize to new people, we're also trying to
  generalize to new stimuli
  (\protect\hyperlink{ref-westfall2015}{Westfall, Judd, and Kenny
  2015}). The problem is that our experiments often use a very small set
  of items, constructed by experimenters in an ad-hoc way rather than
  sampled as representatives of a broader population of stimuli that we
  hope to generalize to with our effect size estimate. What's more, our
  statistical analyses sometimes fail to take stimulus variation into
  account. Unless we know about the relationship of our items to the
  broader population of stimuli, our estimates may be based on
  unrepresentative samples in yet another way.
\end{enumerate}

In sum, experiments in the psychology literature primarily measure
effects from WEIRD convenience samples of people and unsystematic
samples of experimental stimuli. Should we throw up our hands and resign
ourselves to an ungeneralizable ``science'' of sample-specific anecdotes
(\protect\hyperlink{ref-yarkoni2020}{Yarkoni 2020})?

\hypertarget{reasons-for-hope-and-ways-forward}{%
\subsection{Reasons for hope and ways
forward}\label{reasons-for-hope-and-ways-forward}}

We think the situation isn't as bleak as the arguments above might have
suggested. Underlying each of the arguments above is the notion of
\textbf{heterogeneity}, the idea that particular effects vary in the
population.

Let's think through a very simple version of this argument. Say we have
an experiment that measures the smoodling effect, and it turns out that
smoodling is completely universal and invariant throughout the human
population. Now, if we want to get a precise estimate of smoodling, we
can take \emph{any} sample we want because everyone will show the same
pattern. Because smoodling is homogeneous, a non-representative sample
will not cause problems. It turns out that there are some phenomena like
this! For example, the Stroop task produces a consistent and similar
interference effect for almost everyone
(\protect\hyperlink{ref-hedge2018}{Hedge, Powell, and Sumner 2018}).

\begin{figure}

\sidecaption{\label{fig-sampling-heterogeneity}Illustration of the
interaction of heterogeneity and convenience samples. Colors indicate
arbitrary population subgroups. Left hand panels show sample
composition. Individual plots show the distribution of effect sizes in
each subgroup.}

{\centering \includegraphics{images/sampling/heterogeneity.png}

}

\end{figure}

Figure~\ref{fig-sampling-heterogeneity} illustrates this argument more
broadly. If you have a representative sample (top), then your sample
mean and your population mean will converge to the same value,
regardless of whether the effect is homogeneous (right) or heterogeneous
(right). That's the beauty of sampling theory. If you have a convenience
sample, one part of the population is over-represented in the sample.
The convenience sample doesn't cause problems if the size of your effect
is homogeneous in the population -- as with the case of smoodling or
Stroop. The trouble comes when you have an effect that is heterogeneous.
Because one group is over-represented, you get systematic bias in the
sample mean relative to the population mean.

So the problems listed above -- convenience samples, WEIRD samples, and
narrow stimulus samples -- only cause issues if effects are
heterogeneous. Are they? The short answer is, \emph{we don't know}.
Convenience samples are fine in the presence of homogeneous effects, but
we only use convenience samples so we may not know which effects are
homogeneous! Our metaphorical heads are in the sand.

We can't do better than this circularity without a theory of what should
be variable and what should be consistent between
individuals.\sidenote{\footnotesize Many people have theorized about the ways that
  culture and language in general might moderate psychological processes
  (e.g., \protect\hyperlink{ref-markus1991}{Markus and Kitayama 1991}).
  What we're talking about is related but slightly different -- a theory
  not of what's different, but of when there should be any difference
  and when there shouldn't be. As an example, Tsai
  (\protect\hyperlink{ref-tsai2007}{2007})'s ``ideal affect'' theory
  predicts that there should be more similarities in the distribution of
  actual affect across cultures, but that cultural differences should
  emerge in \emph{ideal affect} (what people want to feel like) across
  cultures. This is a theory of when you should see homogeneity and when
  you should see heterogeneity.} As nave observers of human behavior,
differences between people often loom large. We are keen observers of
social characteristics like age, gender, race, class, and education. For
this reason, our intuitive theories of psychology often foreground these
characteristics as the primary locus for variation between people.
Certainly these characteristics are important, but they fail to explain
many of the \emph{in}variances of human psychology as well. An
alternative line of theorizing starts with the idea that ``lower-level''
parts of psychology -- like perception -- should be less variable than
``higher-level'' faculties like social cognition. This kind of theory
sounds like a useful place to start, but there are also counter-examples
in the literature, including cases of cultural variation in perception
(\protect\hyperlink{ref-henrich2010}{Henrich, Heine, and Norenzayan
2010}).

Multi-lab, multi-nation studies can help to address questions about
heterogeneity, breaking the circularity we described above. For example,
ManyLabs 2 systematically investigated the replicability of a set of
phenomena across cultures (\protect\hyperlink{ref-klein2018}{Klein et
al. 2018}), finding limited variation in effects between WEIRD sites and
other sites. And in a study comparing a set of convenience and
probability samples, Coppock, Leeper, and Mullinix
(\protect\hyperlink{ref-coppock2018}{2018}) found limited demographic
heterogeneity in another sample of experimental effects from across the
social sciences. So there are at least some cases where we don't have to
worry as much about heterogeneity. More generally, large-scale studies
like these offer the possibility of measuring and systematically
characterizing demographic and cultural variation -- as well as how
variation itself varies between phenomena!

\hypertarget{biases-in-the-sampling-process}{%
\section{Biases in the sampling
process}\label{biases-in-the-sampling-process}}

In fields like econometrics or epidemiology that use observational
methods to estimate causal effects, reasoning about \textbf{sampling
biases} is a critical part of estimating generalizable effects. If your
sample does not represent the population of interest, then your effect
estimates will be biased.\sidenote{\footnotesize There is a deep literature on
  correcting these biases using causal inference frameworks. These
  techniques are well outside of the scope of this book, but if you're
  interested, you might look at some of the textbooks we recommended
  earlier, e.g. Cunningham
  (\protect\hyperlink{ref-cunningham2021}{2021}).} In the kind of
experimental work we are discussing many of these issues are addressed
by random assignment, including the first issue we treat:
\textbf{collider bias}. Not so for the second one, \textbf{attrition
bias}, which is an issue even in randomized experiments.

\hypertarget{collider-bias}{%
\subsection{Collider bias}\label{collider-bias}}

Imagine you want to measure the association between money and happiness
through a (non-experimental) survey. As we discussed in
\textbf{?@sec-experiments}, there are plenty of causal processes that
could lead to this association. Figure~\ref{fig-sampling-money} shows
several of these scenarios. Money could truly cause happiness (1);
happiness could cause you to make more money (2); or some third factor
-- say having lots of friends -- could cause people to be happier
\emph{and} richer (3.

\begin{marginfigure}

{\centering \includegraphics{images/sampling/money3-drawing.png}

}

\caption{\label{fig-sampling-money}Four reasons why money and happiness
can be correlated in a particular sample: 1. causal relationship, 2.
reverse causality, 3. confounding with friendship, and 4. collider bias.
For this last scenario, we have to assume that our measurement is
\emph{conditioned} on being in this sample, meaning we only look at the
association of money and happiness within the social services sample.}

\end{marginfigure}

But we can also create spurious associations if we are careless in our
sampling. One prominent problem that we can induce is called
\textbf{collider bias}. Suppose we recruited our sample from the clients
of a social services agency. Unfortunately, both of our variables might
affect presence in a social service agency
(Figure~\ref{fig-sampling-money}, 4): people might be interacting with
the agency for financial or benefits assistance, or else for
psychological services (perhaps due to depression).

Being in a social services sample is called a \textbf{collider} variable
because the two causal arrows \emph{collide} into it (they both point to
it. If we look just within the social services sample, we might see a
\emph{negative} association between wealth and happiness -- on average
the people coming for financial assistance would have less wealth and
more happiness than the people coming for psychological services. The
take-home here is that in observational research, you need to think
carefully about the causal structure of your sampling process
(\protect\hyperlink{ref-rohrer2018}{Rohrer 2018})!

If you are doing experimental research, you are mostly protected from
this kind of bias: Random assignment still ``works'' even in
sub-selected samples. If you run a money intervention within a
social-services population using random assignment, you can still make
an unbiased estimate of the effect of money on happiness. But that
estimate will only be valid \emph{for members of that sub-selected
population}.

\hypertarget{attrition-bias}{%
\subsection{Attrition bias}\label{attrition-bias}}

\textbf{Attrition} is when people drop out of your study. You should do
everything you can to improve participants' experiences (see
\textbf{?@sec-collection}) but sometimes -- especially when a
manipulation is onerous for participants or your experiment is
longitudinal and requires tracking participants for some time -- you
will still have participants withdraw from the study.

Attrition on its own can be a threat to the generalizability of an
experimental estimate. Imagine you do an experiment comparing a new very
intense after-school math curriculum to a control curriculum in a sample
of elementary school children over the course of a year. By the end of
the year, suppose many of your participants have dropped out. The
families who have stayed in the study are likely those who care most
about math. Even if you see an effect of the curriculum intervention,
this effect may generalize only to children in families who love math.

\begin{marginfigure}

{\centering \includegraphics{images/sampling/attrition-drawing.png}

}

\caption{\label{fig-sampling-attrition}Selective attrition can lead to a
bias even in the presence of random assignment. Dashed line indicates a
causal relationship that is unobserved by the researcher.}

\end{marginfigure}

But there is a further problem with attrition, known as
\textbf{selective attrition}. If one of your conditions leads to more
attrition than the other, you can end up with a biased estimate, even in
the presence of random assignment
(\protect\hyperlink{ref-nunan2018}{Nunan, Aronson, and Bankhead 2018})!
Imagine students in the control condition of your math intervention
experiment stayed in the sample, but the math intervention itself was so
tough that most families dropped out except those who were very
interested in math. Now, when you compare math scores at the end of the
experiment, your estimate will be biased
(Figure~\ref{fig-sampling-attrition}): scores in the math condition
could be higher simply because of differences in who stuck around to the
end.

Unfortunately, it turns out that attrition bias can be pretty common
even in short studies, especially when they are conducted online when a
participant can drop out simply by closing a browser window. This bias
can be serious enough to lead to false conclusions. For example, Zhou
and Fishbach (\protect\hyperlink{ref-zhou2016}{2016}) ran an experiment
in which they asked online participants to write about either 4 happy
events (low difficulty) or 12 happy events (high difficulty) from the
last year and then asked the participants to rate the difficulty of the
task. Surprisingly, the high difficulty task was rated as easier than
the low difficulty task! Selective attrition was the culprit for this
counter-intuitive conclusion: while only 26\% of participants dropped
out of the low difficulty condition, a full 69\% dropped out of the high
difficulty task. The 31\% that were left were so happy that it was
actually quite easy for them to generate 12 happy events, and so they
rated the objectively harder task as less difficult.

Always try to track and report attrition information. That lets you --
and others -- understand whether attrition is leading to bias in your
estimates or threats to the generalizability of your
findings.\sidenote{\footnotesize If you get interested, there is a whole field of
  statistics that focuses on \textbf{missing data} and provides models
  for reasoning about and dealing with cases where data might not be
  \textbf{missing completely at random}
  (\protect\hyperlink{ref-little2019}{Little and Rubin 2019} is the
  classic reference for these tools). The causal inference frameworks
  referenced above also have very useful ways of thinking about this
  sort of bias.}

\hypertarget{sample-size-planning}{%
\section{Sample size planning}\label{sample-size-planning}}

Now that you have spent some time considering your sample and what
population it represents, how many people will your sample contain?
Continuing to collect data until you observe a \(p < .05\) in an
inferential test is a good way to get a false positive. This practice,
known as ``optional stopping,'' is a good example of a practice that
invalidates \(p\)-values, much like the cases of analytic flexibility
discussed in \textbf{?@sec-replication} and \textbf{?@sec-inference}.

Decisions about when to stop collecting data should not be
data-dependent. Instead you should transparently declaring your data
collection \textbf{stopping rule} in your study preregistration (see
\textbf{?@sec-prereg}). This step will reassure readers that there is no
risk of bias from optional stopping.

The simplest stopping rule is ``I'll collect data until I get to a
target \(N\)'' -- all that's needed in this case is a value for \(N\).
Classically, this value was computed using \textbf{power analysis},
which can provide a value for which you have a good chance of rejecting
the null hypothesis (given a particular expected effect size).

A common justification for power analyses relies on knowing what size
effect you are expecting, which is often an unrealistic assumption. If
we knew what size effect we expected, wouldn't we already have the
information that we are conducting the experiment to get?

In fact, there are many different stopping rules that can be used to
justify a sample size (Table~\ref{tbl-stopping-rules}). Each of these
can provide a valid justification for a particular sample size, but they
are most useful in different situations. We'll begin by introducing
power analysis, a tool used in many sample size justifications.

\footnotesize
\renewcommand{\arraystretch}{2}

\hypertarget{tbl-stopping-rules}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.1596}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.4742}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.3662}}@{}}
\caption{\label{tbl-stopping-rules}Types of data collection stopping
rules.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stopping Rule
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Stopping Rule
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Example
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Power analysis & Stop at N for known probability of rejecting the null
given known effect size & Randomized trial with strong expectations
about effect size \\
Resource constraint & Stop collecting data after a certain amount of
time or after a certain amount of resources are used & Time-limited
field work \\
Smallest effect size of interest & Stop at N for known probability of
rejecting the null for effects greater than some minimum & Measurement
of a theoretically important effect with unknown magnitude \\
Precision analysis & Stop at N that provides some known degree of
precision in measure & Experimental measurement to compare with
predictions of cognitive models \\
Sequential analysis & Stop when a known inferential criterion is reached
& Intervention trial designed to accept or reject null with maximal
efficiency \\
\end{longtable}

\renewcommand{\arraystretch}{1}
\normalsize

\hypertarget{power-analysis}{%
\subsection{Power analysis}\label{power-analysis}}

\begin{marginfigure}

{\centering \includegraphics{images/sampling/power-alpha.png}

}

\caption{\label{fig-sampling-neyman-pearson}Standard decision matrix for
NHST. The lower-left hand quadrant shows power to reject the null.}

\end{marginfigure}

Let's start by reviewing the null-hypothesis significance testing
paradigm that we introduced in \textbf{?@sec-inference}. Recall that we
introduced a decision-theoretic view of testing in
\textbf{?@sec-inference}, shown again in
Figure~\ref{fig-sampling-neyman-pearson}. The idea was that we've got
some null hypothesis \(H_0\) and some alternative \(H_1\) -- something
like ``no effect'' and ``yes, there is some effect with known size''--
and we want to use data to decide which state we're in. \(\alpha\) is
our criterion for rejecting the null, conventionally set to
\(\alpha=.05\).

But what if \(H_0\) is actually false and the alternative \(H_1\) is
true? Not all experiments are equally well set up to reject the null in
those cases. Imagine doing an experiment with \(N=3\). In that case,
we'd almost always fail to reject the null, even if it were false. Our
sample would almost certainly be too small to rule out sampling
variation as the source of our observed data.

Let's try to quantify our willingness to \emph{miss} the effect -- the
false negative rate. We'll denote this probability with \(\beta\). If
\(\beta\) is the probability of missing an effect (failing to reject the
null when it's really false), then \(1-\beta\) is the probability that
we \emph{correctly reject the null when it is false}. That's what we
call the \textbf{statistical power} of the experiment.

We can only compute power if we know the effect size for the alternative
hypothesis. If the alternative hypothesis is a small effect, then the
probability of rejecting the null will typically be low (unless the
sample size is very large). In contrast, if the alternative hypothesis
is a large effect, then the probability of rejecting the null will be
higher.

\begin{figure}

\sidecaption{\label{fig-sampling-power}Illustration of how larger sample
sizes lead to greater power.}

{\centering \includegraphics{images/sampling/power.png}

}

\end{figure}

The same dynamic holds with sample size: the same effect size will be
easier to detect with a larger sample size than with a small one.
Figure~\ref{fig-sampling-power}) shows how this relationship works. A
large sample size creates a tighter null distribution (right side by
reducing sampling error. A tighter null distribution means you can
reject the null more of the time based on the variation in a true
effect. If your sample size is too small to detect your effect much of
the time, we call this being \textbf{under-powered}.\sidenote{\footnotesize You can
  also refer to a design as \textbf{over-powered}, though we object
  slightly to this characterization, since the value of large datasets
  is typically not just to reject the null but also to measure an effect
  with high precision and to investigate how it is moderated by other
  characteristics of the sample.}

Classical power analysis involves computing the sample size \(N\) that's
necessary in order to achieve some level of power, given \(\alpha\) and
a known effect size.\sidenote{\footnotesize Our focus here is on giving you a
  conceptual introduction to power analysis, but we refer you to Cohen
  (\protect\hyperlink{ref-cohen1992}{1992}) for a more detailed
  introduction.} The mathematics of the relationship between \(\alpha\),
\(\beta\), \(N\), and effect size have been worked out for a variety of
different statistical tests (\protect\hyperlink{ref-cohen2013}{Cohen
2013}) and codified in software like G*Power
(\protect\hyperlink{ref-faul2007}{Faul et al. 2007}) and the
\texttt{pwr} package for R
(\protect\hyperlink{ref-champely2017}{Champely et al. 2017}). For other
cases (including mixed effects models), you may have to conduct a
simulation in which you generate many simulated experimental runs under
known assumptions and compute how many of these lead to a significant
effect; luckily, R packages exist for this purpose as well, including
the \texttt{simr} package (\protect\hyperlink{ref-green2016}{Green and
MacLeod 2016}).

\hypertarget{power-analysis-in-practice}{%
\subsection{Power analysis in
practice}\label{power-analysis-in-practice}}

Let's do a power analysis for our hypothetical money and happiness
experiment. Imagine the experiment is a simple two group design in which
participants from a convenience population are randomly assigned either
to receive \$1000 and some advice on saving money (experimental
condition) vs.~just receiving the advice and no money (control
condition). We then follow up a month later and collect self-reported
happiness ratings. How many people should we have in our study in order
to be able to reject the null? The answer to this question depends on
our desired values of \(\alpha\) and \(\beta\) as well as our expected
effect size for the intervention.

For \(\alpha\) we will just set a conventional significance threshold of
\(\alpha = .05\). But what should be our desired level of power? The
usual standard in the social sciences is to aim for power above 80\%
(e.g., \(\beta < .20\)); this gives you 4 out of 5 chances to observe a
significant effect. But just like \(alpha = .05\), this is a
conventional value that is perhaps a little bit too loose for modern
standards -- a strong test of a particular effect should probably have
90\% or 95\% power.\sidenote{\footnotesize Really, researchers interested in using
  power analysis in their work should give some thought to what sort of
  chance of a false negative they are willing to accept. In exploratory
  research perhaps a higher chance of missing an effect is reasonable;
  in contrast, in confirmatory research it might make sense to aim for a
  higher level of power.}

These choices are relatively easy, compared to the fundamental issue:
our power analysis requires some expectation about the effect size for
our intervention! This is the \textbf{first fundamental problem of power
analysis}: if you knew the effect size, you might not need to do the
experiment!

So how are you supposed to get an estimate of effect size? Here are a
few possibilities:

\begin{itemize}
\item
  \textbf{Meta-analysis}. If there is a good meta-analysis of the effect
  that you are trying to measure (or something closely related), then
  you are in luck. A strong meta-analysis will have not only a precise
  effect size estimate but also some diagnostics detecting and
  correcting potential publication bias in the literature (see
  \textbf{?@sec-meta}). While these diagnostics are imperfect, they
  still can give you a sense for whether you can use the meta-analytic
  effect size estimate as the basis for a power analysis.
\item
  \textbf{Specific prior study}. A more complicated scenario is when you
  have only one or a handful of prior studies that you would like to use
  as a guide. The trouble is that any individual effect in the
  literature is likely to be inflated by publication and other selective
  reporting biases (see \textbf{?@sec-replication}). Thus, using this
  estimate likely means your study will be under-powered -- you might
  not get as lucky as a previous study did!
\item
  \textbf{Pilot testing}. Many people (including us) at some point
  learned that one way to do a power analysis is to conduct a pilot
  study, estimate the effect size from the pilot, and then use this
  effect estimate for power analysis in the main study. We don't
  recommend this practice. The trouble is that your pilot study will
  have a small sample size, leading to a very imprecise estimate of
  effect size (\protect\hyperlink{ref-browne1995}{Browne 1995}). If you
  over-estimate the effect size, your main study will be very
  under-powered. If you under-estimate, the opposite will be true. Using
  a pilot for power analysis is a recipe for problems.
\item
  \textbf{General expectations about an effect of interest}. In our
  view, perhaps the best way you can use power analysis (in the absence
  of a really strong meta-analysis, at least) is to start with a general
  idea about the size of effect you expect and would like to be able to
  detect. It is totally reasonable to say, ``I don't know how big my
  effect is going to be, but let's see what my power would be if it were
  \emph{medium-sized} (say \(d=.5\)), since that's the kind of thing
  we're hoping for with our money intervention.'' This kind of power
  analysis can help you set your expectations about what range of
  effects you might be able to detect with a given sample size.
\end{itemize}

For our money study, using our general expectation of a medium size
effect, we can compute power for \(d=.5\). In this case, we'll simply
use the two-sample \(t\)-test introduced in \textbf{?@sec-inference},
for which 80\% power at \(\alpha = .05\) and \(d=.5\) is achieved by
having \(N=64\) in each group.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Classic power analysis in R is quite simple using the \texttt{pwr}
package. The package offers a set of test-specific functions like
\texttt{pwr.t.test}. For each, you supply three of the four parameters
specifying effect size (\texttt{d}), number of observations
(\texttt{n}), significance level (\texttt{sig.level}), and power
(\texttt{power}); the function computes the fourth. For classic power
analysis, we leave out \texttt{n}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pwr.t.test}\NormalTok{(}\AttributeTok{d =}\NormalTok{ .}\DecValTok{5}\NormalTok{, }
           \AttributeTok{power =}\NormalTok{ .}\DecValTok{8}\NormalTok{, }
           \AttributeTok{sig.level =}\NormalTok{ .}\DecValTok{05}\NormalTok{,}
           \AttributeTok{type =} \StringTok{"two.sample"}\NormalTok{, }
           \AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

But it is also possible to use this same function to compute the power
achieved at a combination of \(n\) and \(d\), for example.

\end{tcolorbox}

There's a second issue, however. The \textbf{second fundamental problem
of power analysis} is that the real effect size for an experiment may be
zero. And in that case, \emph{no} sample size will let you correctly
reject the null. Going back to our discussion in
\textbf{?@sec-inference}, the null hypothesis significance testing
framework is just not set up to let you \emph{accept} the null
hypothesis. If you are interested in a bi-directional approach to
hypothesis testing in which you can accept \emph{and} reject the null,
you may need to consider Bayes Factor or \textbf{equivalence testing}
approaches (\protect\hyperlink{ref-lakens2018}{Lakens, Scheel, and
Isager 2018}), which do not fit the assumptions of classical power
analysis.

\hypertarget{alternative-approaches-to-sample-size-planning}{%
\subsection{Alternative approaches to sample size
planning}\label{alternative-approaches-to-sample-size-planning}}

Let's now consider some alternatives to classic power analysis that can
still yield reasonable sample size justifications.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Resource constraint}. In some cases, there are fundamental
  resource constraints that limit data collection. For example, if you
  are doing fieldwork, sometimes the right stopping criterion for data
  collection is ``when the field visit is over,'' since every additional
  datapoint is valuable. When pre-specified, these kinds of sample size
  justifications can be quite reasonable, although they do not preclude
  being under-powered to test a particular hypothesis.
\item
  \textbf{Smallest effect size of interest (SESOI)}. SESOI analysis is a
  variant on power analysis that includes some resource constraint
  planning. Instead of trying to intuit how big your target effect is,
  you instead choose a level below which you might not be interested in
  detecting the effect. This choice can be informed by theory (what is
  predicted), applied concerns (what sort of effect might be useful in a
  particular context), or resource constraints (how expensive or
  time-consuming it might be to run an experiment). In practice, SESOI
  analysis simply a classic power analysis with a particular small
  effect as the target.
\item
  \textbf{Precision-based sample planning}. As we discussed in
  \textbf{?@sec-inference}, the goal of research is not always to reject
  the null hypothesis! Sometimes -- we'd argue that it should be most of
  the time -- the goal is to estimate a particular causal effect of
  interest with a high level of precision, since these estimates are a
  prerequisite for building theories. If what you want is an estimate
  with known precision (say, a confidence interval of a particular
  width), you can compute the sample size necessary to achieve that
  precision (\protect\hyperlink{ref-bland2009}{Bland 2009};
  \protect\hyperlink{ref-rothman2018}{Rothman and Greenland
  2018}).\sidenote{\footnotesize In our experience, this kind of planning is most
    useful when you are attempting to gather measurements with
    sufficient precision to compare between computational models. Since
    the models can make quantitative predictions that differ by some
    known amount, then it's clear how tight your confidence intervals
    need to be.}
\item
  \textbf{Sequential analysis}. Your stopping rule need not be a hard
  cutoff at a specific \(N\). Instead, it's possible to plan a
  \textbf{sequential analysis} using either frequentist or Bayesian
  methods, in which you plan to stop collecting data once a particular
  inferential threshold is reached. For the frequentist version, the key
  thing that keeps sequential analysis from being \(p\)-hacking is that
  you pre-specify particular values of \(N\) at which you will conduct
  tests and then correct your \(p\)-values for having tested multiple
  times (\protect\hyperlink{ref-lakens2014}{Lakens 2014}). For Bayesian
  sequential analysis, you can actually compute a running Bayes factor
  as you collect data and stop when you reach a pre-specified level of
  evidence (\protect\hyperlink{ref-schonbrodt2017}{Schnbrodt et al.
  2017}). This latter alternative has the advantage of allowing you to
  collect evidence \emph{for} the null as well as against it.\sidenote{\footnotesize Another
    interesting variant is sequential parameter estimation, in which you
    collect data until a desired level of precision is achieved
    (\protect\hyperlink{ref-kelley2018}{Kelley, Darku, and Chattopadhyay
    2018}); this approach combines some of the benefits of both
    precision-based analysis and sequential analysis.}
\end{enumerate}

In sum, there are many different ways of justifying your sample size or
your stopping rule. The most important things are 1) to pre-specify your
strategy and 2) to give a clear justification for your choice.
Table~\ref{tbl-sampling-justification} gives an example sample size
justification that draws on several different concepts discussed here,
using classical power computations as one part of the justification. A
reviewer could easily follow the logic of this discussion and form their
own conclusion about whether this study had an adequate sample size and
whether it should have been conducted given the researchers'
constraints.

\footnotesize
\renewcommand{\arraystretch}{2}

\hypertarget{tbl-sampling-justification}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.8000}}@{}}
\caption{\label{tbl-sampling-justification}Example sample size
justification, referencing elements of SESOI, resource-limitation, and
power-based reasoning.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Justification Text
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Element
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Justification Text
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Background & We did not have strong prior information about the likely
effect size, so we could not compute a classical power analysis. \\
Smallest effect of interest & Because of our interest in meaningful
factors affecting word learning, we were interested in effect sizes as
small as d=.5. \\
Resource limitation & We were also limited by our ability to collect
data only at our on-campus preschool. \\
Power computation & We calculated that based on our maximal possible
sample size of N=120 (60 per group), we would achieve at least 80\%
power to reject the null for effects as small as d = .52. \\
\end{longtable}

\renewcommand{\arraystretch}{1}
\normalsize

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{sample-sizes-for-replication-studies}{%
\section*{Sample sizes for replication
studies}\label{sample-sizes-for-replication-studies}}
\addcontentsline{toc}{section}{Sample sizes for replication studies}

\markright{Sample sizes for replication studies}

Setting the sample size for a replication study has been a persistent
issue in the meta-science literature. Navely speaking, it seems like
you should be able to compute the effect size for the original study and
then simply use that as the basis for a classical power analysis.

This nave approach has several flaws, however. First, the effect size
from the original published paper is likely an overestimate of the true
effect size due to publication bias
(\protect\hyperlink{ref-nosek2021}{Nosek et al. 2021}). Second, the
power analysis will only yield the sample size at which the replication
will have a particular chance of rejecting the null at some criterion.
But it's quite possible that the original experiment could be \(p<.05\),
the replication could be \(p>.05\), \emph{and} 3) the original
experiment and the replication results are not significantly different
from each other. So a statistically significant replication of the
original effect size is not necessarily what you want to aim for.

Faced with these issues, a replication sample size can be planned in
several other ways. First, replicators can use standard strategies above
such as SESOI or resource-based planning to rule out large effects,
either with high probability or within a known amount of time or money.
If the SESOI is high or limited resources are allocated, these
strategies can produce an inconclusive result, however. A conclusive
answer can require a very substantial commitment of resources.

Second, Simonsohn (\protect\hyperlink{ref-simonsohn2015}{2015})
recommends the ``small telescopes'' approach. The idea is not to test
whether there \emph{is} an effect, but rather where there is an effect
\emph{large enough that the original study could have detected it}. The
analogy is to astronomy. If a birdwatcher points their binoculars at the
sky and claims to have discovered a new planet, we want to ask not just
whether there is a planet at that location, but also whether there is
any possibility that they could have seen it using binoculars -- if not,
perhaps they are right but for the wrong reasons! Simonsohn shows that,
if a replicator collects 2.5 times as large a sample as the original,
they have 80\% power to detect any effect that was reasonably detectable
by the original. This simple rule of thumb provides one good starting
place for conservative replication studies.

Finally, replicators can make use of sequential Bayesian analysis, in
which they attempt to gather substantial evidence relative to the
support for \(H_1\) \emph{or} \(H_0\). Sequential bayes is an appealing
option because it allows for efficient collection of data that reflects
whether an effect is likely to be present in a particular sample,
especially in the face of the sometimes prohibitively large samples
necessary for SESOI or ``small telescopes'' analyses.

\end{tcolorbox}

\hypertarget{chapter-summary-sampling}{%
\section{Chapter summary: Sampling}\label{chapter-summary-sampling}}

Your goal as an experimenter is to estimate a causal effect. But the
effect for whom? This chapter has tried to help you think about how you
generalize from your experimental sample to some target population. It's
very rare to be conducting an experiment based on a probability sample
in which every member of the population has an equal chance of being
selected. In the case that you are using a convenience sample, you will
need to consider how bias introduced by the sample could relate to the
effect estimate you observed. Do you think this effect is likely to be
very heterogeneous in the population? Are there theories that suggest
that it might be larger or smaller for the convenience sample you
recruited?

Questions about generalizability and sampling depend on the precise
construct you are studying, and there is no mechanistic procedure for
answering them. Instead, you simply have to ask yourself: how does my
sampling procedure qualify the inference I want to make based on my
data? Being transparent about your reasoning can be very helpful -- both
to you and to readers of your work who want to contextualize the
generality of your findings.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We want to understand human cognition generally, but do you think it
  is a more efficient research strategy to start by studying certain
  features of cognition (perception, for example) in WEIRD convenience
  populations and then later check our generalizations in non-WEIRD
  groups? What are the arguments against this efficiency-based strategy?
\item
  One alternative position regarding sampling is that the most
  influential experiments aren't generalizations of some number to a
  population; they are demonstration experiments that show that some
  particular effect is possible under some circumstances (think
  Milgram's conformity studies, see \textbf{?@sec-ethics}). On this
  argument, the specifics of population sampling are often secondary. Do
  you think this position makes sense?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  One line of argument says that we can't ever make generalizations
  about the human mind because so much of the historical human
  population is simply inaccessible to us (we can't do experiments on
  ancient Greek psychology). In other words, sampling from a particular
  population is \emph{also} sampling a particular moment in time. How
  should we qualify our research interpretations to deal with this
  issue?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\item
  The original polemic article on the WEIRD problem: Henrich, J., Heine,
  S. J., \& Norenzayan, A. (2010). The WEIRDest people in the world?
  \emph{Behavioral and Brain Sciences,} 33, 61-83.
\item
  A very accessible introduction to power analysis from its originator:
  Cohen, J. (1992) A power primer. \emph{Psychological Bulletin,} 112,
  155-9.
\item
  A thoughtful and in-depth discussion of generalizability issues:
  Yarkoni, T. (2020). The generalizability crisis. \emph{Behavioral and
  Brain Sciences,} 45, 1-37.
\end{itemize}

\end{tcolorbox}

\leavevmode\vadjust pre{\hypertarget{execution}{}}%
\part{Execution}

\hypertarget{bibliography-13}{%
\section*{References}\label{bibliography-13}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-13}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-bland2009}{}}%
Bland, John Martin. 2009. {``The Tyranny of Power: Is There a Better Way
to Calculate Sample Size?''} \emph{BMJ} 339 (October): b3985.

\leavevmode\vadjust pre{\hypertarget{ref-browne1995}{}}%
Browne, Richard H. 1995. {``On the Use of a Pilot Sample for Sample Size
Determination.''} \emph{Statistics in Medicine} 14 (17): 1933--40.

\leavevmode\vadjust pre{\hypertarget{ref-champely2017}{}}%
Champely, Stephane, Claus Ekstrom, Peter Dalgaard, Jeffrey Gill, Stephan
Weibelzahl, Aditya Anandkumar, Clay Ford, Robert Volcic, and Helios De
Rosario. 2017. {``Pwr: Basic Functions for Power Analysis.''}

\leavevmode\vadjust pre{\hypertarget{ref-cohen1992}{}}%
Cohen, Jacob. 1992. {``A Power Primer.''} \emph{Psychological Bulletin}
112 (1): 155.

\leavevmode\vadjust pre{\hypertarget{ref-cohen2013}{}}%
---------. 2013. \emph{Statistical Power Analysis for the Behavioral
Sciences}. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-coppock2018}{}}%
Coppock, Alexander, Thomas J Leeper, and Kevin J Mullinix. 2018.
{``Generalizability of Heterogeneous Treatment Effect Estimates Across
Samples.''} \emph{Proceedings of the National Academy of Sciences} 115
(49): 12441--46.

\leavevmode\vadjust pre{\hypertarget{ref-cunningham2021}{}}%
Cunningham, Scott. 2021. \emph{Causal Inference}. Yale University Press.

\leavevmode\vadjust pre{\hypertarget{ref-dejesus2019}{}}%
DeJesus, Jasmine M, Maureen A Callanan, Graciela Solis, and Susan A
Gelman. 2019. {``Generic Language in Scientific Communication.''}
\emph{Proceedings of the National Academy of Sciences} 116 (37):
18370--77.

\leavevmode\vadjust pre{\hypertarget{ref-faul2007}{}}%
Faul, Franz, Edgar Erdfelder, Albert-Georg Lang, and Axel Buchner. 2007.
{``G* Power 3: A Flexible Statistical Power Analysis Program for the
Social, Behavioral, and Biomedical Sciences.''} \emph{Behavior Research
Methods} 39 (2): 175--91.

\leavevmode\vadjust pre{\hypertarget{ref-green2016}{}}%
Green, Peter, and Catriona J MacLeod. 2016. {``SIMR: An r Package for
Power Analysis of Generalized Linear Mixed Models by Simulation.''}
\emph{Methods in Ecology and Evolution} 7 (4): 493--98.

\leavevmode\vadjust pre{\hypertarget{ref-hedge2018}{}}%
Hedge, Craig, Georgina Powell, and Petroc Sumner. 2018. {``The
Reliability Paradox: Why Robust Cognitive Tasks Do Not Produce Reliable
Individual Differences.''} \emph{Behavior Research Methods} 50 (3):
1166--86.

\leavevmode\vadjust pre{\hypertarget{ref-henrich2010}{}}%
Henrich, Joseph, Steven J Heine, and Ara Norenzayan. 2010. {``The
Weirdest People in the World?''} \emph{Behavioral and Brain Sciences} 33
(2-3): 61--83.

\leavevmode\vadjust pre{\hypertarget{ref-kelley2018}{}}%
Kelley, Ken, Francis Bilson Darku, and Bhargab Chattopadhyay. 2018.
{``Accuracy in Parameter Estimation for a General Class of Effect Sizes:
A Sequential Approach.''} \emph{Psychological Methods} 23 (2): 226.

\leavevmode\vadjust pre{\hypertarget{ref-klein2018}{}}%
Klein, Olivier, Tom E Hardwicke, Frederik Aust, Johannes Breuer, Henrik
Danielsson, Alicia Hofelich Mohr, Hans IJzerman, Gustav Nilsonne, Wolf
Vanpaemel, and Michael C Frank. 2018. {``A Practical Guide for
Transparency in Psychological Science.''}

\leavevmode\vadjust pre{\hypertarget{ref-lakens2014}{}}%
Lakens, Danil. 2014. {``Performing High-Powered Studies Efficiently
with Sequential Analyses.''} \emph{European Journal of Social
Psychology} 44 (7): 701--10.

\leavevmode\vadjust pre{\hypertarget{ref-lakens2018}{}}%
Lakens, Danil, Anne M. Scheel, and Peder M. Isager. 2018.
{``Equivalence Testing for Psychological Research: A Tutorial.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (2):
259--69. \url{https://doi.org/10.1177/2515245918770963}.

\leavevmode\vadjust pre{\hypertarget{ref-little2019}{}}%
Little, Roderick JA, and Donald B Rubin. 2019. \emph{Statistical
Analysis with Missing Data}. Vol. 793. John Wiley \& Sons.

\leavevmode\vadjust pre{\hypertarget{ref-majid2014}{}}%
Majid, Asifa, and Niclas Burenhult. 2014. {``Odors Are Expressible in
Language, as Long as You Speak the Right Language.''} \emph{Cognition}
130 (2): 266--70.

\leavevmode\vadjust pre{\hypertarget{ref-majid2018}{}}%
Majid, Asifa, and Nicole Kruspe. 2018. {``Hunter-Gatherer Olfaction Is
Special.''} \emph{Current Biology} 28 (3): 409--13.

\leavevmode\vadjust pre{\hypertarget{ref-markus1991}{}}%
Markus, Hazel R, and Shinobu Kitayama. 1991. {``Culture and the Self:
Implications for Cognition, Emotion, and Motivation.''}
\emph{Psychological Review} 98 (2): 224.

\leavevmode\vadjust pre{\hypertarget{ref-neyman1992}{}}%
Neyman, Jerzy. 1992. {``On the Two Different Aspects of the
Representative Method: The Method of Stratified Sampling and the Method
of Purposive Selection.''} In \emph{Breakthroughs in Statistics},
123--50. Springer.

\leavevmode\vadjust pre{\hypertarget{ref-nosek2021}{}}%
Nosek, Brian A, Tom E Hardwicke, Hannah Moshontz, Aurlien Allard,
Katherine S Corker, Anna Dreber Almenberg, Fiona Fidler, et al. 2021.
{``Replicability, Robustness, and Reproducibility in Psychological
Science.''} \emph{Annual Review of Psychology}.

\leavevmode\vadjust pre{\hypertarget{ref-nunan2018}{}}%
Nunan, David, Jeffrey Aronson, and Clare Bankhead. 2018. {``Catalogue of
Bias: Attrition Bias.''} \emph{BMJ Evidence-Based Medicine} 23 (1):
21--22.

\leavevmode\vadjust pre{\hypertarget{ref-piantadosi2014}{}}%
Piantadosi, Steven T, and Edward Gibson. 2014. {``Quantitative Standards
for Absolute Linguistic Universals.''} \emph{Cognitive Science} 38 (4):
736--56.

\leavevmode\vadjust pre{\hypertarget{ref-rohrer2018}{}}%
Rohrer, Julia M. 2018. {``Thinking Clearly about Correlations and
Causation: Graphical Causal Models for Observational Data.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (1):
27--42.

\leavevmode\vadjust pre{\hypertarget{ref-rosenthal1984}{}}%
Rosenthal, Robert, and Ralph L Rosnow. 1984. \emph{Essentials of
Behavioral Research: Methods and Data Analysis}. New York: McGraw-Hill.

\leavevmode\vadjust pre{\hypertarget{ref-rothman2018}{}}%
Rothman, Kenneth J, and Sander Greenland. 2018. {``Planning Study Size
Based on Precision Rather Than Power.''} \emph{Epidemiology} 29 (5):
599--603.

\leavevmode\vadjust pre{\hypertarget{ref-schonbrodt2017}{}}%
Schnbrodt, Felix D, Eric-Jan Wagenmakers, Michael Zehetleitner, and
Marco Perugini. 2017. {``Sequential Hypothesis Testing with Bayes
Factors: Efficiently Testing Mean Differences.''} \emph{Psychol.
Methods} 22 (2): 322--39.

\leavevmode\vadjust pre{\hypertarget{ref-simmons2011}{}}%
Simmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2011.
{``False-Positive Psychology: Undisclosed Flexibility in Data Collection
and Analysis Allows Presenting Anything as Significant.''}
\emph{Psychological Science} 22 (11): 1359--66.

\leavevmode\vadjust pre{\hypertarget{ref-simonsohn2015}{}}%
Simonsohn, Uri. 2015. {``Small Telescopes: Detectability and the
Evaluation of Replication Results.''} \emph{Psychol. Sci.} 26 (5):
559--69.

\leavevmode\vadjust pre{\hypertarget{ref-syed2020}{}}%
Syed, Moin, and U Kathawalla. 2020. {``Cultural Psychology, Diversity,
and Representation in Open Science.''} \emph{Cultural Methods in
Psychology: Describing and Transforming Cultures}, 427--54.

\leavevmode\vadjust pre{\hypertarget{ref-tessler2019}{}}%
Tessler, Michael Henry, and Noah D Goodman. 2019. {``The Language of
Generalization.''} \emph{Psychological Review} 126 (3): 395.

\leavevmode\vadjust pre{\hypertarget{ref-tsai2007}{}}%
Tsai, Jeanne L. 2007. {``Ideal Affect: Cultural Causes and Behavioral
Consequences.''} \emph{Perspectives on Psychological Science} 2 (3):
242--59.

\leavevmode\vadjust pre{\hypertarget{ref-westfall2015}{}}%
Westfall, Jacob, Charles M Judd, and David A Kenny. 2015. {``Replicating
Studies in Which Samples of Participants Respond to Samples of
Stimuli.''} \emph{Perspectives on Psychological Science} 10 (3):
390--99.

\leavevmode\vadjust pre{\hypertarget{ref-yarkoni2020}{}}%
Yarkoni, Tal. 2020. {``The Generalizability Crisis.''} \emph{Behav.
Brain Sci.} 45: 1--37.

\leavevmode\vadjust pre{\hypertarget{ref-yeager2019}{}}%
Yeager, David S, Paul Hanselman, Gregory M Walton, Jared S Murray,
Robert Crosnoe, Chandra Muller, Elizabeth Tipton, et al. 2019. {``A
National Experiment Reveals Where a Growth Mindset Improves
Achievement.''} \emph{Nature} 573 (7774): 364--69.

\leavevmode\vadjust pre{\hypertarget{ref-zhou2016}{}}%
Zhou, Haotian, and Ayelet Fishbach. 2016. {``The Pitfall of
Experimenting on the Web: How Unattended Selective Attrition Leads to
Surprising (yet False) Research Conclusions.''} \emph{Journal of
Personality and Social Psychology} 111 (4): 493.

\end{CSLReferences}

\hypertarget{sec-prereg}{%
\chapter{Preregistration}\label{sec-prereg}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Recognize the dangers of researcher degrees of freedom
\item
  Understand the differences between exploratory and confirmatory modes
  of research
\item
  Articulate how preregistration can reduce risk of bias and increase
  transparency
\end{itemize}

\end{tcolorbox}

\begin{quote}
When not planned beforehand, data analysis can approximate a projective
technique, such as the Rorschach, because the investigator can project
on the data his own expectancies, desires, or biases and can pull out of
the data almost any ``finding'' he may desire.

--- Theodore X. Barber (\protect\hyperlink{ref-barber1976}{1976})
\end{quote}

\begin{quote}
The first principle is that you must not fool yourself--and you are the
easiest person to fool\ldots{} After you've not fooled yourself, it's
easy not to fool other scientists. You just have to be honest in a
conventional way after that.

--- Richard Feynman (\protect\hyperlink{ref-feynman1974}{1974})
\end{quote}

Although there are plenty of \emph{incorrect} ways to design and analyse
experiments, there is no single \emph{correct} way. In fact, most
research decisions have many justifiable choices - sometimes called
``researcher degrees of freedom''. For example, will you stop data
collection after 20, 200, or 2000 participants? Will you remove outlier
values and how will you define them? Will you conduct subgroup analyses
to see whether the results are affected by sex, or age, or some other
factor?

Consider a simplified, hypothetical case where you have to make five
analysis decisions and there are five justifiable choices for each
decision --- this alone would result in 3125 (\(5^5\)) unique ways to
analyze the data! If you were to make these decisions \textbf{post hoc}
(after observing the data) then there's a danger your decisions will be
influenced by the outcome of the analysis (``data-dependent decision
making'') and skew towards choices that generate outcomes more aligned
with your personal preferences. Now think back to the last time you read
a research paper. Of all the possible ways that the data could have been
analyzed, how do you know that the researchers did not just select the
approach that generated results most favourable to their pet hypothesis?

In this chapter, we will find out why flexibility in the design,
analysis, reporting, and interpretation of experiments, combined with
data-dependent decision-making, can introduce bias, and lead to
scientists fooling themselves and fooling each other. We will also learn
about how \textbf{preregistration} -- the process of writing down and
registering your research decisions before you observe the data -- (and
other tools) can be used to protect our research from bias and provide
the transparency that other scientists need to properly evaluate and
interpret our work (\protect\hyperlink{ref-hardwicke2022}{Hardwicke and
Wagenmakers 2022}).

\begin{tcolorbox}[colframe=.blue, title=\faMicroscope \enspace Case study]

\hypertarget{undisclosed-analytic-flexibility}{%
\section*{Undisclosed analytic
flexibility?}\label{undisclosed-analytic-flexibility}}
\addcontentsline{toc}{section}{Undisclosed analytic flexibility?}

\markright{Undisclosed analytic flexibility?}

Educational apps for children are a huge market, but relatively few
randomized trials have been done to see whether or when they produce
educational gains. Filling this important gap, Berkowitz et al.
(\protect\hyperlink{ref-berkowitz2015}{2015}) reported a high-quality
field experiment of a free educational app, ``Bedtime Math at Home,''
with participants randomly assigned to either math or reading conditions
over the course of a full school year. Critically, along with random
assignment, the study also included standardized measures of math and
reading achievement. These measures allowed the authors to compute
effects in grade-level equivalents, a meaningful unit from a policy
perspective. The key result reported in the paper is shown in
Figure~\ref{fig-prereg-berkowitz}. Families who used the math app
frequently showed greater gains in math than the control group.

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{images/prereg/berkowitz-1.png}

}

\caption{\label{fig-prereg-berkowitz}Figure 1 of Berkowitz et al.
(\protect\hyperlink{ref-berkowitz2015}{2015}). Estimated years of math
achievement gained over the school year across groups.}

\end{figure}

Although this finding appeared striking, the figure didn't directly
visualize the primary causal effect of interest, namely the size of the
effect of study condition on math scores. Instead the data were
presented as estimated effects for specific levels of app usage, for a
matched subgroup of participants (panel A) and the entire group (panel
B).

Because the authors made their data openly available, it was possible
for Frank (\protect\hyperlink{ref-frank2016}{2016}) to do a simple
analysis to examine the causal effect of interest. When not splitting
the data by usage and adjusting by covariates, there was no significant
main effect of the intervention on math performance
Figure~\ref{fig-prereg-frank-berkowitz}. Since this analysis was not
favorable to the primary intervention -- and because it was not reported
in the paper -- it could have been the case that the authors had
analyzed the data several ways and chosen to present an analysis that
was more favorable to their hypotheses of interest.

\begin{figure}[H]

{\centering \includegraphics[width=0.6\textwidth,height=\textheight]{images/prereg/ITT.png}

}

\caption{\label{fig-prereg-frank-berkowitz}Estimated years of math
achievement gained over the school year across groups in the Berkowitz
et al. (\protect\hyperlink{ref-berkowitz2016}{2016}) math app trial.
Error bars show 95\% confidence intervals. Figure reproduced from Frank
(\protect\hyperlink{ref-frank2016}{2016}).}

\end{figure}

As is true for many papers prior to the rise of preregistration, it's
not possible to know definitively whether the reported analysis in
Berkowitz et al. (\protect\hyperlink{ref-berkowitz2015}{2015}) was
influenced by the authors' desired result. As we'll see below, such
data-dependent analyses can lead to substantial bias in reported
effects. This uncertainty about a paper's analytic strategy can be
avoided by the use of preregistration. In this case, preregistration
would have convinced readers that the analyses decisions were not
influenced by the data, thereby increasing the value of this otherwise
high-quality study.

\end{tcolorbox}

\hypertarget{lost-in-a-garden-of-forking-paths}{%
\section{Lost in a garden of forking
paths}\label{lost-in-a-garden-of-forking-paths}}

One way to visualize researcher degrees of freedom is as a vast decision
tree or ``garden of forking paths'' , Figure~\ref{fig-forking-paths}.
Each node represents a decision point and each branch represents a
justifiable choice. Each unique pathway through the garden terminates in
an individual research outcome.

\begin{marginfigure}

{\centering \includegraphics{images/prereg/forking-paths.png}

}

\caption{\label{fig-forking-paths}The garden of forking paths: many
justifiable but different analytic choices are possible for an
individual dataset.}

\end{marginfigure}

Because scientific observations typically consist of both noise (random
variation unique to this sample) and signal (regularities that will
reoccur in other samples), some of these pathways will inevitably lead
to results that are misleading (e.g., inflated effect sizes, exaggerated
evidence, or false positives).\sidenote{\footnotesize The signal-to-noise ratio is
  worse in particular situations (alas, common in psychology) that
  involve small effect sizes, high variation, and large measurement
  errors (\protect\hyperlink{ref-ioannidis2005}{Ioannidis 2005}).
  Researcher degrees of freedom may be constrained to some extent by
  strong theory (\protect\hyperlink{ref-oberauer2019}{Oberauer and
  Lewandowsky 2019}), community methodological norms and standards, or
  replication studies, though these constraints may be more implicit
  than explicit, and can still leave plenty of room for flexible
  decision-making.} The more potential paths there are in the garden
that you might explore, the higher the chance of encountering misleading
results.

Statisticians refer to this issue as a \textbf{multiplicity} (multiple
comparisons) problem. As we talked about in \textbf{?@sec-inference},
multiplicity can be addressed to some extent with statistical
countermeasures, like the Bonferroni correction; however, these
adjustment methods need to account for every path that you \emph{could
have} taken (\protect\hyperlink{ref-gelman2014}{Gelman and Loken 2014};
\protect\hyperlink{ref-degroot2014}{de Groot 1956/2014}). When you
navigate the garden of forking paths while working with the data, it is
easy to forget -- or even be unaware of -- every path that you could
have taken, so these methods can no longer be used effectively.

\hypertarget{data-dependent-analysis}{%
\subsection{Data-dependent analysis}\label{data-dependent-analysis}}

When a researcher navigates the garden of forking paths during data
analysis, their choices might be influenced by the data
(\textbf{data-dependent} decision making) which can introduce bias. If a
researcher is seeking a particular kind of result (which is likely --
see the Depth box below), then they are more likely to follow the
branches that steer them in that direction.

You could think of this a bit like playing a game of ``hot
(\textcolor{.orange}{\faFire}) or cold
(\textcolor{.blue}{\faSnowflake})'' where \textcolor{.orange}{\faFire}
indicates that the choice will move the researcher closer to a desirable
overall result and \textcolor{.blue}{\faSnowflake} indicates that the
choice will move them further away. Each time the researcher reaches a
decision point, they try one of the branches and get feedback on how
that choice affects the results. If the feedback is
\textcolor{.orange}{\faFire} then they take that branch. If the answer
is \textcolor{.blue}{\faSnowflake}, they try a different branch. If they
reach the end of a complete pathway, and the result is
\textcolor{.blue}{\faSnowflake}, maybe they even retrace their steps and
try some different branches earlier in the pathway. This strategy
creates a risk of bias because it systematically skews results towards
researcher's preferences
(\protect\hyperlink{ref-hardwicke2022}{Hardwicke and Wagenmakers
2022}).\sidenote{\footnotesize We say ``risk of bias'' rather than just ``bias''
  because in most scientific contexts, we do not have a known ground
  truth to compare the results to. So in any specific situation, we do
  not know the extent to which data-dependent decisions have actually
  biased the results.}

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{only-human-cognitive-biases-and-skewed-incentives}{%
\section*{Only human: Cognitive biases and skewed
incentives}\label{only-human-cognitive-biases-and-skewed-incentives}}
\addcontentsline{toc}{section}{Only human: Cognitive biases and skewed
incentives}

\markright{Only human: Cognitive biases and skewed incentives}

There's a storybook image of the scientist as an objective, rational,
and dispassionate arbiter of truth
(\protect\hyperlink{ref-veldkamp2017}{Veldkamp et al. 2017}). But in
reality, scientists are only human: they have egos, career ambitions,
and rent to pay! So even if we do want to live up to the storybook
image, its important to acknowledge that our decisions and behavior are
also influenced by a range of cognitive biases and external incentives
that can steer us away from that goal. Let's first look at some relevant
cognitive biases that might lead scientists astray:

\begin{itemize}
\item
  \textbf{Confirmation bias}: Preferentially seeking out, recalling, or
  evaluating information in a manner that reinforces one's existing
  beliefs (\protect\hyperlink{ref-nickerson1998}{Nickerson 1998}).
\item
  \textbf{Hindsight bias}: Believing that past events were always more
  likely to occur relative to our actual belief in their likelihood
  before they happened (``I knew it all along!'')
  (\protect\hyperlink{ref-slovic1977}{Slovic and Fischhoff 1977}).
\item
  \textbf{Motivated reasoning}: Rationalizing prior decisions so they
  are framed in a favorable light, even if they were irrational
  (\protect\hyperlink{ref-kunda1990}{Kunda 1990}).
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{images/prereg/apophenia.png}

}

\caption{\label{fig-apophenia}Examples of apophenia: Mars Face, Winnie
the Pooh Cloud, and Jesus Toast.}

\end{figure}

\begin{itemize}
\tightlist
\item
  \textbf{Apophenia}: Detecting seemingly meaningful patterns in noise
  (Figure~\ref{fig-apophenia})
  (\protect\hyperlink{ref-gilovich1985}{Gilovich, Vallone, and Tversky
  1985}).
\end{itemize}

\begin{figure}[H]

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{images/prereg/chrysalis.png}

}

\caption{\label{fig-chrysalis}The Chrysalis Effect, when ugly truth
becomes a beautiful fiction.}

\end{figure}

To make matters worse, the incentive structure of the scientific
ecosystem often adds additional motivation to get things wrong. The
allocation of funding, awards, and publication prestige is often based
on the nature of research results rather than research quality
(\protect\hyperlink{ref-smaldino2016}{Smaldino and McElreath 2016};
\protect\hyperlink{ref-nosek2012}{Nosek, Spies, and Motyl 2012}). For
example, many academic journals, especially those that are widely
considered to be the most prestigious, appear to have a preference for
novel, positive, and statistically significant results over incremental,
negative, or null results (\protect\hyperlink{ref-bakker2012}{Bakker,
Dijk, and Wicherts 2012}). There is also pressure to write articles with
concise, coherent, and compelling narratives
(\protect\hyperlink{ref-giner-sorolla2012}{Giner-Sorolla 2012}). This
set of forces incentivizes scientists to be ``impressive'' over being
right and encourages questionable research practices. The process of
iteratively p-hacking and HARKing one's way to a ``beautiful''
scientific paper has been dubbed ``The Chrysalis Effect''
, Figure~\ref{fig-chrysalis}.

In sum, scientists' human flaws -- and the scientific ecosystem's flawed
incentives -- highlight the need for transparency and intellectual
humility when reporting the findings of our research
(\protect\hyperlink{ref-hoekstra2020}{Hoekstra and Vazire 2020}).

\end{tcolorbox}

In the most egregious cases, a researcher may try multiple pathways
until they obtain a desirable result and then \textbf{selectively
report} that result, neglecting to mention that they have tried several
other analysis strategies.\sidenote{\footnotesize ``If you torture the data long
  enough, it will confess'' (\protect\hyperlink{ref-good1972}{Good
  1972}).} This is sometimes referred to as `p-hacking', because a
common goal is to get p-values to be statistically significant. You may
remember an example of this practice in \textbf{?@sec-replication},
where participants apparently became younger when they listened to
``When I'm 64'' by The Beatles. Another example of how damaging the
garden of forking paths can be comes from the ``discovery'' of brain
activity in a dead Atlantic Salmon! Researchers deliberately exploited
flexibility in the fMRI analysis pipeline and avoided multiple
comparisons corrections, allowing them to find brain activity where
there was only dead fish Figure~\ref{fig-salmon}.

\begin{marginfigure}

{\centering \includegraphics{images/prereg/salmon.jpeg}

}

\caption{\label{fig-salmon}By deliberately exploiting analytic
flexibility in the processing pipeline of fMRI data, Bennett, Miller,
and Wolford (\protect\hyperlink{ref-bennett2009}{2009}) were able to
identify `brain activity' in a dead Atlantic Salmon.}

\end{marginfigure}

\hypertarget{hypothesizing-after-results-are-known}{%
\subsection{Hypothesizing after results are
known}\label{hypothesizing-after-results-are-known}}

In addition to degrees of freedom in experimental design and analysis,
there is additional flexibility in how researchers \emph{interpret}
research results. As we discussed in \textbf{?@sec-theories}, theories
can accommodate even conflicting results in many different ways -- for
example, by positing auxiliary hypotheses that explain why a particular
datapoint is special.

The practice of selecting or developing your hypothesis after observing
the data has been called ``Hypothesizing After the Results are Known'',
or ``HARKing'' (\protect\hyperlink{ref-kerr1998}{Kerr 1998}). HARKing is
potentially problematic because it expands the garden of forking paths
and helps to justify the use of various additional design and analysis
decisions (Figure~\ref{fig-grid}). For example, you may come up with an
explanation for why an intervention is effective in men but not in women
in order to justify a post-hoc subgroup analysis based on sex (see Case
Study. The extent to which HARKing is problematic is contested (for
discussion see \protect\hyperlink{ref-hardwicke2022}{Hardwicke and
Wagenmakers 2022}). But at the very least it's important to be honest
about whether hypotheses were developed before or after observing the
data.

\begin{figure}

\sidecaption{\label{fig-grid}A grid of individual research results. The
horizontal axis provides a simplified illustration of the many
justifiable design and analysis choices that a scientist can use to
generate the evidence. The vertical axis illustrates that there are
often several potential hypotheses which could be constructed or
selected when interpreting the evidence. An unconstrained scientist can
simultaneously fit evidence to hypotheses and fit hypotheses to evidence
in order to obtain their preferred overall result.}

{\centering \includegraphics{images/prereg/grid.png}

}

\end{figure}

But hang on a minute! Isn't it a good thing to seek out interesting
results if they are there in the data? Shouldn't we ``let the data
speak''? The answer is yes! But it's crucial to understand the
distinction between \textbf{exploratory} and \textbf{confirmatory} modes
of research.\sidenote{\footnotesize In practice, an individual study may contain both
  exploratory and confirmatory aspects which is why we describe them as
  different ``modes.''} Confirmation involves making research decisions
\emph{before} you've seen the data whereas exploration involves making
research decisions \emph{after} you've seen data.

The key things to remember about exploratory research are that you need
to (1) be aware of the increased risk of bias arising from
data-dependent decision making and calibrate your confidence in the
results accordingly; (2) be honest with other researchers about your
analysis strategy so they are also aware of the risk of bias and can
calibrate \emph{their} confidence in the outcomes accordingly. In the
next section, we will learn about how preregistration helps us to make
this important distinction between exploratory and confirmation
research.

\hypertarget{reducing-risk-of-bias-increasing-transparency-and-calibrating-confidence-with-preregistration}{%
\section{Reducing risk of bias, increasing transparency, and calibrating
confidence with
preregistration}\label{reducing-risk-of-bias-increasing-transparency-and-calibrating-confidence-with-preregistration}}

You can counter the problem of researcher degrees of freedom and
data-dependent decision-making by making research decisions before you
have seen the data -- like planning your route through the garden of
forking paths before you start your journey
(\protect\hyperlink{ref-wagenmakers2012}{Wagenmakers et al. 2012};
\protect\hyperlink{ref-hardwicke2022}{Hardwicke and Wagenmakers 2022}).
If you stick to the planned route, then you have eliminated the
possibility that your decisions were influenced by the data.

\textbf{Preregistration} is the process of declaring your research
decisions in a public registry before you analyze (and often before you
collect) the data. Preregistration ensures that your research decisions
are data-independent, which reduces risk of bias arising from the issues
described above. Preregistration also transparently conveys to others
what you planned, helping them to determine the risk of bias and
calibrate their confidence in the research results. In other words,
preregistration can dissuade researchers from engaging in questionable
research practices like p-hacking and HARKing, because they can be held
accountable to their original plan, while also providing the context
needed to properly evaluate and interpret research.

\begin{figure}

\sidecaption{\label{fig-continuum}Preregistration clarifies where
research activities fall on the continuum of prespecification. When the
preregistration provides little constraint over researcher degrees of
freedom (i.e., more exploratory research), decisions are more likely to
be data-dependent, and consequently there is a higher risk of bias. When
preregistration provides strong constraint over researcher degrees of
freedom (i.e., more confirmatory research), decisions are less likely to
be data-dependent, and consequently there is a lower risk of bias.
Exploratory research activities are more sensitive to serendipitous
discovery, but also have a higher risk of bias relative to confirmatory
research activities. Preregistration transparently communicates where
particular results are located along the continuum, helping readers to
appropriately calibrate their confidence.}

{\centering \includegraphics{images/prereg/continuum.png}

}

\end{figure}

Preregistration does not require that you specify all research decisions
in advance, only that you are transparent about what was planned, and
what was not planned. This transparency helps to make a distinction
between which aspects of the research were exploratory and which were
confirmatory (Figure~\ref{fig-continuum}). All else being equal, we
should have more confidence in confirmatory results, because there is a
lower risk of bias. Exploratory results have a higher risk of bias, but
they are also more \textbf{sensitive} to serendipitous (unexpected
discoveries. So the confirmatory mode is best suited to testing
hypotheses and the exploratory mode is best suited to generating them.
Therefore, exploratory and confirmatory research are both valuable
activities -- it is just important to differentiate them
(\protect\hyperlink{ref-tukey1980}{Tukey 1980})! Preregistration offers
the best of both worlds by clearly separating one from the other.

In addition to the benefits described above, preregistration may improve
the quality of research by encouraging closer attention to study
planning. We've found that the process of writing a preregistration
really helps facilitate communication between collaborators, and can
catch addressable problems before time and resources are wasted on a
poorly designed study. Detailed advanced planning can also create
opportunities for useful community feedback, particularly in the context
of Registered Reports (see Depth box below), where dedicated peer
reviewers will evaluate your study before it has even begun.

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{preregistration-and-friends-a-toolbox-to-address-researcher-degrees-of-freedom}{%
\section*{Preregistration and friends: A toolbox to address researcher
degrees of
freedom}\label{preregistration-and-friends-a-toolbox-to-address-researcher-degrees-of-freedom}}
\addcontentsline{toc}{section}{Preregistration and friends: A toolbox to
address researcher degrees of freedom}

\markright{Preregistration and friends: A toolbox to address researcher
degrees of freedom}

Several useful tools can be used to complement or extend
preregistration. In general, we would recommend that these tool are
combined with preregistration, rather than used as a replacement because
preregistration provides transparency about the research and planning
process (\protect\hyperlink{ref-hardwicke2022}{Hardwicke and Wagenmakers
2022}). The first two of these are discussed in more detail in the last
section of \textbf{?@sec-models}.

\textbf{Robustness checks}. Robustness checks (also called ``sensitivity
analyses'') assess how different decision choices in the garden of
forking paths affect the eventual pattern of results.

\textbf{Multiverse analyses}. Recently, some researchers have started
running large-scale robustness checks called ``multiverse''
(\protect\hyperlink{ref-steegen2016}{Steegen et al. 2016}) or
``specification curve''
(\protect\hyperlink{ref-simonsohn2020}{Simonsohn, Simmons, and Nelson
2020}) analyses. Some have argued that these large-scale robustness
checks make preregistration redundant; after all, why prespecify a
single path if you can explore them all
(\protect\hyperlink{ref-rubin2020}{Rubin 2020};
\protect\hyperlink{ref-oberauer2019}{Oberauer and Lewandowsky 2019})?
But interpreting the results of a multiverse analysis are not
straightforward; for example, it seems unlikely that all of the decision
choices are equally justifiable
(\protect\hyperlink{ref-giudice2021}{Giudice and Gangestad 2021}).
Furthermore, if multiverse analyses are not preregistered, then they
introduce researcher degrees of freedom, and create an opportunity for
selective reporting, which increases risk of bias.

\textbf{Held-out sample}. One option to benefit from both exploratory
and confirmatory research modes is to split your data into
\textbf{training} and \textbf{test} samples. (The test sample is
commonly called the ``held out'' because it is ``held out'' from the
exploratory process.) You can generate hypotheses in an exploratory mode
in the training sample and use that as the basis to preregister
confirmatory analyses in the hold-out sample. A notable disadvantage of
this strategy is that splitting the data reduces statistical power, but
in cases where data are plentiful -- including in much of machine
learning -- this technique is the gold standard.

\textbf{Masked analysis} (traditionally called ``blind analysis'').
Sometimes problems, such as missing data, attrition, or randomization
failure that you did not anticipate in your preregistered plan can arise
during data collection. How do you diagnose and address these issues
without increasing risk of bias through data-dependent analysis? One
option is masked analysis, which disguises key aspects of the data
related to the results (for example, by shuffling condition labels or
adding noise) while still allowing some degree of data inspection
(\protect\hyperlink{ref-dutilh2019}{Dutilh, Sarafoglou, and Wagenmakers
2019}). After diagnosing a problem, you can adjust your preregistered
plan without increasing risk of bias, because your decisions have not
been influenced by the results.

\textbf{Standard Operating Procedures}. Community norms, perhaps at the
level of your research field or lab, can act as a natural constraint on
researcher degrees of freedom. For example, there may be a generally
accepted approach for handling outliers in your community. You can make
these constraints explicit by writing them down in a Standard Operating
Procedures document - a bit like a living meta-preregistration
(\protect\hyperlink{ref-lin2016}{Lin and Green 2016}). Each time you
preregister an individual study, you can co-register this document
alongside it. Make sure you are clear about which document you will
follow in the event of a mismatch!

\textbf{Open lab notebooks}. Maintaining a lab notebook can be a useful
way to keep a record of your decisions as a research project unfolds.
Preregistration is bit like taking a snapshot of your lab notebook at
the start of the project, when all you have written down is your
research plan. Making your lab notebook publicly available is a great
way to transparently document your research and departures from the
preregistered plan.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{images/prereg/registered-reports.png}

}

\caption{\label{fig-reg-reports}Registered Reports
(\url{https://www.cos.io/initiatives/registered-reports}).}

\end{figure}

\textbf{Registered Reports}. Registered Reports are a type of article
format that embeds preregistration directly into the publication
pipeline , Figure~\ref{fig-reg-reports}. The idea is that you submit
your preregistered protocol to a journal and it is peer reviewed, before
you've even started your study. If the study is approved, the journal
agrees to publish it, regardless of the results. This is a radical
departure from traditional publication models where peer reviewers and
journals evaluate your study \emph{after} its been completed and the
results are known. Because the study is accepted for publication
independently of the results, Registered Reports can offer the benefits
of preregistration with additional protection against publication bias.
They also provide a great opportunity to obtain feedback on your study
design while you can still change it!

\end{tcolorbox}

\hypertarget{how-to-preregister}{%
\section{How to preregister}\label{how-to-preregister}}

High-stakes studies such as medical trials must be preregistered
(\protect\hyperlink{ref-dickersin2012}{Dickersin and Rennie 2012}). In
2005, a large international consortium of medical journals decided that
they would not publish unregistered trials. The discipline of economics
also has strong norms about study registration (see
e.g.~\url{https://www.socialscienceregistry.org}). But preregistration
is actually pretty new to psychology
(\protect\hyperlink{ref-nosek2018}{Nosek et al. 2018}), and there's
still no standard way of doing it -- you're already at the cutting edge!

We recommend using the Open Science Framework (OSF) as your registry.
OSF is one of the most popular registries in psychology and you can do
lots of other useful things on the platform to make your research
transparent, like sharing data, materials, analysis scripts, and
preprints. On the OSF it is possible to ``register'' any file you have
uploaded. When you register a file, it creates a time-stamped, read-only
copy, with a dedicated link. You can add this link to articles reporting
your research.

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.0162}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.9838}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Question
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Data collection. Have any data been collected for this study
already? \\
2 & Hypothesis. What's the main question being asked or hypothesis being
tested in this study? \\
3 & Dependent variable. Describe the key dependent variable(s)
specifying how they will be measured. \\
4 & Conditions. How many and which conditions will participants be
assigned to? \\
5 & Analyses Specify exactly which analyses you will conduct to examine
the main question/hypothesis. \\
6 & Outliers and Exclusions. Describe exactly how outliers will be
defined and handled, and your precise rule(s) for excluding
observations. \\
7 & Sample Size. How many observations will be collected or what will
determine sample size? No need to justify decision, but be precise about
exactly how the number will be determined. \\
8 & Other. Anything else you would like to pre-register? (e.g.,
secondary analyses, variables collected for exploratory purposes,
unusual analyses planned? \\
\end{longtable}

One approach to preregistration is to write a protocol document that
specifies the study rationale, aims or hypotheses, methods, and analysis
plan, and register that document.\sidenote{\footnotesize You can think of a study
  protocol a bit like a research paper without a results and discussion
  section (here's an example from one of our own studies:
  \url{https://osf.io/2cnkq/}).} The OSF also has a collection of
dedicated preregistration templates that you can use if you prefer.
These templates are often tailored to the needs of particular types of
research. For example, there are templates for general quantitative
psychology research (``PRP-QUANT''
\protect\hyperlink{ref-bosnjak2021}{Bosnjak et al. 2022}), cognitive
modelling (\protect\hyperlink{ref-cruwell2021}{Crwell and Evans 2021}),
and secondary data analysis (\protect\hyperlink{ref-akker2019}{Akker et
al. 2019}). The OSF interface may change, but currently
\href{https://help.osf.io/hc/en-us/articles/360019738834-Create-a-Preregistration}{this
guide} provides a set of steps to create a preregistration.

Once you've preregistered your plan, you just go off and run the study
and report the results, right? Well hopefully\ldots{} but things might
not turn out to be that straightforward. It's quite common to forget to
include something in your plan or to have to depart from the plan due to
something unexpected. Preregistration can actually be pretty hard in
practice (\protect\hyperlink{ref-nosek2019}{Nosek et al. 2019})!

Don't worry though - remember that a key goal of preregistration is
transparency to enable others to evaluate and interpret research
results. If you decide to depart from your original plan and conduct
data-dependent analyses, then this decision may increase the risk of
bias. But if you communicate this decision transparently to your
readers, they can appropriately calibrate their confidence in the
results. You may even be able to run both the planned and unplanned
analyses as a robustness check (see Depth box) to evaluate the extent to
which this particular choice impacts the results.

When you report your study, it is important to distinguish between what
was planned and what was not. If you ran a lot of data-dependent
analyses, then it might be worth having separate exploratory and
confirmatory results sections. On the other hand, if you mainly stuck to
your original plan, with only minor departures, then you could include a
table (perhaps in an appendix) that outlines these changes (for example,
see Supplementary Information A of
\href{https://doi.org/10.31222/osf.io/wt5ny}{this article}).

\hypertarget{chapter-summary-preregistration}{%
\section{Chapter summary:
Preregistration}\label{chapter-summary-preregistration}}

We've advocated here for preregistering your study plan. This practice
helps to reduce the risk of bias caused by data-dependent analysis (the
``garden of forking paths'' that we described) and transparently
communicate the risk of bias to other scientists. Importantly,
preregistration is a
``\href{https://www.cos.io/blog/preregistration-plan-not-prison}{plan,
not a prison}'': in most cases preregistered, confirmatory analyses
coexist with exploratory analyses. Both are an important part of good
research -- the key is to disclose which is which!

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  P-hack your way to scientific glory! To get a feel for how
  data-dependent analyses might work in practice, have a play around
  with this app: \url{https://projects.fivethirtyeight.com/p-hacking/}.
  Do you think preregistration would affect your confidence in claims
  made about this dataset?
\item
  Preregister your next experiment! The best way to get started with
  preregistration is to have a go with your next study. Head over to
  \url{https://osf.io/registries/osf/new} and register your study
  protocol or complete one of the templates. What aspects of
  preregistration did you find most difficult and what benefits did it
  bring?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\item
  Nosek, B. A., Ebersole, C. R., DeHaven, A. C., \& Mellor, D. T.
  (2018). The preregistration revolution. \emph{Proceedings of the
  National Academy of Sciences}, \emph{115}, 2600--2606.
  \url{https://doi.org/10.1073/pnas.1708274114}.
\item
  Hardwicke, T. E., \& Wagenmakers, E.-J. (2022). Reducing bias,
  increasing transparency, and calibrating confidence with
  preregistration. \emph{Nature Human Behaviour}.
  \url{https://doi.org/10.31222/osf.io/d7bcu}.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-15}{%
\section*{References}\label{bibliography-15}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-15}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-akker2019}{}}%
Akker, Olmo van den, Sara J. Weston, Lorne Campbell, William J. Chopik,
Rodica I. Damian, Pamela Davis-Kean, Andrew Hall, et al. 2019.
{``Preregistration of Secondary Data Analysis: {A} Template and
Tutorial.''} PsyArXiv. \url{https://psyarxiv.com/hvfmr/}.

\leavevmode\vadjust pre{\hypertarget{ref-bakker2012}{}}%
Bakker, Marjan, Annette van Dijk, and Jelte M. Wicherts. 2012. {``The
Rules of the Game Called Psychological Science.''} \emph{Perspectives on
Psychological Science} 7 (6): 543--54.
\url{https://doi.org/10.1177/1745691612459060}.

\leavevmode\vadjust pre{\hypertarget{ref-barber1976}{}}%
Barber, Theodore Xenophon. 1976. \emph{Pitfalls in {Human} {Research}:
{Ten} {Pivotal} {Points}}. Pergamon General Psychology Series ; v. 67.
New York: Pergamon Press.

\leavevmode\vadjust pre{\hypertarget{ref-bennett2009}{}}%
Bennett, CM, MB Miller, and GL Wolford. 2009. {``Neural Correlates of
Interspecies Perspective Taking in the Post-Mortem {Atlantic} {Salmon}:
An Argument for Multiple Comparisons Correction.''} \emph{NeuroImage},
Organization for {Human} {Brain} {Mapping} 2009 {Annual} {Meeting}, 47
(July): S125. \url{https://doi.org/10.1016/S1053-8119(09)71202-9}.

\leavevmode\vadjust pre{\hypertarget{ref-berkowitz2015}{}}%
Berkowitz, Talia, Marjorie W. Schaeffer, Erin A. Maloney, Lori Peterson,
Courtney Gregor, Susan C. Levine, and Sian L. Beilock. 2015. {``Math at
Home Adds up to Achievement in School.''} \emph{Science} 350 (6257):
196--98. \url{https://doi.org/10.1126/science.aac7427}.

\leavevmode\vadjust pre{\hypertarget{ref-berkowitz2016}{}}%
Berkowitz, Talia, Marjorie W Schaeffer, Christopher S Rozek, Erin A
Maloney, Susan C Levine, and Sian L Beilock. 2016. {``Response to
Comment on {`Math at Home Adds up to Achievement in School'}.''}
\emph{Science} 351 (6278): 1161--61.

\leavevmode\vadjust pre{\hypertarget{ref-bosnjak2021}{}}%
Bosnjak, Michael, Christian Fiebach, David Thomas Mellor, Stefanie
Mueller, Daryl O'Connor, Fred Oswald, and Rose Sokol-Chang. 2022. {``A
Template for Preregistration of Quantitative Research in Psychology:
Report of the Joint Psychological Societies Preregistration Task
Force.''} \emph{American Psychologist} 77 (4): 602--15.
\url{https://doi.org/10.1037/amp0000879}.

\leavevmode\vadjust pre{\hypertarget{ref-chambers2020}{}}%
Chambers, Chris, and Loukia Tzavella. 2020. {``Registered {Reports}:
Past, Present and Future.''} MetaArXiv.
\url{https://doi.org/10.31222/osf.io/43298}.

\leavevmode\vadjust pre{\hypertarget{ref-cruwell2021}{}}%
Crwell, Sophia, and Nathan J. Evans. 2021. {``Preregistration in
Diverse Contexts: A Preregistration Template for the Application of
Cognitive Models.''} \emph{Royal Society Open Science} 8 (10): 210155.
\url{https://doi.org/10.1098/rsos.210155}.

\leavevmode\vadjust pre{\hypertarget{ref-degroot2014}{}}%
de Groot, A. D. 1956/2014. {``The Meaning of {`Significance'} for
Different Types of Research.''} Translated by Eric-Jan Wagenmakers,
Denny Borsboom, Josine Verhagen, Rogier A. Kievit, Marjan Bakker,
Anglique O. J. Cramer, Dora Matzke, Don Mellenbergh, and Han L. J. van
der Maas. \emph{Acta Psychologica} 148 (1956/2014): 188--94.
\url{https://doi.org/10.1016/j.actpsy.2014.02.001}.

\leavevmode\vadjust pre{\hypertarget{ref-dickersin2012}{}}%
Dickersin, Kay, and Drummond Rennie. 2012. {``The Evolution of Trial
Registries and Their Use to Assess the Clinical Trial Enterprise.''}
\emph{JAMA} 307 (17): 1861--64.
\url{https://doi.org/10.1001/jama.2012.4230}.

\leavevmode\vadjust pre{\hypertarget{ref-dutilh2019}{}}%
Dutilh, Gilles, Alexandra Sarafoglou, and Eric-Jan Wagenmakers. 2019.
{``Flexible yet Fair: Blinding Analyses in Experimental Psychology.''}
\emph{Synthese}, August.
https://doi.org/\url{https://doi.org/10.1007/s11229-019-02456-7}.

\leavevmode\vadjust pre{\hypertarget{ref-feynman1974}{}}%
Feynman, Richard P. 1974. {``Cargo {Cult} {Science}.''}
\url{http://calteches.library.caltech.edu/51/2/CargoCult.pdf}.

\leavevmode\vadjust pre{\hypertarget{ref-frank2016}{}}%
Frank, Michael C. 2016. {``Comment on {`Math at Home Adds up to
Achievement in School'}.''} \emph{Science}.

\leavevmode\vadjust pre{\hypertarget{ref-gelman2014}{}}%
Gelman, Andrew, and Eric Loken. 2014. {``The Statistical Crisis in
Science.''} \emph{American Scientist} 102 (6): 460--65.
\url{https://doi.org/10.1511/2014.111.460}.

\leavevmode\vadjust pre{\hypertarget{ref-gilovich1985}{}}%
Gilovich, Thomas, Robert Vallone, and Amos Tversky. 1985. {``The Hot
Hand in Basketball: {On} the Misperception of Random Sequences.''}
\emph{Cognitive Psychology} 17 (3): 295--314.
\url{https://doi.org/10.1016/0010-0285(85)90010-6}.

\leavevmode\vadjust pre{\hypertarget{ref-giner-sorolla2012}{}}%
Giner-Sorolla, Roger. 2012. {``Science or Art? {How} Aesthetic Standards
Grease the Way Through the Publication Bottleneck but Undermine
Science.''} \emph{Perspectives on Psychological Science} 7 (6): 562--71.
\url{https://doi.org/10.1177/1745691612457576}.

\leavevmode\vadjust pre{\hypertarget{ref-giudice2021}{}}%
Giudice, M Del, and SW Gangestad. 2021. {``A Traveler's Guide to the
Multiverse: Promises, Pitfalls, and a Framework for the Evaluation of
Analytic Decisions.''} \emph{Advances in Methods and Practices in
Psychological Science} 4 (1): 1--15.
https://doi.org/\url{https://doi.org/10.1177/2515245920954925}.

\leavevmode\vadjust pre{\hypertarget{ref-good1972}{}}%
Good, I. J. 1972. {``Statistics and {Today}'s {Problems}.''} \emph{The
American Statistician} 26 (3): 11--19.
\url{https://doi.org/10.1080/00031305.1972.10478922}.

\leavevmode\vadjust pre{\hypertarget{ref-hardwicke2022}{}}%
Hardwicke, Tom E, and Eric-Jan Wagenmakers. 2022. {``Reducing Bias,
Increasing Transparency, and Calibrating Confidence with
Preregistration.''} \emph{Nature Human Behaviour}.
\url{https://doi.org/10.31222/osf.io/d7bcu}.

\leavevmode\vadjust pre{\hypertarget{ref-hoekstra2020}{}}%
Hoekstra, Rink, and Simine Vazire. 2020. {``Intellectual Humility Is
Central to Science.''} Preprint. \url{https://osf.io/edh2s}.

\leavevmode\vadjust pre{\hypertarget{ref-ioannidis2005}{}}%
Ioannidis, John P. A. 2005. {``Why Most Published Research Findings Are
False.''} \emph{PLOS Medicine} 2 (8): e124.
\url{https://doi.org/10.1371/journal.pmed.0020124}.

\leavevmode\vadjust pre{\hypertarget{ref-kerr1998}{}}%
Kerr, Norbert L. 1998. {``{HARKing}: {Hypothesizing} {After} the
{Results} Are {Known}.''} \emph{Personality \& Social Psychology Review
(Lawrence Erlbaum Associates)} 2 (3): 196.
\url{https://doi.org/10.1207/s15327957pspr0203_4}.

\leavevmode\vadjust pre{\hypertarget{ref-kunda1990}{}}%
Kunda, Ziva. 1990. {``The Case for Motivated Reasoning.''}
\emph{Psychological Bulletin} 108 (3): 480--98.
\url{https://doi.org/10.1037/0033-2909.108.3.480}.

\leavevmode\vadjust pre{\hypertarget{ref-lin2016}{}}%
Lin, Winston, and Donald P. Green. 2016. {``Standard Operating
Procedures: A Safety Net for Pre-Analysis Plans.''} \emph{PS: Political
Science \& Politics} 49 (03): 495--500.
\url{https://doi.org/10.1017/S1049096516000810}.

\leavevmode\vadjust pre{\hypertarget{ref-nickerson1998}{}}%
Nickerson, Raymond S. 1998. {``Confirmation Bias: A Ubiquitous
Phenomenon in Many Guises.''} \emph{Review of General Psychology} 2 (2):
175--220. \url{https://doi.org/10.1037/1089-2680.2.2.175}.

\leavevmode\vadjust pre{\hypertarget{ref-nosek2019}{}}%
Nosek, Brian A, Emorie D. Beck, Lorne Campbell, Jessica K. Flake, Tom E.
Hardwicke, David T. Mellor, Anna E. van 't Veer, and Simine Vazire.
2019. {``Preregistration Is Hard, and Worthwhile.''} \emph{Trends in
Cognitive Sciences} 23 (10): 815--18.
\url{https://doi.org/10.1016/j.tics.2019.07.009}.

\leavevmode\vadjust pre{\hypertarget{ref-nosek2018}{}}%
Nosek, Brian A, Charles R. Ebersole, Alexander C. DeHaven, and David T.
Mellor. 2018. {``The Preregistration Revolution.''} \emph{Proceedings of
the National Academy of Sciences} 115 (11): 2600--2606.
\url{https://doi.org/10.1073/pnas.1708274114}.

\leavevmode\vadjust pre{\hypertarget{ref-nosek2012}{}}%
Nosek, Brian A, Jeffrey R. Spies, and Matt Motyl. 2012. {``Scientific
{Utopia}: {II}. Restructuring Incentives and Practices to Promote Truth
over Publishability.''} \emph{Perspectives on Psychological Science} 7
(6): 615--31. \url{https://doi.org/10.1177/1745691612459058}.

\leavevmode\vadjust pre{\hypertarget{ref-oboyle2017}{}}%
O'Boyle, Ernest Hugh, George Christopher Banks, and Erik Gonzalez-Mul.
2017. {``The Chrysalis Effect: How Ugly Initial Results Metamorphosize
into Beautiful Articles.''} \emph{Journal of Management} 43 (2):
376--99. \url{https://doi.org/10.1177/0149206314527133}.

\leavevmode\vadjust pre{\hypertarget{ref-oberauer2019}{}}%
Oberauer, Klaus, and Stephan Lewandowsky. 2019. {``Addressing the Theory
Crisis in Psychology.''} \emph{Psychonomic Bulletin \& Review} 26 (5):
1596--1618.

\leavevmode\vadjust pre{\hypertarget{ref-rubin2020}{}}%
Rubin, Mark. 2020. {``Does Preregistration Improve the Credibility of
Research Findings?''} \emph{The Quantitative Methods for Psychology} 16
(4): 15. \url{https://doi.org/10.20982/tqmp.16.4.p376}.

\leavevmode\vadjust pre{\hypertarget{ref-simonsohn2020}{}}%
Simonsohn, Uri, Joseph P Simmons, and Leif D Nelson. 2020.
{``Specification Curve Analysis.''} \emph{Nature Human Behaviour}, July,
1--7. \url{https://doi.org/10.1038/s41562-020-0912-z}.

\leavevmode\vadjust pre{\hypertarget{ref-slovic1977}{}}%
Slovic, Paul, and Baruch Fischhoff. 1977. {``On the Psychology of
Experimental Surprises.''} \emph{Journal of Experimental Psychology:
Human Perception and Performance} 3 (4): 544--51.
\url{https://doi.org/10.1037/0096-1523.3.4.544}.

\leavevmode\vadjust pre{\hypertarget{ref-smaldino2016}{}}%
Smaldino, Paul E, and Richard McElreath. 2016. {``The Natural Selection
of Bad Science.''} \emph{Royal Society Open Science} 3 (9): 160384.
\url{https://doi.org/10.1098/rsos.160384}.

\leavevmode\vadjust pre{\hypertarget{ref-steegen2016}{}}%
Steegen, Sara, Francis Tuerlinckx, Andrew Gelman, and Wolf Vanpaemel.
2016. {``Increasing Transparency Through a Multiverse Analysis.''}
\emph{Perspectives on Psychological Science} 11 (5): 702--12.
\url{https://doi.org/10.1177/1745691616658637}.

\leavevmode\vadjust pre{\hypertarget{ref-tukey1980}{}}%
Tukey, John W. 1980. {``We Need Both Exploratory and Confirmatory.''}
\emph{The American Statistician} 34 (1): 23--25.
\url{https://doi.org/10.2307/2682991}.

\leavevmode\vadjust pre{\hypertarget{ref-veldkamp2017}{}}%
Veldkamp, Coosje L. S., Chris H. J. Hartgerink, Marcel A. L. M. van
Assen, and Jelte M. Wicherts. 2017. {``Who Believes in the Storybook
Image of the Scientist?''} \emph{Accountability in Research} 24 (3):
127--51. \url{https://doi.org/10.1080/08989621.2016.1268922}.

\leavevmode\vadjust pre{\hypertarget{ref-wagenmakers2012}{}}%
Wagenmakers, Eric-Jan, Ruud Wetzels, Denny Borsboom, Han L. J. van der
Maas, and Rogier A. Kievit. 2012. {``An Agenda for Purely Confirmatory
Research.''} \emph{Perspectives on Psychological Science} 7 (6):
632--38. \url{https://doi.org/10.1177/1745691612463078}.

\end{CSLReferences}

\hypertarget{sec-collection}{%
\chapter{Data collection}\label{sec-collection}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Outline key features of informed consent and participant debriefing
\item
  Identify additional protections necessary for working with vulnerable
  populations
\item
  Review best practices for online and in-person data collection
\item
  Implement data integrity checks, manipulation checks, and pilot
  testing
\end{itemize}

\end{tcolorbox}

You have selected your measure and manipulation and planned your sample.
Your preregistration is set. Now it's time to think about the nuts and
bolts of collecting data. Though the details may vary between contexts,
this chapter will describe some general best practices for data
collection.\sidenote{\footnotesize The metaphor of ``collection'' implies to some
  researchers that the data exist independent of the researcher's own
  perspective and actions, so they reject it in favor of the term ``data
  generation.'' Unfortunately, this alternative label doesn't
  distinguish generating data via interactions with participants on the
  one hand and generating data from scratch via statistical simulations
  on the other. We worry that ``data generation'' sounds too much like
  the kinds of fraudulent data generation that we talked about in
  \textbf{?@sec-ethics}, so we have opted to keep the more conventional
  ``data collection'' label.} We organize our discussion of these
practices around two perspectives: the participant and the researcher.

The first section takes the perspective of a participant. We begin by
reviewing the importance of informed consent. A key principle of running
experiments with human participants is that we respect their autonomy,
which includes their right to understand the study and choose whether to
take part. When we neglect the impact of our research on the people we
study, we not only violate regulations governing research, we also
create distrust that undermines the moral basis of scientific research.

In the second section, we begin to shift perspectives, discussing the
choice of online vs.~in-person data collection and how to optimize the
experimental experience for participants in both settings. We then end
by taking the experimenter's perspective more fully, discussing how we
can maximize data quality using pilot testing, manipulation checks, and
attention checks, while still being cognizant of both the integrity of
our statistical inferences and how these changes affect the
participant's experience.

\begin{tcolorbox}[colframe=.blue, title=\faMicroscope \enspace Case study]

\hypertarget{the-rise-of-online-data-collection}{%
\section*{The rise of online data
collection}\label{the-rise-of-online-data-collection}}
\addcontentsline{toc}{section}{The rise of online data collection}

\markright{The rise of online data collection}

Since the rise of experimental psychology laboratories in university
settings during the period after World War 2
(\protect\hyperlink{ref-benjamin2000}{Benjamin 2000}), experiments have
typically been conducted by recruiting participants from what has been
referred to as the ``subject pool.'' This term denotes a group of people
who can be recruited for experiments, typically students from
introductory psychology courses
(\protect\hyperlink{ref-sieber1989}{Sieber and Saks 1989}) who are
required to complete a certain number of experiments as part of their
course work. The ready availability of this convenient population
inevitably led to the massive over-representation of undergraduates in
published psychology research, undermining its generalizability
(\protect\hyperlink{ref-sears1986}{Sears 1986};
\protect\hyperlink{ref-henrich2010}{Henrich, Heine, and Norenzayan
2010}).

Yet over the last couple of decades, there has been a revolution in data
collection. Instead of focusing on university undergraduates,
increasingly, researchers recruit individuals from crowdsourcing
websites like Amazon Mechanical Turk (AMT) and Prolific Academic.
Crowdsourcing services were originally designed to recruit and pay
workers for ad-hoc business tasks like retyping receipts, but they have
also become marketplaces to connect researchers with research
participants who are willing to complete surveys and experimental tasks
for small payments (\protect\hyperlink{ref-litman2017}{Litman, Robinson,
and Abberbock 2017}). As of 2015, more than a third of studies in top
social and personality psychology journals were conducted on
crowdsourcing platforms (another third were still conducted with college
undergraduates) and this proportion is likely continuing to grow
(\protect\hyperlink{ref-anderson2019}{Anderson et al. 2019}).

Initially, many researchers worried that crowdsourced data from online
convenience samples would lead to a decrease in data quality. However,
several studies suggest that data quality from online convenience
samples is typically comparable to in-lab convenience samples
(\protect\hyperlink{ref-mason2012}{Mason and Suri 2012};
\protect\hyperlink{ref-buhrmester2016}{Buhrmester, Kwang, and Gosling
2016}). In one particularly compelling demonstration, a set of online
experiments were used to replicate a group of classic phenomena in
cognitive psychology, with clear successes on every experiment except
those requiring sub-50 millisecond stimulus presentation
(\protect\hyperlink{ref-crump2013}{Crump, McDonnell, and Gureckis
2013}). Further, as we discuss below, researchers have developed a suite
of tools to ensure that online participants understand and comply with
the instructions in complex experimental tasks.

Since these initial successes, however, attention has moved away from
the validity of online experiments to the ethical challenges of engaging
with crowdworkers. In 2020, nearly 130,000 people completed MTurk
studies (\protect\hyperlink{ref-moss2020}{Moss et al. 2020}). Of those,
an estimated 70\% identified as White, 56\% identified as women, and
48\% had an annual household income below \$50,000. A sampling of crowd
work determined that the average wage earned was just \$2.00 per hour,
and less than 5\% of workers were paid at least the federal minimum wage
(\protect\hyperlink{ref-hara2018}{Hara et al. 2018}). Further, many
experimenters routinely withheld payment from workers based on their
performance in experiments. These practices clearly violate ethical
guidelines for research with human participants, but are often
overlooked by institutional review boards who may be unfamiliar with
online recruitment platforms or consider that platforms are offering a
``service'' rather than simply being alternative routes for paying
individuals.

With greater attention to the conditions of workers (e.g.,
\protect\hyperlink{ref-salehi2015}{Salehi et al. 2015}), best practices
for online research have progressed considerably. As we describe below,
working with online populations requires attention to both standard
ethical issues of consent and compensation, as well as new issues around
the ``user experience'' of participating in research. The availability
of online convenience samples can be transformative for the pace of
research, for example by enabling large studies to be run in a single
day rather than over many months. But online participants are vulnerable
in different ways than university convenience samples, and we must take
care to ensure that research online is conducted ethically.

\end{tcolorbox}

\hypertarget{informed-consent-and-debriefing}{%
\section{Informed consent and
debriefing}\label{informed-consent-and-debriefing}}

As we discussed in \textbf{?@sec-ethics}, experimenters must respect the
autonomy of their participants: they must be informed about the risks
and benefits of participation before they agree to participate.
Researchers must also discuss and contextualize the research by
debriefing participants after they have completed the study. Here we
look at the nuts and bolts of each of these processes, ending with
guidance on the special protections that are required to protect the
autonomy of especially vulnerable populations.

\hypertarget{getting-consent}{%
\subsection{Getting consent}\label{getting-consent}}

Experimental participants must give consent. In most regulatory
frameworks, there are clear guidelines about what the process of giving
consent should look like. Typically participants are expected to read
and sign a \textbf{consent form}: a document that explains the goals of
the research and its procedures, describes potential risks and benefits,
and asks for participants' explicit consent to participate voluntarily.
Table~\ref{tbl-consent-requirements} gives the full list of consent form
requirements from the US Office for Human Research Protections, and
Figure~\ref{fig-collection-annotated-consent} shows how these individual
requirements are reflected in a real consent form used in our research.

\footnotesize

\hypertarget{tbl-consent-requirements}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.0231}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.9769}}@{}}
\caption{\label{tbl-consent-requirements}US Office of Human Research
Protections requirements for a consent form (edited for
length).}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Requirement
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Requirement
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & A statement that the study involves research \\
2 & An explanation of the purposes of the research \\
3 & The expected duration of the subject's participation \\
4 & A description of the procedures to be followed \\
5 & Identification of any procedures which are experimental \\
6 & A description of any reasonably foreseeable risks or discomforts to
the subject \\
7 & A description of any benefits to the subject or to others which may
reasonably be expected from the research \\
8 & A disclosure of appropriate alternative procedures or courses of
treatment, if any, that might be advantageous to the subject \\
9 & A statement describing the extent, if any, to which confidentiality
of records identifying the subject will be maintained \\
10 & For research involving more than minimal risk, an explanation as to
whether any compensation or medical treatments are available if injury
occurs \\
11 & An explanation of whom to contact for answers to pertinent
questions about the research and research subjects' rights \\
12 & A statement that participation is voluntary, refusal to participate
will involve no penalty, and that subject may discontinue participation
at any time without penalty \\
\end{longtable}

\normalsize

\begin{figure}

\sidecaption{\label{fig-collection-annotated-consent}Consent form
annotated to show how specific text fulfills the requirements in
Table~\ref{tbl-consent-requirements}. Categories 5, 8, and 10 were not
required for this minimal risk psychology experiment.}

{\centering \includegraphics{images/collection/annotated_consent.png}

}

\end{figure}

These are just samples. Since ethics regulation is almost always managed
at the institutional level, your local ethics board will often provide
guidance on the specific information you should include in the consent
form and they will almost always need to approve the form before you are
allowed to begin recruiting participants.

When providing consent information, researchers should focus on what
someone might think or feel as a result of participating in the study.
Are there any physical or emotional risks associated? What should
someone know about the study that may give them pause about agreeing to
participate in the first place? Our advice is to center the
\emph{participant} in the consent process rather than the research
question. Information about specific research goals can typically be
provided during debriefing.\sidenote{\footnotesize Some experimenters worry that
  informing participants about the study that they are about to
  participate in may influence their behavior in the study via so-called
  ``demand characteristics'', discussed in \textbf{?@sec-design}. But
  the goal of a consent form is not to explain the specific
  psychological construct being manipulated. Instead, a consent form
  typically focuses on the experience of being in the study (for
  example, that a participant would be asked to provide quick verbal
  responses to pictures). This sort of general explanation should not
  create demand characteristics.}

If there are specific pieces of information that about study goals or
procedures that \emph{must} be withheld from participants during
consent, \textbf{deception} of participants may be warranted. Deception
can be approved by ethics boards as long as it poses little risk and is
effectively addressed via more extensive debriefing. But an experimental
protocol that includes deception will likely undergo greater scrutiny
during ethics review, as it must be justified by a specific experimental
need.

During the consent process, researchers should explain to participants
what will be done with their data. Requirement 9 in
Table~\ref{tbl-consent-requirements}) merely asks for a statement about
data confidentiality, but such a statement is a mere minimum. Some
modern consent forms explicitly describe different uses of the data and
ask for consent for each. For example, the form in
Figure~\ref{fig-collection-annotated-consent} asks permission for
showing recordings as part of presentations.\sidenote{\footnotesize Some ethics boards
  will ask for consent for sharing even anonymized data files. As we
  discuss in \textbf{?@sec-management}, fully anonymized data can often
  be shared without explicit consent. You may still choose to ask
  participants' permission, but this practice may lead to an awkward
  situation, for example, a dataset with heterogeneous sharing
  permissions such that most but not all data can be shared publicly.}

\hypertarget{prerequisites-of-consent}{%
\subsection{Prerequisites of consent}\label{prerequisites-of-consent}}

In order to give consent, participants must have the cognitive capacity
to make decisions (competence), understand what they are being asked to
do (comprehension), and know that they have the right to withdraw
consent at any time (voluntariness)
(\protect\hyperlink{ref-kadam2017}{Kadam 2017}).

Typically, we assume competence for adult volunteers in our experiments,
but if we are working with children or other vulnerable populations (see
below), we may need to consider whether they are legally competent to
provide consent. Participants who cannot consent on their own should
still be informed about participation in an experiment and, if possible,
you should still obtain their \textbf{assent} (informal agreement) to
participate. When a person has no legal ability to consent, you must
obtain consent from their legal guardian. But if they do not assent, you
should also respect their decision not to participate -- even if you
previously obtained consent from their guardian.

The second prerequisite is comprehension. It is good practice to discuss
consent forms verbally with participants, especially if the study is
involved and takes place in person. If the study is online, ensure that
participants know how to contact you if they have questions about the
study. The consent form itself must be readable for a broad audience,
meaning care should be taken to use accessible language and clear
formatting. Consider giving participants a copy of the consent form in
advance so they can read at their own pace, think of any questions they
might have, and decide how to proceed without any chance of feeling
coerced (\protect\hyperlink{ref-young1990}{Young, Hooker, and Freeberg
1990}).

Finally, participants must understand that their involvement is
voluntary, meaning that they are under no obligation to be involved in a
study and always have the right to withdraw at any time. Experimenters
should not only state that participation is voluntary, they should also
pay attention to other features of the study environment that might lead
to \textbf{structural coercion}
(\protect\hyperlink{ref-fisher2013}{Fisher 2013}). For example, high
levels of compensation can make it difficult for lower-income
participants to withdraw from research. Similarly, factors like race,
gender, and social class can lead participants to feel discomfort around
discontinuing a study. It is incumbent on experimenters to provide a
comfortable study environment and to avoid such coercive factors
wherever possible.

\hypertarget{debriefing-participants}{%
\subsection{Debriefing participants}\label{debriefing-participants}}

Once a study is completed, researchers should always debrief
participants. A debriefing is composed of sevearl parts: (1) gratitude,
(2) discussion of goals, (3) explanation of deception (if relevant), and
(4) questions and clarification (\protect\hyperlink{ref-allen2017}{Allen
2017}). Together these serve to contextualize the experience for the
participant and to mitigate any potential harms from the study.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Gratitude.} Thank participants for their contribution!
  Sometimes thanks is enough (for a short experiment), but many studies
  also include monetary compensation or course credit. Compensation
  should be commensurate with the amount of time and effort required for
  participation. Compensation structures vary widely from place to
  place; typically local ethics boards will have specific guidelines.
\item
  \textbf{Discussion of goals.} Researchers should share the purpose of
  the research with participants in, aiming for a short and accessible
  statement that avoids technical jargon. Sharing goals is especially
  important when some aspect of the study appears evaluative --
  participants will often be interested in knowing how well they
  performed against their peers. For example, a parent whose child
  completed a word-recognition task may request information about their
  child's performance. It can assuage parents' worries to highlight that
  the goals of the study are about measuring a particular experimental
  effect, not about individual evaluation and ranking.\sidenote{\footnotesize At the
    study's conclusion, you might also consider sharing any findings
    with participants -- many participants appreciate learning about
    research findings that they contributed to, even months or years
    after participation.}
\item
  \textbf{Explanation of deception.} Researchers must reveal any
  deception during debriefing, regardless of how minor the deception
  seems to the researcher. This component of the debriefing process can
  be thought of as ``dehoaxing'' because it is meant to illuminate any
  aspects of the study that were previously misleading or inaccurate
  (\protect\hyperlink{ref-holmes1976}{Holmes 1976}). The goal is both to
  reveal the true intent of the study and to alleviate any potential
  anxiety associated with the deception. Experimenters should make clear
  both where in the study the deception occurred and why the deception
  was necessary for the study's success.
\item
  \textbf{Questions and clarification.} Finally, researchers should
  answer any questions or address any concerns raised by participants.
  Many researchers use this opportunity to ask participants about their
  own ideas about the study goals. This practice not only illuminates
  aspects of the study design that may have been unclear to or hidden
  from participants, it also begins a discussion where both researchers
  and participants can communicate about this joint experience. This
  step is also helpful in identifying negative emotions or feelings
  resulting from the study (\protect\hyperlink{ref-allen2017}{Allen
  2017}). When participants do express negative emotions, researchers
  are responsible for sharing resources participants can use to help
  them.\sidenote{\footnotesize In the case that participants report substantial
    concerns or negative reactions to an experiment -- during debriefing
    or otherwise -- researchers will typically have an obligation to
    report these to their ethics board.}
\end{enumerate}

\hypertarget{special-considerations-for-vulnerable-populations}{%
\subsection{Special considerations for vulnerable
populations}\label{special-considerations-for-vulnerable-populations}}

Regardless of who is participating in research, investigators have an
obligation to protect the rights and well-being of all participants.
Some populations are considered especially \textbf{vulnerable} because
of their decreased agency -- either in general or in the face of
potentially coercive situations. Research with these populations
receives additional regulatory oversight. In this section, we will
consider several vulnerable populations.

\textbf{Children.} Children are some of the most commonly used
vulnerable populations in research because the study of development can
contribute both to children's welfare and to our understanding of the
human mind. In the US, children under the age of 18 may only participate
in research with written consent from a parent or guardian. Unless they
are pre-verbal, children should additionally be asked for their assent.
The risks associated with a research study focusing on children also
must be no greater than minimal unless participants may receive some
direct benefit from participating or participating in the study may
improve a disorder or condition the participant was formally diagnosed
with.

\textbf{People with disabilities.} There are thousands of disabilities
that affect cognition, development, motor ability, communication, and
decision-making with varying degrees of interference, so it is first
important to remember that considerations for this population will be
just as diverse as its members. No laws preclude people with
disabilities from participating in research. However, those with
cognitive disabilities who are unable to make their own decisions may
only participant with written consent from a legal guardian and with
their individual assent (if applicable). Those retaining full cognitive
capacity but who have other disabilities that make it challenging to
participate normally in the study should receive appropriate assistance
to access information about the study, including the risks and benefits
of participation.

\textbf{Incarcerated populations.} Nearly 2.1 million people are
incarcerated in the United States alone
(\protect\hyperlink{ref-gramlich2021}{Gramlich 2021}). Due to early (and
repugnant) use of prisoners as a convenience population that could not
provide consent, the use of prisoners in research has been a key focus
of protective efforts. The US Office for Human Research Protections
(OHRP) supports their involvement in research under very limited
circumstances -- typically when the research specifically focuses on
issues relevant to incarcerated populations
(\protect\hyperlink{ref-ohrp2003}{{``Prisoner Involvement in Research''}
2003}). When researchers propose to study incarcerated individuals, the
local ethics board must reconfigure to include at least one active
prisoner (or someone who can speak from a prisoner's perspective) and
ensure that less than half of the board has any affiliation to the
prison system, public or private. Importantly, researchers must not
suggest or promise that participation will have any bearing on an
individual's prison sentence or parole eligibility, and compensation
must be otherwise commensurate with their contribution.

\textbf{Low-income populations.} Participants with fewer resources may
be more persuaded to participate by monetary incentives, creating a
potentially coercive situation. Researchers should consult with their
local ethics board to conform to local standards for non-coercive
payment.

\textbf{Indigenous populations.} There is a long and negative history of
the involvement of indigenous populations in research without their
consent. In the case that research requires the participation of
indigenous individuals -- because of potential benefits to their
communities, rather than due to convenience -- then community leadership
must be involved to discuss the appropriateness of the research as well
as how the consent process should be structured
(\protect\hyperlink{ref-fitzpatrick2016}{Fitzpatrick et al. 2016}).

\textbf{Crowdworkers.} Ethics boards do not usually consider
crowdworkers on platforms like Amazon Mechanical Turk to be a specific
vulnerable population, but many of the same concerns about diminished
autonomy and greater need for protection still arise (see Depth Box
below). Without platform or ethics board standards, it is up to
individual experimenters to commit to fair pay, which should ideally
match or exceed the applicable minimum wage (e.g., the US federal
minimum wage). Further, in the context of reputation management systems
like those of Amazon Mechanical Turk, participants can be penalized for
withdrawing from an experiment -- once they have their work ``rejected''
by an experimenter, it can be harder for them to find new jobs, causing
serious long-term harm to their ability to earn on the platform.

\hypertarget{designing-the-research-experience}{%
\section{Designing the ``research
experience''}\label{designing-the-research-experience}}

For the majority of psychology experiments, the biggest factor that
governs whether a participant has a positive or negative experience of
an experiment is not its risk profile, since for many psychology
experiments the quantifiable risk to participants is minimal.\sidenote{\footnotesize There
  are of course exceptions, including research with more sensitive
  content. Even in these cases, however, attention to the participant's
  experience can be important for ensuring good scientific outcomes.}
Instead, it is the participants' experience. Did they feel welcome? Did
they understand the instructions? Did the software work as designed? Was
their compensation clearly described and promptly delivered? These
aspects of ``user experience'' are critical both for ensuring that
participants have a good experience in the study (an ethical imperative)
and for gathering good data. An experiment that leaves participants
unhappy typically doesn't satisfy either the ethical or the scientific
goals of research. In this section, we'll discuss how to optimize the
research experience for both in-person and online experiments, as well
as providing some guidance on how to decide between these two
administration contexts.

\hypertarget{ensuring-good-experiences-for-in-lab-participants}{%
\subsection{Ensuring good experiences for in-lab
participants}\label{ensuring-good-experiences-for-in-lab-participants}}

A participant's experience begins even before they arrive at the lab.
Negative experiences with the recruitment process (e.g., unclear consent
forms, poor communication, complicated scheduling) or transit to the lab
(e.g., difficulty navigating or finding parking) can lead to frustrated
participants with a negative view of your research. Anything you can do
to make these experiences smoother and more predicable -- prompt
communication, well-tested directions, reserved parking slots, etc. --
will make your participants happier and increase the quality of your
data.\sidenote{\footnotesize For some reason, the Stanford Psychology Department
  building is notoriously difficult to navigate. This seemingly minor
  issue has resulted in a substantial number of late, frustrated, and
  flustered participants over the years.}

Once a participant enters the lab, every aspect of the interaction with
the experimenter can have an effect on their measured behavior
(\protect\hyperlink{ref-gass2018}{Gass and Seiter 2018})! For example, a
likable and authoritative experimenter who clearly describes the
benefits of participation is following general principles for persuasion
(\protect\hyperlink{ref-cialdini2004}{Cialdini and Goldstein 2004}).
This interaction should lead to better compliance with experimental
instructions, and hence better data, than an interaction with an unclear
or indifferent experimenter.

Any interaction with participants must be scripted and standardized so
that all participants have as similar an experience as possible. A lack
of standardization can result in differential treatment for participants
with different characteristics, which could result in data with greater
variability or even specific sociodemographic biases. An experimenter
that was kinder and more welcoming to one demographic group would be
acting unethically, and they also might find a very different result
than they intended.

Even more importantly, experimenters who interact with participants
should ideally be unaware of the experimental condition each participant
is assigned to. This practice is often called ``blinding'' or
``masking''. Otherwise it is easy for experimenter knowledge to result
in small differences in interaction across conditions, which in turn can
influence participants' behavior, resulting in experimenter expectancy
effects (see \textbf{?@sec-design})! Even if the experimenter must know
a participant's condition assignment -- as is sometimes the case -- this
information should be revealed at the last possible moment to avoid
contamination of other aspects of the experimental session.\sidenote{\footnotesize In
  some experiments, an experimenter delivers a manipulation and hence it
  cannot be masked from them. In such cases, it's common to have two
  experimenters such that one delivers the manipulation and another
  (masked to condition) collects the measurements. This situation often
  comes up with studies of infancy, since stimuli are often delivered
  via an in-person puppet show; at a minimum, behavior should be coded
  by someone other than the puppeteer.}

\hypertarget{ensuring-good-experiences-for-online-participants}{%
\subsection{Ensuring good experiences for online
participants}\label{ensuring-good-experiences-for-online-participants}}

The design challenges for online experiments are very different than for
in-lab experiments. As the experimental procedure is delivered through a
web browser, experimenter variability and potential expectancy effects
are almost completely eliminated. On the other hand, some online
participants do many hours of online tasks a day and many are
multi-tasking in other windows or on other devices. It can be much
harder to induce interest and engagement in your research when your
manipulation is one of dozens the participant has experienced that day
and when your interactions are mediated by a small window on a computer
screen.

When creating an online experimental experience, we consider four
issues: (1) design, (2) communication, (3) payment policies, and (4)
effective consent and debriefing:\sidenote{\footnotesize For extensive further
  guidance on this topic, see Litman and Robinson
  (\protect\hyperlink{ref-litman2020}{2020}).}

\textbf{Basic UX design}. Good experiment design online is a subset of
good web \textbf{user experience} (UX) design more generally. If your
web experiment is unpleasant to interact with, participants will likely
become confused and frustrated. They will either drop out or provide
data that are lower quality. A good interface should be clean and
well-tested and should offer clear places where the participant must
type or click to interact. If a participant presses a key at an
appropriate time, the experiment should offer a response -- otherwise
the participant will likely press it again. If the participant is
uncertain how many trials are left, they may be more likely to drop out
of the experiment so it is also helpful to provide an indication of
their progress. And if they are performing a speeded paradigm, they
should receive practice trials to ensure that they understand the
experiment prior to beginning the critical blocks of trials.

\textbf{Communication}. Many online studies involve almost no direct
contact with participants. When participants do communicate with you it
is very important to be responsive and polite (as it is with in-lab
participants, of course). Unlike the typical undergraduate participant,
the work that a crowdworker is doing for your study may be part of how
they earn their livelihood, and a small issue in the study for you may
feel very important for them. For that reason, rapid resolution of
issues with studies -- typically through appropriate compensation -- is
very important. Crowdworkers often track the reputation of specific labs
and experimenters {[}sometimes through forums or specialized software;
Irani and Silberman (\protect\hyperlink{ref-irani2013}{2013}){]}. A
quick and generous response to an issue will ensure that future
crowdworkers do not avoid your studies.

\textbf{Payment policies}. Unclear or punitive payment policies can have
a major impact on crowdworkers. We strongly recommend \emph{always}
paying workers if they complete your experiment, regardless of result.
This policy is comparable to standard payment policies for in-lab work.
We assume good faith in our participants: if someone comes to the lab,
they are paid for the experiment, even if it turns out that they did not
perform correctly. The major counterargument to this policy is that some
online marketplaces have a population of workers who are looking to
cheat by being non-compliant with the experiment (e.g., entering
gibberish or even using scripts or artificial intelligence tools to
progress quickly through studies). Our recommendation is to address this
issue through the thoughtful use of ``check'' trials (see below) -- not
through punitive non-payment. The easiest way for a participant to
complete your experiment should be by complying with your instructions.

\hypertarget{tbl-online-consent}{}
\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 0\tabcolsep) * \real{1.0000}}@{}}
\caption{\label{tbl-online-consent}Sample online consent statement from
our course.}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
By answering the following questions, you are participating in a study
being performed by cognitive scientists in the Stanford Department of
Psychology. If you have questions about this research, please contact us
at stanfordpsych251@gmail.com. You must be at least 18 years old to
participate. Your participation in this research is voluntary. You may
decline to answer any or all of the following questions. You may decline
further participation, at any time, without adverse consequences. Your
anonymity is assured; the researchers who have requested your
participation will not receive any personal information about you.
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
By answering the following questions, you are participating in a study
being performed by cognitive scientists in the Stanford Department of
Psychology. If you have questions about this research, please contact us
at stanfordpsych251@gmail.com. You must be at least 18 years old to
participate. Your participation in this research is voluntary. You may
decline to answer any or all of the following questions. You may decline
further participation, at any time, without adverse consequences. Your
anonymity is assured; the researchers who have requested your
participation will not receive any personal information about you.
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\end{longtable}

\textbf{Consent and debriefing}. Because online studies are typically
fully automated, participants do not have a chance to interact with
researchers around consent and debriefing. Further, engagement with long
consent forms may be minimal. In our work we have typically relied on
short consent statements such as the one from our class that is shown in
Table~\ref{tbl-online-consent}. Similarly, debriefing often occurs
through a set of pages that summarize all components of the debriefing
process (participation gratitude, discussion of goals, explanation of
deception if relevant, and questions and clarification). Because these
interactions are so short, it is especially important to include contact
information prominently so that participants can follow up.

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{the-rise-of-online-data-collection-1}{%
\section*{The rise of online data
collection}\label{the-rise-of-online-data-collection-1}}
\addcontentsline{toc}{section}{The rise of online data collection}

\markright{The rise of online data collection}

Since the rise of experimental psychology laboratories in university
settings during the period after World War 2
(\protect\hyperlink{ref-benjamin2000}{Benjamin 2000}), experiments have
typically been conducted by recruiting participants from what has been
referred to as the ``subject pool.'' This term denotes a group of people
who can be recruited for experiments, typically students from
introductory psychology courses
(\protect\hyperlink{ref-sieber1989}{Sieber and Saks 1989}) who are
required to complete a certain number of experiments as part of their
course work. The ready availability of this convenient population
inevitably led to a vast over-representation of undergraduates in
published psychology research, undermining its generalizability
(\protect\hyperlink{ref-sears1986}{Sears 1986};
\protect\hyperlink{ref-henrich2010}{Henrich, Heine, and Norenzayan
2010}).

Yet over the last couple of decades, there has been a revolution in data
collection. Instead of focusing on university undergraduates,
increasingly, researchers recruit individuals from crowdsourcing
websites like Amazon Mechanical Turk (AMT) and Prolific Academic.
Crowdsourcing services were originally designed to recruit and pay
workers for ad-hoc business tasks such as retyping receipts, but they
have become marketplaces to connect researchers with research
participants who are willing to complete surveys and experimental tasks
for small payments (\protect\hyperlink{ref-litman2017}{Litman, Robinson,
and Abberbock 2017}). As of 2015, more than a third of studies in top
social and personality psychology journals were conducted on
crowdsourcing platforms (another third were still conducted with college
undergraduates); this proportion has likely continued to grow over the
years since the last systematic surveys were done
(\protect\hyperlink{ref-anderson2019}{Anderson et al. 2019}).

Initially, many researchers worried that crowdsourced data from online
convenience samples would lead to a decrease in data quality. However,
several studies suggest that data quality from online convenience
samples is typically comparable to in-lab convenience samples
(\protect\hyperlink{ref-mason2012}{Mason and Suri 2012};
\protect\hyperlink{ref-buhrmester2016}{Buhrmester, Kwang, and Gosling
2016}). In one particularly compelling demonstration, Crump, McDonnell,
and Gureckis (\protect\hyperlink{ref-crump2013}{2013}) repeated a set of
classic experiments in cognitive psychology using online participants,
successfully replicating all except those requiring sub-50 millisecond
stimulus presentation. Further, as we discuss below, researchers have
developed a suite of tools to ensure that online participants understand
and comply with the instructions in complex experimental tasks.

As the use of online data collection rises, it is important to engage
with the ethical challenges of working with crowdworkers to collect
psychological data. In 2020, nearly 130,000 people completed MTurk
studies (\protect\hyperlink{ref-moss2020}{Moss et al. 2020}). Of those,
an estimated 70\% identified as White, 56\% identified as women, and
48\% had an annual household income below \$50,000. A sampling of crowd
work determined that the average wage earned was just \$2.00 per hour,
and less than 5\% of workers were paid at least the federal minimum wage
(\protect\hyperlink{ref-hara2018}{Hara et al. 2018}). Further, many
experimenters routinely withheld payment from workers based on their
performance in experiments. These practices -- low compensation and base
compensation only contingent on performance -- clearly violate ethical
guidelines for research with human participants, but are often
overlooked by institutional review boards who may be unfamiliar with
online recruitment platforms.

Working with online populations requires attention to both standard
ethical issues of consent and compensation, as well as new issues around
the ``user experience'' of participating in research
(\protect\hyperlink{ref-salehi2015}{Salehi et al. 2015}). The
availability of online convenience samples can be transformative for the
pace of research, for example by enabling large studies to be run in a
single day rather than over many months. But online participants are
vulnerable in different ways than university convenience samples, and we
must take care to ensure that research online is conducted ethically.

\end{tcolorbox}

\hypertarget{when-to-collect-data-online}{%
\subsection{When to collect data
online?}\label{when-to-collect-data-online}}

Online data collection is increasingly ubiquitous in the behavioral
sciences. Further, the web browser -- alongside survey software like
Qualtrics or packages like jsPsych
(\protect\hyperlink{ref-de-leeuw2015}{De Leeuw 2015}) -- can be a major
aid to transparency in sharing experimental materials. Replication and
reuse of experimental materials is vastly simpler if readers and
reviewers can click on a link and share the same experience as a
participant in your experiment. By and large, well-designed studies
yield data that are as reliable as in-lab data {[}see Depth Box above;
Buhrmester, Kwang, and Gosling
(\protect\hyperlink{ref-buhrmester2016}{2016});Mason and Suri
(\protect\hyperlink{ref-mason2012}{2012});Crump, McDonnell, and Gureckis
(\protect\hyperlink{ref-crump2013}{2013}){]}.

Still, online data collection is not right for every experiment. Studies
that have substantial deception or that induce negative emotions may
require an experimenter present to alleviate ethical concerns or provide
detailed debriefing. Beyond ethical issues, we discuss four broader
concerns to consider when deciding whether to conduct data collection
online: (1) population availability, (2) the availability of particular
measures, (3) the feasibility of particular manipulations, and (4) the
length of experiments.

\textbf{Population}. Not every target population can be tested online.
Indeed, initially, convenience samples from Amazon Mechanical Turk were
the only group easily available for online studies. More recently, new
tools have emerged to allow pre-screening of crowd participants,
including sites like Cloud Research and Prolific
(\protect\hyperlink{ref-eyal2021}{Eyal et al. 2021};
\protect\hyperlink{ref-peer2021}{Peer et al. 2021}).\sidenote{\footnotesize These
  tools still have significant weaknesses for accessing
  socio-demographically diverse populations within and outside the US,
  however -- screening tools can remove participants, but if the
  underlying population does not contain many participants from a
  particular demographic, it can be hard to gather large enough samples.
  For an example of using crowdsourcing and social media sites to gather
  diverse participants, see DeMayo et al.
  (\protect\hyperlink{ref-demayo2021}{2021}).} And it may initially have
seemed implausible that children could be recruited online, but during
the COVID-19 pandemic a substantial amount of developmental data
collection moved online, with many studies yielding comparable results
to in-lab studies (e.g., \protect\hyperlink{ref-chuey2021}{Chuey et al.
2021}).\sidenote{\footnotesize Sites like \href{https://lookit.mit.edu}{LookIt} now
  offer sophisticated platforms for hosting studies for children and
  families (\protect\hyperlink{ref-scott2017}{Scott and Schulz 2017}).}
Finally, new, non-US crowdsourcing platforms continue to grow in
popularity, leading to greater global diversity in the available online
populations.

\textbf{Online measures}. Not all measures are available online, though
more and more are. Although online data collection was initially
restricted to the use of survey measures -- including ratings and text
responses -- measurement options have rapidly expanded. The widespread
use of libraries like jsPsych (\protect\hyperlink{ref-de-leeuw2015}{De
Leeuw 2015}) has meant that millisecond accuracy in capturing response
times is now possible within web-browsers; thus, most reaction time
tasks are quite feasible (\protect\hyperlink{ref-crump2013}{Crump,
McDonnell, and Gureckis 2013}). The capture of sound and video is
possible with modern browser frameworks
(\protect\hyperlink{ref-scott2017}{Scott and Schulz 2017}). Further,
even measures like mouse- and eye-tracking are beginning to become
available (\protect\hyperlink{ref-maldonado2019}{Maldonado, Dunbar, and
Chemla 2019}; \protect\hyperlink{ref-slim2021}{Slim and Hartsuiker
2021}). In general, almost any variable that can be measured in the lab
without specialized apparatus can also be collected online. On the other
hand, studies that measure a broader range of physiological variables
(e.g., heart rate or skin conductance) or a larger range of physical
behaviors (e.g., walking speed or pose) are still likely difficult to
implement online.

\textbf{Online manipulations}. Online experiments are limited to the set
of manipulations that can be created within a browser window -- but this
restriction excludes many different manipulations that involve real-time
social interactions with a human being.\sidenote{\footnotesize So-called ``moderated''
  experiments -- in which the experimental session is administered
  through a synchronous video chat have been used widely in online
  experiments for children but these designs are less common in
  experiments with adults because they are expensive and time-consuming
  to administer (\protect\hyperlink{ref-chuey2021}{Chuey et al. 2021}).}
Synchronous chat sessions can be a useful substitute
(\protect\hyperlink{ref-hawkins2020}{Hawkins, Frank, and Goodman 2020}),
but these focus the experiment on the content of what is said and
exclude the broader set of non-verbal cues available to participants in
a live interaction (e.g., gaze, race, appearance, accent, etc.).
Creative experimenters can circumvent these limitations by using
pictures, videos, and other methods. But more broadly, an experimenter
interested in implementing a particular manipulation online should ask
how compelling the online implementation is compared with an in-lab
implementation. If the intention is to induce some psychological state
-- say stress, fear, or disgust -- experimenters must trade off the
greater ease of recruitment and larger scale of online studies with the
more compelling experience they may be able to offer in a controlled lab
context.

\textbf{The length of online studies}. One last concern is about
attention and focus in online studies. Early guidance around online
studies tended to focus on making studies short and easy, with the
rationale that crowdsourcing workers were used to short jobs. Our sense
is that this guidance no longer holds. Increasingly, researchers are
deploying long and complex batteries of tasks to relatively good effect
(e.g., \protect\hyperlink{ref-enkavi2019}{Enkavi et al. 2019}) and
conducting repeated longitudinal sampling protocols (discussed in depth
in \protect\hyperlink{ref-litman2020}{Litman and Robinson 2020}). Rather
than relying on hard and fast rules about study length, a better
approach for online testing is to ensure that participants' experience
is as smooth and compelling as possible. Under these conditions, if an
experiment is viable in the lab, it is likely viable online.

Online testing tools continue to grow and change but they are already
mature enough that using them should be part of most behavioral
researchers' basic toolkit.\sidenote{\footnotesize It is of course import to keep in
  mind that if a person works part- or full-time on a crowdsourcing
  platform, they are not a representative sample of the broader national
  population. Unfortunately, similar caveats hold true for in-person
  convenience samples (see \textbf{?@sec-sampling}). Ultimately,
  researchers must reason about what their generalization goal is and
  whether that goal is consistent with the samples they can access
  (online or otherwise).}

\hypertarget{ensuring-high-quality-data}{%
\section{Ensuring high quality data}\label{ensuring-high-quality-data}}

In the final section of this chapter, we review some key data collection
practices that can help researchers collect high quality data while
respecting our ethical obligations to participants. By ``high quality,''
here we especially mean datasets that are uncontaminated by responses
generated by misunderstanding of instructions, fatigue, incomprehension,
or intentional neglect of the experimental task.

We'll begin by discussing the issue of pilot testing; we recommend a
systematic procedure for piloting that can maximize the chance of
collecting high quality data. Next, we'll discuss the practice of
checking participants' comprehension and attention and what such checks
should and shouldn't be used for. Finally, we'll discuss the importance
of maintaining consistent data collection records.

\hypertarget{conduct-effective-pilot-studies}{%
\subsection{Conduct effective pilot
studies}\label{conduct-effective-pilot-studies}}

A \textbf{pilot study} is a small study conducted before you collect
your main sample. The goal is to ensure smooth and successful data
collection by first checking if your experimental procedures and data
collection workflow are working correctly. Pilot studies are also an
opportunity to get feedback from participants about their experience of
the experimental task, for example, is it too easy, too difficult, or
too boring.

Because pilot studies usually involve a small number of participants,
they are not a reliable indicator of the study results, such as the
expected effect size or statistical significance (as we discussed in
\textbf{?@sec-sampling}). \emph{Don't} use pilots to check if your
effect is present or to estimate an effect size for power analysis.

What pilots \emph{can} do is tell you about whether your experimental
procedure is viable. For example, pilots studies can reveal:

\begin{itemize}
\tightlist
\item
  if your code crashes under certain circumstances
\item
  if your instructions confuse a substantial portion of your
  participants
\item
  if you have a very high dropout rate
\item
  if your data collection procedure fails to log variables of interest,
  or
\item
  if participants are disgruntled by the end of the experiment.
\end{itemize}

We recommend that all experimenters perform -- at the very minimum --
two pilot studies before they launch a new experiment.\sidenote{\footnotesize We mean
  especially when deploying a new experimental paradigm or when
  collecting data from a new population. Once you have run many studies
  with a similar procedure and similar sample, extensive piloting is
  less important. Any time you change something, it's always good to run
  one or two pilots, though, just to check that you didn't inadvertently
  mess up your experiment.}

The first pilot, which we call your \textbf{non-nave participant
pilot}, can make use of participants who know the goals of the
experiment and understand the experimental manipulation -- this could be
a friend, collaborator, colleague, or family member.\sidenote{\footnotesize In a pinch
  you can even run yourself through the experiment a bunch of times
  (though this isn't preferable because you're likely to miss a lot of
  aspects of the experience that you are habituated to, especially if
  you've been debugging the experiment already).} The goal of this pilot
study is to ensure that your experiment is comprehensible, that
participants can complete it, and that the data are logged
appropriately. You must \emph{analyze} the data from the non-naive
pilot, at least to the point of checking that the relevant data about
each trial is logged.

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{data-logging-much}{%
\section*{Data logging much?}\label{data-logging-much}}
\addcontentsline{toc}{section}{Data logging much?}

\markright{Data logging much?}

When Mike was in graduate school, his lab got a contract to test a very
large group of participants in a battery of experiments, bringing them
into the lab over the course of a series of intense bursts of
participant testing. He got the opportunity to add an experiment to the
battery, allowing him to test a much larger sample than resources would
otherwise allow. He quickly coded up a new experiment as part of a
series of ongoing studies and began deploying it, coming to the lab
every weekend for several months to help move participants through the
testing protocol. Eagerly opening up the data file to reap the reward of
this hard work, he found that the condition variable was missing from
the data files. Although the experimental manipulation had been deployed
properly, there was no record of which condition each participant had
been run in, and so the data were essentially worthless. Had he run a
quick pilot (even with non-naive participants) and attempted to analyze
the data, this error would have been detected, and many hours of
participant and experimenter effort would not have been lost.

\end{tcolorbox}

The second pilot, your \textbf{nave participant pilot}, should consist
of a test of a small set of participants recruited via the channel you
plan to use for your main study. The number of participants you should
pilot depends on the cost of the experiment in time, money, and
opportunity as well as its novelty. A brand new paradigm is likely more
prone to error than a tried and tested paradigm. For a short online
survey-style experiment, a pilot of 10--20 people is reasonable. A more
time-consuming laboratory study might require piloting just two or three
people.\sidenote{\footnotesize In the case of especially expensive experiments, it can
  be a dilemma whether to run a larger pilot to identify difficulties
  since such a pilot will be costly. In these cases, one possibility is
  to plan to include the pilot participants in the main dataset if no
  major procedural changes are required. In this case, it is helpful to
  preregister a contingent testing strategy to avoid introducing
  data-dependent bias (see \textbf{?@sec-prereg}). For example, in a
  planned sample of 100 participants, you could preregister running 20
  as a pilot sample with the stipulation that you will look only at
  their dropout rate -- and not at any condition differences. Then the
  preregistration can state that, if the dropout rate is lower than
  25\%, you will collect the next 80 participants and analyze the whole
  dataset, including the initial pilot, but if dropout rate is higher
  than 25\%, you will discard the pilot sample and make changes. This
  kind of strategy can help you split the difference between cautious
  piloting and conservation of rare or costly data.}

The goal of the nave pilot study is to understand properties of the
participant experience. Were participants confused? Did they withdraw
before the study finished? Even a small number of pilots can tell you
that your dropout rate is likely too high: for example, if 5 of 10 pilot
participants withdraw you likely need to reconsider aspects of your
design. It's critical for your nave participant pilot that you debrief
more extensively with your participants. This debriefing often takes the
form of an interview questionnaire after the study is over. ``What did
you think the study was about?'' and ``is there any way we could improve
the experience of being in the study?'' can be helpful questions. Often
this debriefing is more effective if it is interactive, so even if you
are running an online study you may want to find some way to chat with
your participants.

Piloting -- especially piloting with nave participants to optimize the
participant experience -- is typically an iterative process. We
frequently launch an experiment for a naive pilot, then recognize from
the data or from participant feedback that the experience can be
improved. We make tweaks and pilot again. Be careful not to over-fit to
small differences in pilot data, however. Piloting should be more like
workshopping a manuscript to remove typos than doing statistical
analysis. If someone has trouble understanding a particular sentence --
whether in your manuscript or in your experiment instructions -- you
should edit to make it clearer!

\hypertarget{measure-participant-compliance}{%
\subsection{Measure participant
compliance}\label{measure-participant-compliance}}

You've constructed your experiment and piloted it. You are almost ready
to go -- but there is one more family of tricks for helping to achieve
high quality data: integrating measures of participant compliance into
your paradigm. Collecting data on compliance (whether participants
followed the experimental procedures as expected) can help you quantify
whether participants understood your task, engaged with your
manipulation, and paid attention to the full experimental experience.
These measures in turn can be used both to modify your experimental
paradigm and to exclude specific participants that were especially
non-compliant (\protect\hyperlink{ref-hauser2018}{Hauser, Ellsworth, and
Gonzalez 2018}; \protect\hyperlink{ref-ejelov2020}{Ejelv and Luke
2020}).

Below we discuss four types of compliance checks: (1) passive measures,
(2) comprehension checks, (3) manipulation checks, and (4) attention
checks. Passive measures and comprehension checks are very helpful for
enhancing data quality. Manipulation checks also often have a role to
play. In contrast, we typically caution in the use of attention checks.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Passive measures of compliance}. Even if you do not ask
  participants anything extra in an experiment, it is often possible to
  tell if they have engaged with the experimental procedure simply by
  how long it takes them to complete the experiment. If you see
  participants with completion times substantially above or below the
  median, there is a good chance that they are either multi-tasking or
  rushing through the experiment without engaging.\sidenote{\footnotesize Measurements
    of per-page or per-element completion times can be even more
    specific since they can, for example, identify participants that
    simply did not read an assigned passage.} Passive measures cost
  little to implement and should be inserted whenever possible in
  experiments.\sidenote{\footnotesize One variation that we endorse in certain cases
    is to force participants to engage with particular pages for a
    certain amount of time through the use of timers. Though, beware,
    this kind of feature can lead to an adversarial relationship with
    participants -- in the face of this kind of coercion, many will opt
    to pull out their phone and multi-task until the timer runs down.}
\item
  \textbf{Comprehension checks}. For tasks with complex instructions or
  experimental materials (say a passage that must be understood for a
  judgment to be made about it), it can be very helpful to get a signal
  that participants have understood what they have read or viewed.
  Comprehension checks, which ask about the content of the experimental
  instructions or materials, are often included for this purpose. For
  the comprehension of instructions, the best kinds of questions simply
  query the knowledge necessary to succeed in the experiment, for
  example, ``what are you supposed to do when you see a red circle flash
  on the screen?'' In many platforms, it is possible to make
  participants reread the instructions again until they can answer these
  correctly. This kind of repetition is nice because it corrects
  participants' misconceptions rather than allowing them to continue in
  the experiment when they do not understand.\sidenote{\footnotesize If you are
    querying comprehension of experimental materials rather than
    instructions, you may not want to re-expose participants to the same
    passage again in order to avoid confounding a participants' initial
    comprehension and the amount of exposure that they receive.}
\item
  \textbf{Manipulation checks}. If your experiment involves more than a
  very transient manipulation -- for example, if you plan to induce some
  state in participants or have them learn some content -- then you can
  include a measure in your experiment that confirms that your
  manipulation succeeded (\protect\hyperlink{ref-ejelov2020}{Ejelv and
  Luke 2020}). This measure is known as a manipulation check because it
  measures some prerequisite difference between conditions that is not
  the key causal effect of interest but is causally prerequisite to this
  effect. For example, if you want to see if anger affects moral
  judgment, then it makes sense to measure whether participants in your
  anger induction condition rate themselves as angrier than participants
  in your control condition. Manipulation checks are useful in the
  interpretation of experimental findings because they can decouple the
  failure of a manipulation from the failure of a manipulation to affect
  your specific measure of interest.\sidenote{\footnotesize Hauser, Ellsworth, and
    Gonzalez (\protect\hyperlink{ref-hauser2018}{2018}) worry that
    manipulation checks can themselves change the effect of a
    manipulation -- this worry strikes us as sensible, especially for
    some types of manipulations like emotion inductions. Their
    recommendation is to test the efficacy of the manipulation in a
    separate study, rather than trying to nest the manipulation check
    within the main study.}
\item
  \textbf{Attention checks}. A final type of compliance check is a check
  that participants are paying attention to the experiment at all. One
  simple technique is to add questions that have a known and fairly
  obvious right answer (e.g., ``what's the capital of the United
  States.''). These trials can catch participants that are simply
  ignoring all text and ``mashing buttons'', but they will not find
  participants who are mildly inattentive. Sometimes experimenters also
  use trickier compliance checks, such as putting an instruction for
  participants to click a particular answer deep within a question text
  that otherwise would have a different answer
  Figure~\ref{fig-collection-attention-check}. Such compliance checks
  decrease so-called ``satisficing'' behavior, in which participants
  read as quickly as they can get away with (doing only the minimum. On
  the other hand, participants may see such trials as indications that
  the experimenter is trying to trick them, and adopt a more adversarial
  stance towards the experiment, which may result in less compliance
  with other aspects of the design {[}unless they are at the end of the
  experiment; Hauser, Ellsworth, and Gonzalez
  (\protect\hyperlink{ref-hauser2018}{2018}){]}. If you choose to
  include attention checks like these, be aware that you are likely
  reducing variability in your sample -- trading off representativeness
  for compliance.
\end{enumerate}

\begin{figure}

\sidecaption{\label{fig-collection-attention-check}An attention check
trial from Oppenheimer, Meyvis, and Davidenko
(\protect\hyperlink{ref-oppenheimer2009}{2009}). These trials can
decrease variability in participant attention, but at the cost of
selecting a subsample of participants, so they should be used
cautiously.}

{\centering \includegraphics{images/collection/instructional-manip.jpg}

}

\end{figure}

Data from all of these types of checks are used in many different --
often inconsistent -- ways in the literature. We recommend that you:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use passive measures and comprehension checks as pre-registered
  exclusion criteria to eliminate a (hopefully small) group of
  participants who might be non-compliant with your experiment.
\item
  Check that exclusions are low and that they are uniform across
  conditions. If exclusion rates are high, your design may have deeper
  issues. If exclusions are asymmetric across conditions, you may be
  compromising your randomization by creating a situation in which (on
  average) different kinds of participants are included in one condition
  compared with the other. Both of these situations substantially
  compromise any estimate of the causal effect of interest.
\item
  Deploy manipulation checks if you are concerned about whether your
  manipulation effectively induces a difference between groups. Analyze
  the manipulation check separately from the dependent variable to test
  whether the manipulation was causally effective
  (\protect\hyperlink{ref-ejelov2020}{Ejelv and Luke 2020}).
\item
  Make sure that your attention checks are not confounded in any way
  with condition -- remember our cautionary tale from
  \textbf{?@sec-design}, in which an attention check that was different
  across conditions actually created an experimental effect.
\item
  \emph{Do not} include any of these checks in your analytic models as a
  covariate, as including this information in your analysis compromises
  the causal inference from randomization and introduces bias in your
  analysis (\protect\hyperlink{ref-montgomery2018}{Montgomery, Nyhan,
  and Torres 2018}).\sidenote{\footnotesize Including this information means you are
    ``conditioning on a post-treatment variable,'' as we described in
    \textbf{?@sec-models}. In medicine, analysts distinguish
    ``intent-to-treat'' analysis, where you analyze data from everyone
    you gave a drug, and ``as treated'' analysis, where you analyze data
    depending on how much of the drug people actually took. In general,
    intent-to-treat gives you the generalizable causal estimate. In our
    current situation, if you include compliance as a covariate, you are
    essential doing an ``as treated'' analysis and your estimate can be
    biased as a result. Although there is a place for such analyses, in
    general you probably want to avoid these analyses.}
\end{enumerate}

Used appropriately, compliance checks can provide both a useful set of
exclusion criteria and a powerful tool for diagnosing potential issues
with your experiment during data analysis and correcting them down the
road.

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{does-data-quality-vary-throughout-the-semester}{%
\section*{Does data quality vary throughout the
semester?}\label{does-data-quality-vary-throughout-the-semester}}
\addcontentsline{toc}{section}{Does data quality vary throughout the
semester?}

\markright{Does data quality vary throughout the semester?}

Every lab that collects empirical data repeatedly using the same
population builds up lore about how that population varies in different
contexts. Many researchers who conducted experiments with college
undergraduates were taught never to run their studies at the end of the
semester. Exhausted and stressed students would likely yield low-quality
data, or so the argument went. Until the rise of multi-lab collaborative
projects like ManyLabs (see \textbf{?@sec-replication}), such beliefs
were almost impossible to test.

ManyLabs 3 aimed specifically to evaluate data quality variation across
the academic calendar (\protect\hyperlink{ref-ebersole2016}{Ebersole et
al. 2016}). With 2,696 participants at 20 sites, the study conducted
replications of 13 previously published findings. Although only six of
these findings showed strong evidence of replicating across sites, none
of the six effects was substantially moderated by being collected later
in the semester. The biggest effect they observed was a change in the
Stroop effect from \(d=.89\) during the beginning and middle of the
semester to \(d=.92\) at the end. There was some evidence that
participants \emph{reported} being less attentive at the end of the
semester, but this trend wasn't accompanied by a moderation of
experimental effects.

Researchers are subject to the same cognitive illusions and biases as
any human. One of these biases is the search to find meaning in the
random fluctuations they sometimes observe in their experiments. The
intuitions formed through this process can be helpful prompts for
generating hypotheses -- but beware of adopting them into your
``standard operating procedures'' without further examination. Labs that
avoided data collection during the end of the semester might have
sacrificed 10--20\% of their data collection capacity for no reason!

\end{tcolorbox}

\hypertarget{keep-consistent-data-collection-records}{%
\subsection{Keep consistent data collection
records}\label{keep-consistent-data-collection-records}}

As an experimentalist, one of the worst feelings is to come back to your
data directory and see a group of data files, \texttt{run1.csv},
\texttt{run2.csv}, \texttt{run3.csv} and not know what experimental
protocol was run for each. Was \texttt{run1} the pilot? Maybe a little
bit of personal archaeology with timestamps and version history can tell
you the answer, but there is no guarantee.\sidenote{\footnotesize We'll have a lot to
  say about this issue in \textbf{?@sec-management}.}

\begin{figure}

\sidecaption{\label{fig-collection-runsheet}Part of a run sheet for a
developmental study.}

{\centering \includegraphics{images/collection/runsheet.png}

}

\end{figure}

As well as collecting the actual data in whatever form they take (e.g.,
paper surveys, videos, or files on a computer), it is important to log
\textbf{metadata} -- data about your data -- including relevant
information like the date of data collection, the sample that was
collected, the experiment version, the research assistants who were
present, etc. The relevant meta-data will vary substantially from study
to study -- the important part is that you keep detailed records.
Figure~\ref{fig-collection-runsheet} and Figure~\ref{fig-collection-log}
give two examples from our own research. The key feature is that they
provide some persistent metadata about how the experiments were
conducted.

\begin{figure}

\sidecaption{\label{fig-collection-log}Excerpt of a log for an iterative
run of online experiments.}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{images/collection/log.png}

}

\end{figure}

\hypertarget{chapter-summary-data-collection}{%
\section{Chapter summary: Data
collection}\label{chapter-summary-data-collection}}

In this chapter, we took the perspective of both the participant and the
researcher. Our goal was to discuss how to achieve a good research
outcome for both. On the side of the participant, we highlighted the
responsibility of the experimenter to ensure a robust consent and
debriefing process. We also discussed the importance of a good
experimental experience in the lab and online -- ensuring that the
experiment is not only conducted ethically but is also pleasant to
participate in. Finally, we discussed how to address some concerns about
data quality from the researcher perspective, recommending both the
extensive use of non-naive and naive pilot participants and the use of
comprehension and manipulation checks.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  ``Citizen science'' is a movement to have a broader base of
  individuals participate in research because they are interested in
  discoveries and want to help. In practice, citizen science projects in
  psychology like \href{https://implicit.harvard.edu/implicit/}{Project
  Implicit}, \href{https://lookit.mit.edu}{Children Helping Science},
  and \href{https://themusiclab.org}{TheMusicLab.org} have all succeeded
  by offering participants a compelling experience. Check one of these
  out, participate in a study, and make a list the features that make it
  fun and easy to contribute data.
\item
  Be a Turker! Sign up for an account as an Amazon Mechanical Turk
  worker and complete three Human Intelligence Tasks. How did you feel
  about browsing the marketplace looking for work? What features of
  tasks attracted your interest? How hard was it to figure out how to
  participate in each task? And how long did it take to get paid?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\tightlist
\item
  An introduction to online research: Buhrmester, M. D., Talaifar, S.,
  \& Gosling, S. D. (2018). An evaluation of Amazon's Mechanical Turk,
  its rapid rise, and its effective use. Perspectives on Psychological
  Science, 13(2), 149-154.
  \url{https://doi.org/10.1177/1745691617706516}.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-16}{%
\section*{References}\label{bibliography-16}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-16}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-allen2017}{}}%
Allen, Michael. 2017. {``Debriefing of Participants.''} In \emph{The
SAGE Encyclopedia of Communication Research Methods}. Vol. 1--4.
Thousand Oaks, CA: Sage Publications.

\leavevmode\vadjust pre{\hypertarget{ref-anderson2019}{}}%
Anderson, Craig A, Johnie J Allen, Courtney Plante, Adele
Quigley-McBride, Alison Lovett, and Jeffrey N Rokkum. 2019. {``The
MTurkification of Social and Personality Psychology.''}
\emph{Personality and Social Psychology Bulletin} 45 (6): 842--50.

\leavevmode\vadjust pre{\hypertarget{ref-benjamin2000}{}}%
Benjamin, Ludy T. 2000. {``The Psychology Laboratory at the Turn of the
20th Century.''} \emph{American Psychologist} 55 (3): 318.

\leavevmode\vadjust pre{\hypertarget{ref-buhrmester2016}{}}%
Buhrmester, Michael, Tracy Kwang, and Samuel D Gosling. 2016.
{``Amazon's Mechanical Turk: A New Source of Inexpensive, yet
High-Quality Data?''}

\leavevmode\vadjust pre{\hypertarget{ref-chuey2021}{}}%
Chuey, Aaron, Mika Asaba, Sophie Bridgers, Brandon Carrillo, Griffin
Dietz, Teresa Garcia, Julia A Leonard, et al. 2021. {``Moderated Online
Data-Collection for Developmental Research: Methods and Replications.''}
\emph{Frontiers in Psychology}, 4968.

\leavevmode\vadjust pre{\hypertarget{ref-cialdini2004}{}}%
Cialdini, Robert B, and Noah J Goldstein. 2004. {``Social Influence:
Compliance and Conformity.''} \emph{Annual Review of Psychology} 55 (1):
591--621.

\leavevmode\vadjust pre{\hypertarget{ref-crump2013}{}}%
Crump, Matthew J C, John V McDonnell, and Todd M Gureckis. 2013.
{``Evaluating Amazon's Mechanical Turk as a Tool for Experimental
Behavioral Research.''} \emph{PLoS One} 8 (3): e57410.

\leavevmode\vadjust pre{\hypertarget{ref-de-leeuw2015}{}}%
De Leeuw, Joshua R. 2015. {``jsPsych: A JavaScript Library for Creating
Behavioral Experiments in a Web Browser.''} \emph{Behavior Research
Methods} 47 (1): 1--12.

\leavevmode\vadjust pre{\hypertarget{ref-demayo2021}{}}%
DeMayo, Benjamin, Danielle Kellier, Mika Braginsky, Christina Bergmann,
Cielke Hendriks, Caroline F Rowland, Michael Frank, and Virginia
Marchman. 2021. {``Web-CDI: A System for Online Administration of the
MacArthur-Bates Communicative Development Inventories.''} \emph{Language
Development Research}.

\leavevmode\vadjust pre{\hypertarget{ref-ebersole2016}{}}%
Ebersole, Charles R, Olivia E Atherton, Aimee L Belanger, Hayley M
Skulborstad, Jill M Allen, Jonathan B Banks, Erica Baranski, et al.
2016. {``Many Labs 3: Evaluating Participant Pool Quality Across the
Academic Semester via Replication.''} \emph{J. Exp. Soc. Psychol.} 67
(November): 68--82.

\leavevmode\vadjust pre{\hypertarget{ref-ejelov2020}{}}%
Ejelv, Emma, and Timothy J Luke. 2020. {``{`Rarely Safe to Assume'}:
Evaluating the Use and Interpretation of Manipulation Checks in
Experimental Social Psychology.''} \emph{Journal of Experimental Social
Psychology} 87: 103937.

\leavevmode\vadjust pre{\hypertarget{ref-enkavi2019}{}}%
Enkavi, A Zeynep, Ian W Eisenberg, Patrick G Bissett, Gina L Mazza,
David P MacKinnon, Lisa A Marsch, and Russell A Poldrack. 2019.
{``Large-Scale Analysis of Test--Retest Reliabilities of Self-Regulation
Measures.''} \emph{Proceedings of the National Academy of Sciences} 116
(12): 5472--77.

\leavevmode\vadjust pre{\hypertarget{ref-eyal2021}{}}%
Eyal, Peer, Rothschild David, Gordon Andrew, Evernden Zak, and Damer
Ekaterina. 2021. {``Data Quality of Platforms and Panels for Online
Behavioral Research.''} \emph{Behavior Research Methods}, 1--20.

\leavevmode\vadjust pre{\hypertarget{ref-fisher2013}{}}%
Fisher, Jill A. 2013. {``Expanding the Frame of" Voluntariness" in
Informed Consent: Structural Coercion and the Power of Social and
Economic Context.''} \emph{Kennedy Institute of Ethics Journal} 23 (4):
355--79.

\leavevmode\vadjust pre{\hypertarget{ref-fitzpatrick2016}{}}%
Fitzpatrick, Emily FM, Alexandra LC Martiniuk, Heather D'Antoine, June
Oscar, Maureen Carter, and Elizabeth J Elliott. 2016. {``Seeking Consent
for Research with Indigenous Communities: A Systematic Review.''}
\emph{BMC Medical Ethics} 17 (1): 1--18.

\leavevmode\vadjust pre{\hypertarget{ref-gass2018}{}}%
Gass, Robert H, and John S Seiter. 2018. \emph{Persuasion: Social
Influence and Compliance Gaining}. Routledge.

\leavevmode\vadjust pre{\hypertarget{ref-gramlich2021}{}}%
Gramlich, John. 2021. {``America's Incarceration Rate Falls to Lowest
Level Since 1995.''}
\url{https://www.pewresearch.org/fact-tank/2021/08/16/americas-incarceration-rate-lowest-since-1995/}.

\leavevmode\vadjust pre{\hypertarget{ref-hara2018}{}}%
Hara, Kotaro, Abigail Adams, Kristy Milland, Saiph Savage, Chris
Callison-Burch, and Jeffrey P Bigham. 2018. {``A Data-Driven Analysis of
Workers' Earnings on Amazon Mechanical Turk.''} In \emph{Proceedings of
the 2018 CHI Conference on Human Factors in Computing Systems}, 1--14.

\leavevmode\vadjust pre{\hypertarget{ref-hauser2018}{}}%
Hauser, David J, Phoebe C Ellsworth, and Richard Gonzalez. 2018. {``Are
Manipulation Checks Necessary?''} \emph{Frontiers in Psychology} 9: 998.

\leavevmode\vadjust pre{\hypertarget{ref-hawkins2020}{}}%
Hawkins, Robert D, Michael C Frank, and Noah D Goodman. 2020.
{``Characterizing the Dynamics of Learning in Repeated Reference
Games.''} \emph{Cognitive Science} 44 (6): e12845.

\leavevmode\vadjust pre{\hypertarget{ref-henrich2010}{}}%
Henrich, Joseph, Steven J Heine, and Ara Norenzayan. 2010. {``The
Weirdest People in the World?''} \emph{Behavioral and Brain Sciences} 33
(2-3): 61--83.

\leavevmode\vadjust pre{\hypertarget{ref-holmes1976}{}}%
Holmes, David S. 1976. {``Debriefing After Psychological Experiments: I.
Effectiveness of Postdeception Dehoaxing.''} \emph{American
Psychologist} 31 (12): 858.

\leavevmode\vadjust pre{\hypertarget{ref-irani2013}{}}%
Irani, Lilly C, and M Six Silberman. 2013. {``Turkopticon: Interrupting
Worker Invisibility in Amazon Mechanical Turk.''} In \emph{Proceedings
of the SIGCHI Conference on Human Factors in Computing Systems},
611--20.

\leavevmode\vadjust pre{\hypertarget{ref-kadam2017}{}}%
Kadam, Rashmi Ashish. 2017. {``Informed Consent Process: A Step Further
Towards Making It Meaningful!''} \emph{Perspectives in Clinical
Research} 8 (3): 107.

\leavevmode\vadjust pre{\hypertarget{ref-litman2020}{}}%
Litman, Leib, and Jonathan Robinson. 2020. \emph{Conducting Online
Research on Amazon Mechanical Turk and Beyond}. Sage Publications.

\leavevmode\vadjust pre{\hypertarget{ref-litman2017}{}}%
Litman, Leib, Jonathan Robinson, and Tzvi Abberbock. 2017. {``TurkPrime.
Com: A Versatile Crowdsourcing Data Acquisition Platform for the
Behavioral Sciences.''} \emph{Behavior Research Methods} 49 (2):
433--42.

\leavevmode\vadjust pre{\hypertarget{ref-maldonado2019}{}}%
Maldonado, Mora, Ewan Dunbar, and Emmanuel Chemla. 2019. {``Mouse
Tracking as a Window into Decision Making.''} \emph{Behavior Research
Methods} 51 (3): 1085--1101.

\leavevmode\vadjust pre{\hypertarget{ref-mason2012}{}}%
Mason, Winter, and Siddharth Suri. 2012. {``Conducting Behavioral
Research on Amazon's Mechanical Turk.''} \emph{Behavior Research
Methods} 44 (1): 1--23.

\leavevmode\vadjust pre{\hypertarget{ref-montgomery2018}{}}%
Montgomery, Jacob M, Brendan Nyhan, and Michelle Torres. 2018. {``How
Conditioning on Posttreatment Variables Can Ruin Your Experiment and
What to Do about It.''} \emph{Am. J. Pol. Sci.} 62 (3): 760--75.

\leavevmode\vadjust pre{\hypertarget{ref-moss2020}{}}%
Moss, Aaron J, Cheskie Rosenzweig, Jonathan Robinson, and Leib Litman.
2020. {``Demographic Stability on Mechanical Turk Despite COVID-19.''}
\emph{Trends in Cognitive Sciences} 24 (9): 678--80.

\leavevmode\vadjust pre{\hypertarget{ref-oppenheimer2009}{}}%
Oppenheimer, Daniel M, Tom Meyvis, and Nicolas Davidenko. 2009.
{``Instructional Manipulation Checks: Detecting Satisficing to Increase
Statistical Power.''} \emph{Journal of Experimental Social Psychology}
45 (4): 867--72.

\leavevmode\vadjust pre{\hypertarget{ref-peer2021}{}}%
Peer, Eyal, David M Rothschild, Zak Evernden, Andrew Gordon, and
Ekaterina Damer. 2021. {``MTurk, Prolific or Panels? Choosing the Right
Audience for Online Research.''} \emph{Choosing the Right Audience for
Online Research (January 10, 2021)}.

\leavevmode\vadjust pre{\hypertarget{ref-ohrp2003}{}}%
{``Prisoner Involvement in Research.''} 2003.
\url{https://www.hhs.gov/ohrp/regulations-and-policy/guidance/prisoner-research-ohrp-guidance-2003/index.html}.

\leavevmode\vadjust pre{\hypertarget{ref-salehi2015}{}}%
Salehi, Niloufar, Lilly C Irani, Michael S Bernstein, Ali Alkhatib, Eva
Ogbe, and Kristy Milland. 2015. {``We Are Dynamo: Overcoming Stalling
and Friction in Collective Action for Crowd Workers.''} In
\emph{Proceedings of the 33rd Annual ACM Conference on Human Factors in
Computing Systems}, 1621--30.

\leavevmode\vadjust pre{\hypertarget{ref-scott2017}{}}%
Scott, Kimberly, and Laura Schulz. 2017. {``Lookit (Part 1): A New
Online Platform for Developmental Research.''} \emph{Open Mind} 1 (1):
4--14.

\leavevmode\vadjust pre{\hypertarget{ref-sears1986}{}}%
Sears, David O. 1986. {``College Sophomores in the Laboratory:
Influences of a Narrow Data Base on Social Psychology's View of Human
Nature.''} \emph{Journal of Personality and Social Psychology} 51 (3):
515.

\leavevmode\vadjust pre{\hypertarget{ref-sieber1989}{}}%
Sieber, Joan E, and Michael J Saks. 1989. {``A Census of Subject Pool
Characteristics and Policies.''} \emph{American Psychologist} 44 (7):
1053.

\leavevmode\vadjust pre{\hypertarget{ref-slim2021}{}}%
Slim, Mieke Sarah, and Robert Hartsuiker. 2021. {``Visual World
Eyetracking Using WebGazer. Js.''}

\leavevmode\vadjust pre{\hypertarget{ref-young1990}{}}%
Young, Daniel R, Donald T Hooker, and Fred E Freeberg. 1990. {``Informed
Consent Documents: Increasing Comprehension by Reducing Reading
Level.''} \emph{IRB: Ethics \& Human Research} 12 (3): 1--5.

\end{CSLReferences}

\hypertarget{sec-management}{%
\chapter{Project management}\label{sec-management}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Manage your research projects efficiently and transparently
\item
  Develop strategies for data organization
\item
  Optimize sharing of research products, like data and analysis code, by
  ensuring they are Findable, Accessible, Interoperable, Reusable (FAIR)
\item
  Discuss potential ethical constraints on sharing research products
\end{itemize}

\end{tcolorbox}

\begin{quote}
Your closest collaborator is you six months ago, but you don't reply to
emails.

--- Karl Broman (2016)
\end{quote}

\begin{marginfigure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{images/management/versions_xkcd.png}

}

\caption{\label{fig-versions}Poor file management creates chaos! By xkcd
(https://xkcd.com/1459).}

\end{marginfigure}

Have you ever returned to an old project folder to find a chaotic mess
of files with names like \texttt{analysis-FINAL},
\texttt{analysis-FINAL-COPY}, and \texttt{analysis-FINAL-COPY-v2}? Which
file is actually the final version!? Or perhaps you've spent hours
searching for a data file to send to your advisor, only to realize with
horror that it was \emph{only} stored on your old laptop -- the one that
experienced a catastrophic hard drive failure when you spilled coffee
all over it one sleepy Sunday morning. These experiences may make you
sympathetic to Karl Broman's quip above. Good project management
practices not only make it easier to share your research with others,
they also make for a more efficient and less error prone workflow that
will avoid giving your future self a headache. This chapter is about the
process of managing all of the products of your research workflow --
methodological protocols, materials\sidenote{\footnotesize We use the term
  ``materials'' here to cover a range of things another researcher might
  need in order to repeat your study, for example, stimuli, survey
  instruments, and code for computer-based experiments.}, data, and
analysis scripts -- in ways that maximize their value to you and to the
broader research community.

When we talk about research products, we typically think of articles in
academic journals, which have been scientists' main method of
communication since the scientific revolution in the 1600s.\sidenote{\footnotesize The
  world's oldest scientific journal is the \emph{Philosophical
  Transactions of the Royal Society}, first published in 1665.} But
articles only provide written summaries of research; they are not the
original research products. In recent years, there have been widespread
calls for increased sharing of research products, such as materials,
data, and analysis code (\protect\hyperlink{ref-munafo2017}{Munaf et
al. 2017}). When shared appropriately, these other products can be as
valuable as a summary article: Shared stimulus materials can be reused
for new studies in creative ways; shared analysis scripts can allow for
reproduction of reported results and become templates for new analyses;
and shared data can enable new analyses or meta-analyses. Indeed, many
funding agencies, and some journals, now require that research products
be shared publicly, except when there are justified ethical or legal
constraints, such as with sensitive medical data
(\protect\hyperlink{ref-nosek2015}{Nosek et al. 2015}).

Data sharing, in particular, has been the focus of intense interest.
Sharing data is associated with benefits in terms of error detection
(\protect\hyperlink{ref-hardwicke2021d}{Hardwicke et al. 2021}),
creative re-use that generates new discoveries
(\protect\hyperlink{ref-voytek2016}{Voytek 2016}), increased citations
(\protect\hyperlink{ref-piwowar2013}{Piwowar and Vision 2013}), and
detection of fraud (\protect\hyperlink{ref-simonsohn2013}{Simonsohn
2013}). According to surveys, researchers are usually willing to share
data in principle (\protect\hyperlink{ref-houtkoop2018}{Houtkoop et al.
2018}), but unfortunately, in practice, they often do not, even if you
directly ask them (\protect\hyperlink{ref-hardwicke2018c}{Hardwicke and
Ioannidis 2018})! Often authors simply do not respond, but when they do,
they frequently report that data have been lost because they were stored
on a misplaced or damaged computer or drive, or team members with access
to the data are no longer contactable
(\protect\hyperlink{ref-tenopir2020}{Tenopir et al. 2020}).

As we have discussed in \textbf{?@sec-replication}, even when data are
shared, they are not always formatted in a way that they can be easily
understood and re-used by other researchers, or even the original
authors! This issue highlights the critical role of \textbf{metadata}:
information that documents the data (and other products) that you share,
including README files, \textbf{codebooks} that document datasets
themselves, licenses that provide legal restrictions on reuse, etc. We
will discuss best-practices for metadata throughout the chapter.

\begin{marginfigure}

{\centering \includegraphics{images/management/chain.png}

}

\caption{\label{fig-management-chain}Illustration of the analytic chain
from raw data through to research report.}

\end{marginfigure}

Sound project management practices and sharing of research projects are
mutually reinforcing goals that bring benefits for both yourself, the
broader research community, and scientific progress. One particularly
important benefit of good project management practices is that they
enable reproducibility. As we discussed in \textbf{?@sec-replication},
computational reproducibility involves being able to trace the
provenance of any reported analytic result in a research report back to
its original source. That means being able to recreate the entire
analytic chain from data collection to data files, though analytic
specifications to the research results reported in text, tables, and
figures. If data collection is documented appropriately, and if data are
stored, organized, and shared, then the provenance of a particular
result is relatively easy to verify. But once this chain is broken it
can be hard to reconstruct , Figure~\ref{fig-management-chain}. That's
why it's critical to build good project management practices into your
research workflow right from the start.

In this chapter, you will learn how to manage your research project both
efficiently and transparently. Working towards these goals can create a
virtuous cycle: if you organize your research products well, they are
easier to share later, and if you assume that you will be sharing, you
will be motivated to organize your work better! We begin by discussing
some important principles of project management, including folder
structure, file naming, organization, and version control. Then we zoom
in specifically on data and discuss best practices for data sharing. We
end by discussing the question of what research products to share and
some of the potential ethical issues that might limit your ability to
share in certain circumstances.\sidenote{\footnotesize This chapter -- especially the
  last section -- draws heavily on Klein et al.
  (\protect\hyperlink{ref-klein2018}{2018}), an article on research
  transparency that several of us contributed to.}

\begin{tcolorbox}[colframe=.blue, title=\faMicroscope \enspace Case study]

\hypertarget{manybabies-manyspreadsheetformats}{%
\section*{ManyBabies,
ManySpreadsheetFormats!}\label{manybabies-manyspreadsheetformats}}
\addcontentsline{toc}{section}{ManyBabies, ManySpreadsheetFormats!}

\markright{ManyBabies, ManySpreadsheetFormats!}

The ManyBabies project is an example of ``Big Team Science'' in
psychology. A group of developmental psychology researchers (including
some of us) were worried about many of the issues of reproducibility,
replicability, and experimental methods that we've been discussing
throughout this book, so they set up a large-scale collaboration to
replicate key effects in developmental science. The first of these
studies was ManyBabies 1 (\protect\hyperlink{ref-manybabies2020}{The
ManyBabies Consortium et al. 2020}), a study of infants' preference for
baby-talk (also known as ``infant directed speech'').

The core team expected a handful of labs to contribute, but after a
year-long data collection period, they ended up receiving data from 69
labs around the world! The outpouring of interest signaled a lot of
enthusiasm from the community for this kind of collaborative science.
Unfortunately, it also made for a tremendous data management headache.
All kinds of complications and hilarity ensued as the idiosyncratic data
formatting preferences of the various labs were reorganised to fit into
a single standardized analysis pipeline
(\protect\hyperlink{ref-byers-heinlein2020}{Byers-Heinlein et al.
2020}).

All of the specific formatting changes that individual labs made were
reasonable -- altering column names for clarity, combining templates
into a single Excel file, changing units (e.g., from seconds to
milliseconds) -- but together they created a very challenging
\textbf{data validation} problem for the core analysis team, requiring
many dozens of hours of coding and hand-checking. The data checking was
critical: an error in one lab's data was flagged during validation and
led to the painful decision to drop those data from the final dataset.
In future ManyBabies projects, the group has committed to using data
validation software to ensure that data files uploaded by individual
labs conforms to a shared standard.

\end{tcolorbox}

\hypertarget{principles-of-project-management}{%
\section{Principles of project
management}\label{principles-of-project-management}}

A lot of project management problems can be avoided by following a very
simple file organisation system.\sidenote{\footnotesize We're going to talk in this
  chapter about managing research products, which is one important part
  of project management. We won't talk about some other aspects of
  managing projects such as calendaring, managing tasks, or project
  communications. These are all important, they are just a bit out of
  scope for a book on doing experiments!} For those researchers that
``grew up'' managing their files locally on their own computers and
emailing colleagues versions of data files and manuscripts with names
like \texttt{manuscript-FINAL-JS-rev1.xlsx}, a few aspects of this
system may seem disconcerting. However, with a little practice, this new
way of working will start to feel intuitive and have substantial
benefits.

Here are the principles:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  There should be exactly one definitive copy of each document in the
  project, with its name denoting what it is. For example,
  \texttt{fifo\_manuscript.Rmd} or \texttt{fifo\_manuscript.docx} is the
  write-up of the ``fifo'' project as a journal manuscript.
\item
  The location of each document should be within a folder which serves
  to uniquely identify the document's function within the project. For
  example,
  \texttt{/analysis/experiment1/eye\_tracking\_preprocessing.Rmd} is
  clearly the file that performs pre-processing for the analysis of
  eye-tracking data from Experiment 1.
\item
  The full project should be accessible to all collaborators via the
  cloud, either using a version control platform (e.g.,
  \href{}{github.com}) or another cloud storage provider (e.g., Dropbox,
  Google Drive).
\item
  The revision history of all text- and text-based documents (minimally,
  data, analysis code, and manuscript files) should be archived
  automatically. Automatic versioning is the key feature of all version
  control systems and is often included by cloud storage providers.
\end{enumerate}

Keeping these principles in mind, we discuss best practices for project
organization, version control, and file naming.

\hypertarget{organizing-your-project}{%
\subsection{Organizing your project}\label{organizing-your-project}}

To the greatest extent possible, all files related to a project should
be stored in the same project folder (with appropriate sub-folders), and
on the same storage provider.\sidenote{\footnotesize There are cases where this is
  impractical due to the limitations of different software packages. For
  example, in many cases a team will manage its data and analysis code
  via github but decide to write collaboratively using google docs,
  overleaf, or another collaborative platform. (It can also be hard to
  ask all collaborators to use a version control system they are
  unfamiliar with.) In that case, the final paper should still be linked
  in some way to the project repository. The biggest issue that comes up
  in using a split workflow like this is the need to ensure reproducible
  written products, a process we cover in \textbf{?@sec-writing}.}

Figure~\ref{fig-management-organization-ex} shows an example project
stored on the Open Science Framework. The top level folder contains
sub-folders for analyses, materials, raw and processed data (kept
separately). It also contains the paper manuscript, and, critically, a
README file in a text format that describes the project. A README is a
great way to document any other metadata that the authors would like to
be associated with the research products, for example a license,
explained below.

\begin{figure}

\sidecaption{\label{fig-management-organization-ex}Sample top level
folder structure for a project. From Klein et al.
(\protect\hyperlink{ref-klein2018}{2018}). Original visible on the
\href{https://osf.io/xf6ug/}{Open Science Framework}.}

{\centering \includegraphics{images/management/org-ex.png}

}

\end{figure}

There are many reasonable ways to organize the sub-folders of a research
project, but the broad categories of materials, data, analysis, and
writing are typically present.\sidenote{\footnotesize We like the scheme followed by
  \href{https://www.projecttier.org/}{Project TIER}, which provides very
  clear guidance about file structure and naming conventions. TIER is
  primarily designed for a copy-and-paste workflow, which is slightly
  different from the ``dynamic documents'' workflow that we primarily
  advocate for (e.g., using R Markdown as in \textbf{?@sec-rmarkdown}).}
In some projects -- such as those involving multiple experiments or
complex data types -- you may have to adopt a more complex structure. In
many of our projects, it's not uncommon to find paths like
\texttt{/data/raw\_data/exp1/demographics}. The key principle is to
create a hierarchical structure in which subfolders uniquely identify
the part of the broader space of research products that are found inside
them -- that is, \texttt{/data/raw\_data/exp1} contains all the raw data
from Experiment 1, and \texttt{/data/raw\_data/exp1/demographics}
contains all the raw \emph{demographics} data from that particular
experiment.

\hypertarget{versioning}{%
\subsection{Versioning}\label{versioning}}

Probably everyone who has ever collaborated electronically has
experienced the frustration of editing a document, only to find out that
you are editing the wrong version -- perhaps some of the problems you
are working on have already been corrected, or perhaps the section you
are adding has already been written by someone else. A second common
source of frustration comes when you take a wrong turn in a project,
perhaps by reorganizing a manuscript in a way that doesn't work or
refactoring code in a way that turns out to be short-sighted.

These two problems are solved by modern version control systems. Here we
focus on the use of \textbf{git}, which is the most widely used version
control system. Git is a great general solution for version control, but
many people -- including several of us -- don't love it for
collaborative manuscript writing. We'll introduce git and its principles
here, while noting that online collaboration tools like Google Docs and
Overleaf\sidenote{\footnotesize Overleaf is actually supported by git on the back-end!}
can be easier for writing prose (as opposed to code); we cover this
topic in a bit more depth in \textbf{?@sec-writing}.

\begin{marginfigure}

{\centering \includegraphics{images/management/git2.png}

}

\caption{\label{fig-management-git}isualisation of Git version control
showing a series of commits (circles) on three different branches: the
main branch (green) and two others (blue and red). Branches can be
created and then merged back into the main branch.}

\end{marginfigure}

Git is a tool for creating and managing projects, which are called
\textbf{repositories}. A Git repository is a directory whose revision
history is tracked via a series of \textbf{commits} -- snapshots of the
state of the project. These commits can form a tree with different
\textbf{branches}, as when two contributors to the project are working
on two different parts simultaneously (Figure~\ref{fig-management-git}).
These branches can later be \textbf{merged} either automatically or via
manual intervention in the case of conflicting changes.

Commonly, Git repositories are hosted by an online service like
\href{https://github.com}{Github} to facilitate collaboration. With this
workflow. a user makes changes to a local version of the repository on
their own computer and \textbf{pushes} those changes to the online
repository. Another user can then \textbf{pull} those changes from the
online repository to their own local version. The online ``origin'' copy
is always the definitive copy of the project and a record is kept of all
changes. \textbf{?@sec-git} provides a practical introduction to Git and
Github, and there are a variety of good tutorials available online and
in print (\protect\hyperlink{ref-blischak2016}{Blischak, Davenport, and
Wilson 2016}).

Collaboration using version control tools is designed to solve many of
the problems we've been discussing:

\begin{itemize}
\tightlist
\item
  A remotely hosted Git repository is a cloud-based backup of your work,
  meaning it is less vulnerable to accidental erasure.\sidenote{\footnotesize In 48BC,
    Julius Caesar accidentally burned down part of the Great Library of
    Alexandria where the sole copies of many valuable ancient works were
    stored. To this day, many scientists have apparently retained the
    habit of storing single copies of important information in
    vulnerable locations. Even in the age of cloud computing, hard drive
    failure is a surprisingly common source of problems!}
\item
  By virtue of having versioning history, you have access to previous
  drafts in case you find you have been following a blind alley and want
  to roll back your changes.
\item
  By creating new branches, you can create another, parallel history for
  your project, so that you can try out major changes or additions
  without disturbing the main branch in the process.
\item
  A project's commit history is labeled with each commit's author and
  date, facilitating record keeping and collaboration.
\item
  Automatic merging can allow synchronous editing of different parts of
  a manuscript or codebase.\sidenote{\footnotesize Version control isn't magic, and if
    you and a collaborator edit the same paragraph or function, you will
    likely have to merge your changes by hand. But Git will at least
    show you where the conflict is!}
\end{itemize}

Organizing a project repository for collaboration and hosting on a
remote platform is an important first step towards sharing! Many of our
projects (like this book) are actually \textbf{born open}: we do all of
our work on a publicly hosted repository for everyone to see
(\protect\hyperlink{ref-rouder2015}{Rouder 2015}). This philosophy of
``working in the open'' encourages good organization practices from the
beginning. It can feel uncomfortable at first, but this discomfort soon
vanishes as you realize that basically no one is looking at your
in-progress project.\sidenote{\footnotesize One concern that many people raise about
  sharing in-progress research openly is the possibility of ``scooping''
  -- that is, other researchers getting an idea or even data from the
  repository and writing a paper before you do. We have two responses to
  this concern. First, the empirical frequency of this sort of scooping
  is difficult to determine, but likely very low -- we don't know of any
  documented cases. Mostly, the problem is getting people to care about
  your experiment at all, not people caring so much that they would
  publish using your data or materials! In Gary King's
  \href{https://www.youtube.com/watch?v=jD6CcFxRelY}{words}, ``The thing
  that matters the least is being scooped. The thing that matters the
  most is being ignored.'' On the other hand, if you are in an area of
  research that you perceive to be competitive, or where there is some
  significant risk of this kind of shenanigans, it's very easy to keep
  part, or all, of a repository, private among your collaborators until
  you are ready to share more widely. All of the benefits we described
  still accrue. For an appropriately organized and hosted project, often
  the only steps required to share materials, data, and code are 1) to
  make the hosted repository public and 2) to link it to an archival
  storage platform like the Open Science Framework.}

\hypertarget{file-names}{%
\subsection{File names}\label{file-names}}

As \href{https://www.karlton.org/2017/12/naming-things-hard/}{Phil
Karlton reportedly said}, ``There are only two hard things in Computer
Science: cache invalidation and naming things.'' What's true for
computer science is true for research in general.\sidenote{\footnotesize We won't talk
  about cache invalidation; that's a more technical problem in computer
  science that is beyond the scope of this book.} Naming files is hard!
Some very organized people survive on systems like
\texttt{INFO-r1-draft-2020-07-13-js.docx} - meaning, ``the INFO project
revision 1 draft of July 13th, 2020, with edits by JS.'' But this kind
of system needs a lot of rules and discipline, and it requires everyone
in a project to buy in completely.

On the other hand, if you are naming a file in a hierarchically
organized version control repository, the naming problem gets
dramatically easier. All of a sudden, you have a context in which names
make sense. \texttt{data.csv} is a terrible name for a data file on its
own. But the name is actually perfectly informative -- in the context of
a project repository with a README that states that there is only a
single experiment, a repository structure such that the file lives in a
folder called \texttt{raw\_data}, and a commit history that indicates
the file's commit date and author.

As this example shows, naming is hard \emph{out of context}. So here's
our rule: name a file with what it contains. Don't use the name to
convey the context of who edited it, when, or where it should go in a
project. That is metadata that the platform should take care
of.\sidenote{\footnotesize The platform won't take care of it if you email it to a
  collaborator -- precisely why you should share access to the full
  \emph{platform}, not just the out-of-context file!}

\hypertarget{data-management}{%
\section{Data Management}\label{data-management}}

We've just discussed how to manage projects in general; in this section
we zoom in on datasets specifically. Data are often the most valuable
research product because they represent the evidence generated by our
research. We maximize the value of the evidence when other scientists
can reuse it for independent verification or generation of novel
discoveries. Yet lots of research data are not reusable, even when they
are shared. In \textbf{?@sec-replication}, we discussed Hardwicke et al.
(\protect\hyperlink{ref-hardwicke2018b}{2018})'s study of
\emph{analytic} reproducibility. But before we were even able to try and
reproduce the analytic results, we had to look at the data. When we did
that, we found that only 64\% of shared datasets were both complete and
understandable.

How can you make sure that your data are managed so as to enable
effective sharing? We make four primary recommendations:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  save your raw data,
\item
  document your data collection process,
\item
  organize your raw data for later analysis, and
\item
  document your data using a codebook or other appropriate metadata.
\end{enumerate}

\noindent Let's look at each in turn.

\hypertarget{save-your-raw-data}{%
\subsection{Save your raw data}\label{save-your-raw-data}}

Raw data take many forms. For many of us, the raw data are those
returned by the experimental software; for others, the raw data are
videos of the experiment being carried out. Regardless of the form of
these data, save them! They are often the only way to check issues in
whatever processing pipeline brings these data from their initial state
to the form you analyze. They also can be invaluable for addressing
critiques or questions about your methods or results later in the
process. If you need to correct something about your raw data, \emph{do
not alter the original files}. Make a copy, and make a note about how
the copy differs from the original.\sidenote{\footnotesize Future you will thank
  present you for explaining why there are two copies of subject 19's
  data after you went back and corrected a typo.}

Raw data are often not anonymized -- or even anonymizable. Anonymizing
them sometimes means altering them (e.g., in the case of downloaded logs
from a service that might include IDs or IP addresses). Or in some
cases, anonymization is difficult or impossible without significant
effort and loss of some value from the data, e.g.~for video data or MRI
data (\protect\hyperlink{ref-bischoff-grethe2007}{Bischoff-Grethe et al.
2007}). Unless you have specific permission for broad distribution of
these identifiable data, the raw data may then need to be stored in a
different way. In these cases, we recommend saving your raw data in a
separate repository with the appropriate permissions. For example, in
the ManyBabies 1 study we described above, the public repository does
not contain the raw data contributed by participating labs, which the
team could not guarantee was anonymized; these data are instead stored
in a private repository.\sidenote{\footnotesize The precise repository you use for
  this task is likely to vary by the kind of data that you're trying to
  store and the local regulatory environment. For example, in the United
  States, to store de-anonymized data with certain fields requires a
  server that is certified for HIPAA (the relevant privacy law). Many --
  but by no means all -- universities provide HIPAA-compliant cloud
  storage.}

You can use your repository's README to describe what is and is not
shared. For example, a README might state that ``We provide anonymized
versions of the files originally downloaded from Qualtrics'' or
``Participants did not provide permission for public distribution of raw
video recordings, which are retained on a secure university server.''
Critically, if you share the derived tabular data, it should still be
possible to reproduce the analytic results in your paper, even if
checking the provenance of those numbers from the raw data is not
possible for every reader.\sidenote{\footnotesize One way we organize the raw data in
  some of our paper is to have three different subfolders in the
  \texttt{data/} directory: \texttt{raw/}, for the original data;
  \texttt{processed/}, for the anonymized or otherwise pre-processed
  data; and \texttt{/scripts}, for the code that does the preprocessing.
  Since these folders are in a git repository, we can then add
  \texttt{raw/*} to the \texttt{.gitignore} file, ensuring that they are
  never added to the public version of the repository even though they
  sit within our local file hierarchy in the appropriate place.}

\begin{figure}

\sidecaption{\label{fig-management-mb-datafiles}Example participant
(top) and trial (bottom) level data from the ManyBabies (2020) case
study.}

{\centering \includegraphics{images/management/mb-combined.png}

}

\end{figure}

One common practice is the use of participant identifiers to link
specific experimental data -- which, if they are responses on
standardized measures, rarely pose a significant identifiability risk --
to demographic data sheets that might include more sensitive and
potentially identifiable data.\sidenote{\footnotesize A word about subject
  identifiers. These should be anonymous identifiers, like randomly
  generated numbers, that cannot be linked to participant identities
  (like data of birth) and are unique. You laugh, but one of us was in a
  lab where all the subject IDs were the date of test and the initials
  of the participant. These were neither unique nor anonymous. One
  common convention is to give your study a code-name and to number
  participants sequentially, so your first participant in a sequence of
  experiments on information processing might be \texttt{INFO-1-01}.}
Depending on the nature of the analyses being reported, the experimental
data can then be shared with limited risk. Then a selected set of
demographic variables -- for example, those that do not increase privacy
risks but are necessary for particular analyses -- can be distributed as
a separate file and joined back into the data later.

\hypertarget{document-your-data-collection-process}{%
\subsection{Document your data collection
process}\label{document-your-data-collection-process}}

In order to understand the meaning of the raw data, it's helpful to
share as much as possible about the context in which they were
collected. This practice also helps communicate the experience that
participants had in your experiment. Documentation of this experience
can take many forms.

If the experimental experience was a web-based questionnaire, archiving
this experience can be as simple as downloading the questionnaire
source.\sidenote{\footnotesize If it's in a proprietary format like a Qualtrics
  \texttt{.QSF} file, a good practice is to convert it to a simple plain
  text format as well so it can be opened and re-used by folks who do
  not have access to Qualtrics (which may include future you!).} For
more involved studies, it can be more difficult to reconstruct what
participants went through. This kind of situation is where video data
can shine (\protect\hyperlink{ref-gilmore2017}{Gilmore and Adolph
2017}). A video recording of a typical experimental session can provide
a valuable tutorial for other experimenters -- as well as good context
for readers of your paper. This is doubly true if there is a substantial
interactive element to your experimental experience, as is often the
case for experiments with children. For example, in our ManyBabies case
study, the project shared
\href{https://nyu.databrary.org/volume/896}{``walk through'' videos of
experimental sessions} for many of the participating labs, creating a
repository of standard experiences for infant development studies. If
nothing else, a video of an experimental session can sometimes be a very
nice archive of a particular context.\sidenote{\footnotesize Videos of experimental
  sessions also are great demos to show in a presentation about your
  experiment, provided you have permission from the participant.}

Regardless of what specific documentation you keep, it's critical to
create some record linking your data to the documentation. For a
questionnaire study, for example, this documentation might be as simple
as a README that says that the data in the \texttt{data/raw/} directory
were collected on a particular date using the file named
\texttt{experiment1.qsf}. This kind of ``connective tissue'' linking
data to materials can be very important when you return to a project
with questions. If you spot a potential error in your data, you will
want to be able to examine the precise version of the materials that you
used to gather those data in order to identify the source of the
problem.

\hypertarget{organize-your-data-for-later-analysis-spreadsheets}{%
\subsection{Organize your data for later analysis:
Spreadsheets}\label{organize-your-data-for-later-analysis-spreadsheets}}

Data come in many forms, but chances are that at some point during your
project you will end up with a spreadsheet full of information.
Well-organized spreadsheets cam mean the difference between project
success and failure! A wonderful article by Broman and Woo
(\protect\hyperlink{ref-broman2018}{2018}) lays out principles of good
spreadsheet design. We highlight some of their principles here (with our
own, opinionated ordering):

\begin{figure}

\sidecaption{\label{fig-management-broman-nonrect}Examples of
non-rectangular spreadsheet formats that are likely to cause problems in
analysis. From Broman and Woo
(\protect\hyperlink{ref-broman2018}{2018}).}

{\centering \includegraphics{images/management/broman2018.png}

}

\end{figure}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \emph{Make it a rectangle}.\sidenote{\footnotesize Think of your data like a
    well-ordered plate of sushi, neatly packed together without any
    gaps.} Nearly all data analysis software, like SPSS, Stata, Jamovi
  and JASP (and many R packages), require data to be in a tabular
  format.\sidenote{\footnotesize Tabular data is a precursor to ``tidy'' data, which
    we describe in more detail in Appendix \textbf{?@sec-tidyverse}.} If
  you are used to analyzing data exclusively in a spreadsheet, this kind
  of tabular data isn't quite as readable, but readable formatting gets
  in the way of almost any analysis you want to do.
  Figure~\ref{fig-management-broman-nonrect} gives some examples of
  non-rectangular spreadsheets. All of these will cause any analytic
  package to choke because of inconsistencies in how rows and columns
  are used!
\item
  \emph{Choose good names for your variables}. No one convention for
  name formatting is best, but it's important to be consistent. We tend
  to follow the \href{https://style.tidyverse.org}{tidyverse style
  guide} and use lowercase words separated by underscores (\texttt{\_}).
  It's also helpful to give units where these are available, e.g., are
  reaction times in seconds or milliseconds. Table~\ref{tbl-broman-ex}
  gives some examples of good and bad variable names.
\end{enumerate}

\hypertarget{tbl-broman-ex}{}
\begin{longtable}[]{@{}lll@{}}
\caption{\label{tbl-broman-ex}Examples of good and bad variable names.
Adapted from Broman and Woo
(\protect\hyperlink{ref-broman2018}{2018}).}\tabularnewline
\toprule\noalign{}
Good name & Good alternative & Avoid \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
Good name & Good alternative & Avoid \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
subject\_id & SubID & subject \# \\
sex & female & M/F \\
rt\_msec & reaction\_time\_ms & reaction time (millisec.) \\
\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  \emph{Be consistent with your cell formatting}. Each column should
  have one \emph{kind} of thing in it. For example, if you have a column
  of numerical values, don't all of a sudden introduce text data like
  ``missing'' into one of the cells. This kind of mixing of data types
  can cause havoc down the road. Mixed or multiple entries also don't
  work, so don't write ``0 (missing)'' as the value of a cell. Leaving
  cells blank is also risky because it is ambiguous. Most software
  packages have a standard value for missing data (e.g.~\texttt{NA} is
  what R uses). If you are writing dates, please be sure to use the
  ``global standard'' (ISO 8601), which is YYYY-MM-DD. Anything else can
  be misinterpreted easily.\sidenote{\footnotesize Dates in Excel deserve special
    mention as a source of terribleness. Excel has an unfortunate habit
    of interpreting information that has nothing to do with dates as
    dates, destroying the original content in the process. Excel's issue
    with dates has caused unending horror in the genetics literature,
    where gene names are automatically converted to dates, sometimes
    without the researchers noticing
    (\protect\hyperlink{ref-ziemann2016}{Ziemann, Eren, and El-Osta
    2016}). In fact, some gene names have had to be changed in order to
    avoid this issue!}
\item
  \emph{Decoration isn't data}. Decorating your data with bold headings
  or highlighting may seem useful for humans, but it isn't uniformly
  interpreted or even recognized by analysis software (e.g., reading an
  Excel spreadsheet into R will scrub all your beautiful highlighting
  and artistic fonts) so do not rely on it.
\item
  \emph{Save data in plain text files}. The CSV (comma-delimited) file
  format is a common standard for data that is uniformly understood by
  most analysis software (it is an ``interoperable'' file
  format).\sidenote{\footnotesize Be aware of some interesting differences in how
    these files are output by European vs.~American versions of
    Microsoft Excel! You might find semi-colons instead of commas in
    some datasets.} The advantage of CSVs is that they are not
  proprietary to Microsoft or another tech company and can be inspected
  in a text editor, but be careful: they do not preserve Excel formulas
  or formatting!
\end{enumerate}

Given the points above, we recommend that you avoid analyzing your data
in Excel. If it is necessary to analyze your data in a spreadsheet
program, we urge you to save the raw data as a separate CSV and then
create a distinct analysis spreadsheet so as to be sure to retain the
raw data unaltered by your (or Excel's) manipulations.

\hypertarget{organize-your-data-for-later-analysis-software}{%
\subsection{Organize your data for later analysis:
Software}\label{organize-your-data-for-later-analysis-software}}

Many researchers do not create data by manually entering information
into a spreadsheet. Instead they receive data as the output from a web
platform, software package, or device. These tools typically provide
researchers limited control over the format of the resulting tabular
data export. Case in point is the survey platform Qualtrics, which -- at
least at hte moment -- provides data with not one but two header rows,
complicating import into almost all analysis software!\sidenote{\footnotesize The R
  package \texttt{qualtRics} can help with this.}

That said, if your platform \emph{does} allow you to control what comes
out, you can try to use the principles of good tabular data design
outlined above. For example, try to give your variables (e.g., questions
in Qualtrics) sensible names!

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{bad-variable-naming-can-lead-to-analytic-errors}{%
\section*{Bad variable naming can lead to analytic
errors!}\label{bad-variable-naming-can-lead-to-analytic-errors}}
\addcontentsline{toc}{section}{Bad variable naming can lead to analytic
errors!}

\markright{Bad variable naming can lead to analytic errors!}

In our methods class, students often try to reproduce the original
analyses from a published study before attempting to replicate the
results in a new sample of participants. When Kengthsagn Louis looked at
the code for the study she was interested in, she noticed that the
variables in the analysis code were named horribly (presumably because
they were output this way by the survey software). For example, one
piece of Stata code looked like this:

\begin{verbatim}
gen recall1=.
replace recall1=0 if Q21==1 
replace recall1=1 if Q21==3 | Q21==5 | Q21==6
replace recall1=2 if Q21==2 | Q21==4 | Q21==7 | Q21==8
replace recall1=0 if Q69==1 
replace recall1=1 if Q69==3 | Q69==5 | Q69==6
replace recall1=2 if Q69==2 | Q69==4 | Q69==7 | Q69==8
ta recall1
\end{verbatim}

In the process of translating this code into R in order to reproduce the
analyses, Kengthsagn and a course teaching assistant, Andrew Lampinen,
noticed that some participant responses had been assigned to the wrong
variables. Because the variable names were not human-readable, this
error was almost impossible to detect. Since the problem affected some
of the inferential conclusions of the article, the article's author --
to their credit -- issued an immediate correction
(\protect\hyperlink{ref-petersen2019}{Petersen 2019}).

The moral of the story: Obscure variable names can hide existing errors
and create opportunities for further error! Sometimes you can adjust
these within your experimental software, avoiding the issue. If not,
make sure to create a ``key'' and translate the names immediately,
double checking after you are done.

\end{tcolorbox}

\hypertarget{document-the-format-of-your-data}{%
\subsection{Document the format of your
data}\label{document-the-format-of-your-data}}

Even the best-organized tabular data are not always easy to understand
by other researchers, or even yourself, especially after some time has
passed. For that reason, you should make a \textbf{codebook} (also known
as a \textbf{data dictionary}) that explicitly documents what each
variable is. Figure~\ref{fig-management-mb-codebook} shows an example
codebook for the trial-level data in the bottom of
Figure~\ref{fig-management-mb-datafiles}. Each row represents one
variable in the associated dataset. Codebooks often describe what type
of variable a column is (e.g., numeric, string), and what values can
appear in that column. A human-readable explanation is often given as
well, providing providing units (e.g., ``seconds'') and a translation of
numeric codes (e.g., ``test condition is coded as 1'') where relevant.

\begin{figure}

\sidecaption{\label{fig-management-mb-codebook}Codebook for trial-level
data (see above) from the ManyBabies (2020) case study.}

{\centering \includegraphics{images/management/mb1-codebook.png}

}

\end{figure}

Creating a codebook need not require a lot of work. Almost any
documentation is better than nothing! There are also several R packages
that can automatically generate a codebook for you, for example
\texttt{codebook}, \texttt{dataspice}, and \texttt{dataMaid}
(\protect\hyperlink{ref-arslan2019}{Arslan 2019}). Adding a codebook can
substantially increase the reuse value of the data and prevent hours of
frustration as future you and others try to decode your variable names
and assumptions.

\hypertarget{sharing-research-products}{%
\section{Sharing Research Products}\label{sharing-research-products}}

As we've been discussing throughout this chapter, if you've managed your
research products effectively, sharing them with others is a far less
daunting prospect, and usually just requires uploading them to an online
repository like the Open Science Framework. This section addresses some
potential limitations on sharing that you should bear in mind and
discusses where and how to share research products.

\hypertarget{what-you-can-and-cant-share}{%
\subsection{What you can and can't
share}\label{what-you-can-and-cant-share}}

We've been advocating that you share all of your research products,
especially your data. In practice, however, \textbf{participant privacy}
(as well as a few other constraints) limits what you can share. Luckily,
there are some concrete steps you can take to make sure that you protect
participants and comply with your obligations while still realizing the
benefits of data sharing.

Unless they explicitly waive their rights, participants in psychology
experiments have the expectation of privacy -- that is, no one should be
able to identify them from the data they have provided. Protecting
participant privacy is an important part of researchers' ethical
responsibilities (\protect\hyperlink{ref-ross2018}{Ross, Iguchi, and
Panicker 2018}), and needs to be balanced against the ethical
imperatives to share (see \textbf{?@sec-ethics}).\sidenote{\footnotesize Meyer
  (\protect\hyperlink{ref-meyer2018}{2018}) gives an excellent overview
  of how to navigate various legal and ethical issues around data
  sharing in the US context.}

Furthermore, there are legal regulations that protect participants'
data, though these vary from country to country. In the US, the relevant
regulation is \textbf{HIPAA}, the Health Insurance Portability and
Accountability Act, which limits disclosures of private health
information (\textbf{PHI}). In the European Union, the relevant
regulation is the European \textbf{GDPR} (General Data Protection
Regulation). It's beyond the scope of this book to give a full treatment
of these regulatory frameworks; you should consult with your local IRB
regarding compliance, but here is the way we have navigated this
situation while still sharing data.

Under both frameworks, \textbf{anonymization} (or equivalently
\textbf{de-identification}) of data is a key concept, such that data
sharing is generally just fine if the data meet the relevant standard.
Under US guidelines, researchers can follow the ``safe harbor''
standard\sidenote{\footnotesize \href{https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html}{As
  described on the relevant DHHS page}.} under which data are considered
to be anonymized if they do not contain identifiers like names,
telephone numbers, email addresses, social security numbers, dates of
birth, faces, etc. Thus, data that only contain participant IDs and
nothing from this list can typically be shared without participant
consent without a problem.\sidenote{\footnotesize US IRBs are a very de-centralized
  bunch and their interpretations often vary considerably. For reasons
  of liability or ethics, they may not allow data sharing even though it
  is permitted by US law. If you feel like arguing with an IRB that
  takes this kind of stand, you could mention that the DHHS rule
  actually doesn't consider de-identified data to be ``human subjects''
  data at all, and thus the IRB may not have regulatory authority over
  it. We're not lawyers, and we're not sure if you'll succeed but it
  could be worth a try.}

The EU's GDPR also allows fully anonymized data sharing, with one big
complication. Putting anonymous identifiers in a data file and removing
identifiable fields does not itself suffice for GDPR anonymization if
the data are still \textbf{in-principle re-identifiable} because you
have maintained documentation linking IDs to identifiable data like
names or email addresses. Only when the key linking identifiers to data
has been destroyed are the data truly de-identified according to this
standard.

De-identification is not always enough. As datasets get richer,
\textbf{statistical reidentification risks} go up substantially such
that, with a little bit of outside information, data can be matched with
a unique individual. These risks are especially high with linguistic,
physiological, and geospatial data, but they can be present even for
simple behavioral experiments. In one influential demonstration, knowing
a person's location on two occasions was often enough to identify their
data uniquely in a huge database of credit card transactions
(\protect\hyperlink{ref-de-montjoye2015}{De Montjoye et al.
2015}).\sidenote{\footnotesize For an example closer to home, many of the contributing
  labs in the ManyBabies project logged the date of test for each
  participant. This useful and seemingly innocuous piece of information
  is unlikely to identify any particular participant -- but alongside a
  social media post about a lab visit or a dataset about travel records,
  it could easily reveal a particular participant's identity.} Thus,
simply removing fields from the data is a good starting point -- but if
you are collecting richer data about participants' behavior you may need
to consult an expert.

Privacy issues are ubiquitous in data sharing, and almost every
experimental research project will need to solve them before sharing
data. For simple projects, often these are the only issues that preclude
data sharing. However, in more complex projects, other concerns can
arise. Funders may have specific mandates regarding where your data
should be shared. Data use agreements or collaborator preferences may
restrict where and when you can share. And certain data types require
much more sensitivity since they are more consequential than, say, the
reaction times on a Stroop task. We include here a set of questions to
walk through to plan your sharing
(Figure~\ref{fig-management-sharing-chart}). When in doubt, it's often a
good idea to consult with the relevant local authority, e.g.~your IRB
for ethical issues or your research management office for regulatory
issues.

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{really-anonymous}{%
\section*{Really anonymous?}\label{really-anonymous}}
\addcontentsline{toc}{section}{Really anonymous?}

\markright{Really anonymous?}

When we first began teaching Psych 251, our experimental methods course
at Stanford, one of the biggest contributions of the course was simply
showing students how to do experiments online. Amazon's Mechanical Turk
crowdsourcing service was relatively new, and our IRB did not have a
good sense of what this service really was. We proposed that we would
share data from the class and received approval for this practice. Our
datasets were downloaded directly from Mechanical Turk and included
participants' MTurk IDs (long alphanumeric strings that seemed
completely anonymous). Several experiences caused us to reconsider this
practice!

First, we discovered that MTurk IDs were in some cases linked to study
participants' public Amazon ``wish lists,'' which could both
inadvertently provide information about the participant and also even
potentially provide a basis for reidentification (in rare cases). This
discovery led us to consult with our IRB and provide more explicit
consent language in our class experiments, linking to instructions for
making Amazon profiles private.

Then, a little later we received an irate email from an MTurk
participant who had discovered their data on github via a search for
their MTurk ID. Although they were not identified in this dataset, it
convinced us that at least some participants would not like this ID
shared. After another consultation with the IRB, we apologized to this
individual and removed their and others' IDs from our github commit
histories across that and other repositories. Prior to posting data, we
now take care to anonymize IDs by creating a secret mapping between the
IDs we post and the actual MTurk IDs.

\end{tcolorbox}

\begin{figure}

\sidecaption{\label{fig-management-sharing-chart}A decision chart for
thinking about sharing research products. Adapted from Klein et al.
(\protect\hyperlink{ref-klein2018}{2018}).}

{\centering \includegraphics{images/management/kline2.png}

}

\end{figure}

\hypertarget{where-and-how-to-share-the-fair-principles}{%
\subsection{Where and how to share: the FAIR
principles}\label{where-and-how-to-share-the-fair-principles}}

For shared research products\sidenote{\footnotesize Most of this discussion is about
  data, because that's where the community has focused its efforts. That
  said, almost everything here applies to other research products as
  well!} to be usable by others, they should meet the FAIR standard by
being Findable, Accessible, Interoperable, and Reusable
(\protect\hyperlink{ref-wilkinson2016}{Wilkinson et al. 2016}).

\begin{itemize}
\tightlist
\item
  \textbf{Findable} products are easily discoverable to both humans and
  machines. That means linking to them in research reports using unique
  persistent identifiers (e.g.~a digital object identifier
  {[}DOI{]}).\sidenote{\footnotesize DOIs are those long URL-like things that are
    often used to link to papers. Turns out they can also be associated
    with datasets and other research products. Critically, they are
    guaranteed to work to find stuff, whereas standard web URLs often go
    stale after several years when people refactor their website. Most
    online repositories, like the Open Science Framework, will issue
    DOIs for the research products you store there.} and attaching them
  with metadata describing what they are so they can be indexed by
  search engines.
\item
  \textbf{Accessibility} means that research products need to be
  preserved across the long-term and are retrievable via their
  standardized identifier.
\item
  \textbf{Interoperability} means that the research products needs to be
  in a format that people and machines (e.g., search engines and
  analysis software) can understand.
\item
  \textbf{Reusable} means that the research products need to be well
  organized, documented, and licensed so that others know how to use
  them.
\end{itemize}

If you've followed the guidance in the rest of this chapter, then you
will already be well on your way to making your research products FAIR.
There are a few final steps to consider. An important decision is where
you are going to share the research products. We recommend uploading the
files to a repository that's designed according to support FAIR
principles. Personal websites don't cut it, since these sites tend to go
out of date and disappear. There's also no easy way to find research
products on personal sites unless you know who created them. Github,
though it's a great platform for collaboration, isn't a FAIR repository
-- for one thing, products there don't have DOIs\sidenote{\footnotesize You can get a
  DOI for github software through a partnership with
  \href{zenodo.org}{Zenodo}, a FAIR-compliant repository.} -- and there
are no archival guarantees on files that are shared there. Perhaps
surprisingly for some researchers, journal supplementary materials are
also not a great place to put research products. Often supplementary
materials are assigned no unique DOI or metadata, have limited supported
formats, and have no persistence guarantees
(\protect\hyperlink{ref-evangelou2005}{Evangelou, Trikalinos, and
Ioannidis 2005}).

Fortunately, there are many repositories that help you conform to FAIR
standards. Zenodo, Figshare, the Open Science Framework (OSF), and the
various Dataverse sites are designed for this purpose, though there are
many other domain-specific repositories that are particularly relevant
for different research fields. We often use the OSF as it makes it easy
to share all research products connected to a project in one place. OSF
is FAIR compatible and allows users to assign DOIs to their data and
provide appropriate metadata.

We recommend you attach a license to your research products. Academic
culture is (usually) unburdened by discussion of intellectual property
and legal rights and instead relies on scholarly norms about citation
and attribution. The basic expectation is that if you rely on someone
else's research, you explicitly acknowledge the relevant journal article
through a citation. Although norms are still evolving, using research
products created by others generally adheres to the same scholarly
principle. Research products can also be useful in non-academic
contexts, however. Perhaps you created software that a company would
like to use. Maybe a pediatrician would like to use a research
instrument you've been working on to assess their patients. These
applications (and many other reuses of the data) require a legal
license. In practice, there are a number of simple, open source licenses
that permit reuse. We tend to favor
\href{https://creativecommons.org}{Creative Commons licenses}, which
come in a variety of flavors such as
\href{https://creativecommons.org/share-your-work/public-domain/cc0/}{CC0}
(which allows all reuse),
\href{https://creativecommons.org/licenses/by/4.0/}{CC-BY} (which allows
reuse as long as there is attribution), and
\href{https://creativecommons.org/licenses/by/4.0/}{CC-BY-NC} (which
only allows attributed, non-commercial reuse).\sidenote{\footnotesize Klein et al.
  (\protect\hyperlink{ref-klein2018}{2018}) recommend the CC0 license,
  which puts no limits on what can be done with your data. At first
  blush it may seem like a license that requires attribution is useful.
  But academic norms, rather than the threat of litigation, lead to good
  citation practices. In addition, more restrictive licenses can mean
  that some legitimate uses of your data or research can be blocked.}
Regardless of what license you choose, having a license means that your
products won't be in a ``not sure what I'm allowed to do with this''
limbo for others who are interested in reusing them.

As we have discussed, you may want to consider storing your work in a
public repository from the outset of the project. If you are using
Github to manage your project, you can link the Git repository to the
Open Science Framework so it automatically syncs. This provides a
valuable incentive to organize your work properly throughout your
project and makes sharing super easy, because you've already done it! On
the other hand, this way of working can feel exposed for some
researchers, and it does carry some risks, however small, of
``scooping'' or pre-emption by other groups working in the same space.
Fortunately you can set up the same Git-OSF workflow and keep it private
until your ready to make it public later on.

The next stage at which you should consider sharing your research
products is when you submit your study to a journal. If you're still
hesitant to make the project entirely public, many repositories
(including OSF) will allow you to create special links that facilitate
limited access to, for example, reviewers and editors. In general, the
earlier you share your research products the better because there are
more opportunities for others to learn from, build on, and verify your
research.\sidenote{\footnotesize If there are errors in our work, we'd certainly love
  to hear about it \emph{before} the article is published in a journal
  rather than after!} But if neither of these options seem appealing,
please do share your research products once your paper is accepted.
Doing so will increase the value (and the impact) of your publication.

\hypertarget{chapter-summary}{%
\section{Chapter summary}\label{chapter-summary}}

All of the hard work you put into your experiments -- not to mention the
contributions of your participants -- can be undermined by bad data and
project management. As our accident reports and case study show, bad
organizational practices can at a minimum cause huge headaches.
Sometimes the consequences can be even worse. On the flip side, starting
with a firm organizational foundation sets your experiment up for
success. These practices also make it easier to share all of the
products of your research, not just your findings. Such sharing is both
useful for individual researchers and for the field as a whole.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find an Open Science Framework repository that corresponds to a
  published paper. What is their strategy for documenting what is
  shared? How easy is it to figure out where everything is and if the
  data and materials sharing is complete?
\item
  Open up the US Department of Health and Human Services ``safe harbor''
  standards
  \href{https://www.hhs.gov/hipaa/for-professionals/privacy/special-topics/de-identification/index.html}{here}
  and navigate to the section called ``The De-identification Standard.''
  Go through the list of identifiers that must be removed. Are there any
  on this list that you would need to conduct your own research? Can you
  think of any others that do not fall on this list?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\tightlist
\item
  A more in-depth tutorial on various aspects of scientific openness:
  Klein, O., Hardwicke, T. E., Aust, F., Breuer, J., Danielsson, H.,
  Hofelich Mohr, A., Ijzerman, H., Nilsonne, G., Vanpaemel, W., \&
  Frank, M. C. (2018). A practical guide for transparency in
  psychological science. \emph{Collabra: Psychology}, \emph{4}, 20.
  \url{https://doi.org/10.1525/collabra.158}.
\end{itemize}

\end{tcolorbox}

\leavevmode\vadjust pre{\hypertarget{reporting}{}}%
\part{Reporting}

\hypertarget{bibliography-17}{%
\section*{References}\label{bibliography-17}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-17}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-arslan2019}{}}%
Arslan, Ruben C. 2019. {``How to Automatically Document Data with the
Codebook Package to Facilitate Data Reuse.''} \emph{Advances in Methods
and Practices in Psychological Science} 2 (2): 169--87.

\leavevmode\vadjust pre{\hypertarget{ref-bischoff-grethe2007}{}}%
Bischoff-Grethe, Amanda, I Burak Ozyurt, Evelina Busa, Brian T Quinn,
Christine Fennema-Notestine, Camellia P Clark, Shaunna Morris, et al.
2007. {``A Technique for the Deidentification of Structural Brain MR
Images.''} \emph{Human Brain Mapping} 28 (9): 892--903.

\leavevmode\vadjust pre{\hypertarget{ref-blischak2016}{}}%
Blischak, John D, Emily R Davenport, and Greg Wilson. 2016. {``A Quick
Introduction to Version Control with Git and GitHub.''} \emph{PLoS
Computational Biology} 12 (1): e1004668.

\leavevmode\vadjust pre{\hypertarget{ref-broman2018}{}}%
Broman, Karl W, and Kara H Woo. 2018. {``Data Organization in
Spreadsheets.''} \emph{The American Statistician} 72 (1): 2--10.

\leavevmode\vadjust pre{\hypertarget{ref-byers-heinlein2020}{}}%
Byers-Heinlein, Krista, Christina Bergmann, Catherine Davies, Michael C
Frank, J Kiley Hamlin, Melissa Kline, Jonathan F Kominsky, et al. 2020.
{``Building a Collaborative Psychological Science: Lessons Learned from
ManyBabies 1.''} \emph{Canadian Psychology/Psychologie Canadienne} 61
(4): 349.

\leavevmode\vadjust pre{\hypertarget{ref-de-montjoye2015}{}}%
De Montjoye, Yves-Alexandre, Laura Radaelli, Vivek Kumar Singh, et al.
2015. {``Unique in the Shopping Mall: On the Reidentifiability of Credit
Card Metadata.''} \emph{Science} 347 (6221): 536--39.

\leavevmode\vadjust pre{\hypertarget{ref-evangelou2005}{}}%
Evangelou, Evangelos, Thomas A Trikalinos, and John PA Ioannidis. 2005.
{``Unavailability of Online Supplementary Scientific Information from
Articles Published in Major Journals.''} \emph{The FASEB Journal} 19
(14): 1943--44.

\leavevmode\vadjust pre{\hypertarget{ref-gilmore2017}{}}%
Gilmore, Rick O, and Karen E Adolph. 2017. {``Video Can Make Behavioural
Science More Reproducible.''} \emph{Nature Human Behaviour}.

\leavevmode\vadjust pre{\hypertarget{ref-hardwicke2021d}{}}%
Hardwicke, Tom E, Manuel Bohn, Kyle MacDonald, Emily Hembacher, Michle
B. Nuijten, Benjamin N. Peloquin, Benjamin E. deMayo, Bria Long, Erica
J. Yoon, and Michael C. Frank. 2021. {``Analytic Reproducibility in
Articles Receiving Open Data Badges at the Journal {Psychological}
{Science}: An Observational Study.''} \emph{Royal Society Open Science}
8 (1): 201494. \url{https://doi.org/10.1098/rsos.201494}.

\leavevmode\vadjust pre{\hypertarget{ref-hardwicke2018c}{}}%
Hardwicke, Tom E, and John P. A. Ioannidis. 2018. {``Populating the
{Data} {Ark}: {An} Attempt to Retrieve, Preserve, and Liberate Data from
the Most Highly-Cited Psychology and Psychiatry Articles.''} \emph{PLOS
ONE} 13 (8): e0201856.
\url{https://doi.org/10.1371/journal.pone.0201856}.

\leavevmode\vadjust pre{\hypertarget{ref-hardwicke2018b}{}}%
Hardwicke, Tom E, Maya B Mathur, Kyle Earl MacDonald, Gustav Nilsonne,
George Christopher Banks, Mallory Kidwell, Alicia Hofelich Mohr, et al.
2018. {``Data Availability, Reusability, and Analytic Reproducibility:
Evaluating the Impact of a Mandatory Open Data Policy at the Journal
Cognition.''}

\leavevmode\vadjust pre{\hypertarget{ref-houtkoop2018}{}}%
Houtkoop, Bobby Lee, Chris Chambers, Malcolm Macleod, Dorothy V. M.
Bishop, Thomas E. Nichols, and Eric-Jan Wagenmakers. 2018. {``Data
Sharing in Psychology: A Survey on Barriers and Preconditions.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (1):
70--85. \url{https://doi.org/10.1177/2515245917751886}.

\leavevmode\vadjust pre{\hypertarget{ref-klein2018}{}}%
Klein, Olivier, Tom E Hardwicke, Frederik Aust, Johannes Breuer, Henrik
Danielsson, Alicia Hofelich Mohr, Hans IJzerman, Gustav Nilsonne, Wolf
Vanpaemel, and Michael C Frank. 2018. {``A Practical Guide for
Transparency in Psychological Science.''}

\leavevmode\vadjust pre{\hypertarget{ref-meyer2018}{}}%
Meyer, Michelle N. 2018. {``Practical Tips for Ethical Data Sharing.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (1):
131--44.

\leavevmode\vadjust pre{\hypertarget{ref-munafo2017}{}}%
Munaf, Marcus R., Brian A Nosek, Dorothy V. M. Bishop, Katherine S.
Button, Christopher D. Chambers, Nathalie Percie du Sert, Uri Simonsohn,
Eric-Jan Wagenmakers, Jennifer J. Ware, and John P. A. Ioannidis. 2017.
{``A Manifesto for Reproducible Science.''} \emph{Nature Human
Behaviour} 1 (1): 1--9. \url{https://doi.org/10.1038/s41562-016-0021}.

\leavevmode\vadjust pre{\hypertarget{ref-nosek2015}{}}%
Nosek, Brian A, G. Alter, G. C. Banks, D. Borsboom, S. D. Bowman, S. J.
Breckler, S. Buck, et al. 2015. {``Promoting an Open Research
Culture.''} \emph{Science} 348 (6242): 1422--25.
\url{https://doi.org/10.1126/science.aab2374}.

\leavevmode\vadjust pre{\hypertarget{ref-petersen2019}{}}%
Petersen, Michael Bang. 2019. {``" Healthy Out-Group Members Are
Represented Psychologically as Infected in-Group Members":
Corrigendum.''}

\leavevmode\vadjust pre{\hypertarget{ref-piwowar2013}{}}%
Piwowar, Heather A, and Todd J Vision. 2013. {``Data Reuse and the Open
Data Citation Advantage.''} \emph{PeerJ} 1: e175.

\leavevmode\vadjust pre{\hypertarget{ref-ross2018}{}}%
Ross, Michael W, Martin Y Iguchi, and Sangeeta Panicker. 2018.
{``Ethical Aspects of Data Sharing and Research Participant
Protections.''} \emph{American Psychologist} 73 (2): 138.

\leavevmode\vadjust pre{\hypertarget{ref-rouder2015}{}}%
Rouder, Jeffrey N. 2015. {``The What, Why, and How of Born-Open Data.''}
\emph{Behavior Research Methods} 48 (3): 1062--69.
\url{https://doi.org/10.3758/s13428-015-0630-z}.

\leavevmode\vadjust pre{\hypertarget{ref-simonsohn2013}{}}%
Simonsohn, Uri. 2013. {``Just Post It: The Lesson from Two Cases of
Fabricated Data Detected by Statistics Alone.''} \emph{Psychological
Science} 24 (10): 1875--88.
\url{https://doi.org/10.1177/0956797613480366}.

\leavevmode\vadjust pre{\hypertarget{ref-tenopir2020}{}}%
Tenopir, Carol, Natalie M. Rice, Suzie Allard, Lynn Baird, Josh Borycz,
Lisa Christian, Bruce Grant, Robert Olendorf, and Robert J. Sandusky.
2020. {``Data Sharing, Management, Use, and Reuse: Practices and
Perceptions of Scientists Worldwide.''} Edited by Sergi Lozano.
\emph{{PLOS} {ONE}} 15 (3): e0229003.
\url{https://doi.org/10.1371/journal.pone.0229003}.

\leavevmode\vadjust pre{\hypertarget{ref-manybabies2020}{}}%
The ManyBabies Consortium, Michael C Frank, Katherine Jane Alcock,
Natalia Arias-Trejo, Gisa Aschersleben, Dare Baldwin, Stphanie Barbu,
et al. 2020. {``Quantifying Sources of Variability in Infancy Research
Using the {Infant-Directed-Speech} Preference.''} \emph{Advances in
Methods and Practices in Psychological Science}.

\leavevmode\vadjust pre{\hypertarget{ref-voytek2016}{}}%
Voytek, Bradley. 2016. {``The Virtuous Cycle of a Data Ecosystem.''}
\emph{PLOS Computational Biology} 12 (8): e1005037.
\url{https://doi.org/10.1371/journal.pcbi.1005037}.

\leavevmode\vadjust pre{\hypertarget{ref-wilkinson2016}{}}%
Wilkinson, Mark D, Michel Dumontier, I Jsbrand Jan Aalbersberg,
Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, et al.
2016. {``The {FAIR} Guiding Principles for Scientific Data Management
and Stewardship.''} \emph{Sci Data} 3 (March): 160018.

\leavevmode\vadjust pre{\hypertarget{ref-ziemann2016}{}}%
Ziemann, Mark, Yotam Eren, and Assam El-Osta. 2016. {``Gene Name Errors
Are Widespread in the Scientific Literature.''} \emph{Genome Biology} 17
(1): 1--3.

\end{CSLReferences}

\hypertarget{sec-writing}{%
\chapter{Writing}\label{sec-writing}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Write clearly by being concise, using structure, and adjusting to your
  audience
\item
  Write reproducibly by interleaving writing and analysis code
\item
  Write responsibly by acknowledging limitations, correcting errors, and
  calibrating your conclusions
\end{itemize}

\end{tcolorbox}

All of the effort you put into designing and running an effective
experiment may be wasted if you cannot clearly communicate what you did.
Writing is a powerful tool -- though you contribute to the conversation
only once, it enables you to speak to a potentially infinite number of
readers. So it's important to get it right! In this chapter, we'll
provide some guidance on how to write scientific papers -- the primary
method for reporting on experiments -- clearly, reproducibly, and
responsibly.\sidenote{\footnotesize Clarity of communication was a founding principle
  of modern science. Early proto-scientists conducting alchemical
  experiments often made their work deliberately obscure - even writing
  in cryptic codes - so that others could not discover the ``powerful
  secrets of nature.'' Pioneers of scientific methodology, like Francis
  Bacon and Robert Boyle, pushed instead for transparency and clarity.
  Notoriously, Issac Newton (originally an alchemist and later a
  scientist), continued to write in a deliberately obscure fashion in
  order to ``protect'' his work (\protect\hyperlink{ref-heard2016}{Heard
  2016}).}

\hypertarget{writing-clearly}{%
\section{Writing clearly}\label{writing-clearly}}

What is the purpose of writing? ``Telepathy, of course'' says Stephen
King (\protect\hyperlink{ref-king2000}{King 2000}). The goal of writing
is to transfer information from your mind to the reader's as effectively
as possible. Unfortunately, for most of us, writing clearly does not
come naturally; it is a craft we need to work at.

One of the most effective ways to learn to write clearly is to read and
to imitate the writing you admire. Many scientific articles are not
clearly written, so you will need to be selective in which models you
imitate. Fortunately, as a reader, you will know good writing when you
see it -- you will feel like the writer is sending ideas directly from
their mind to yours. When you come across writing like that, try to find
more work by the same author. The more good scientific writing you are
exposed to, the more you will develop a sense of what works and what
does not. You may pick up bad habits as well as good ones (we sure
have!), but over time, your writing will improve if you make a conscious
effort to weed out the bad, and keep the good.

There are no strict rules of clear writing, but there are some generally
accepted conventions that we will share with you here, drawing from both
general style guides and those specific to scientific writing
(\protect\hyperlink{ref-zinsser2006}{Zinsser 2006};
\protect\hyperlink{ref-heard2016}{Heard 2016};
\protect\hyperlink{ref-gernsbacher2018}{Gernsbacher 2018};
\protect\hyperlink{ref-savage2019}{Savage and Yeh 2019}).

\hypertarget{the-structure-of-a-scientific-paper}{%
\subsection{The structure of a scientific
paper}\label{the-structure-of-a-scientific-paper}}

A scientific paper is not a novel. Rather than reading from beginning to
end, readers typically jump between sections to extract information
efficiently (\protect\hyperlink{ref-doumont2009}{Doumont 2009}). This
``random access'' is possible because research articles typically follow
the same conventional structure (see Figure~\ref{fig-writing-imrad}).
The main body of the article includes four main sections: Introduction,
Methods, Results, and Discussion (IMRaD).\sidenote{\footnotesize In the old old days,
  there were few conventions -- scientists would share their latest
  findings by writing letters to each other. But as the number of
  scientists and studies increased, this approach became unsustainable.
  The IMRaD structure gained traction in the 1800s and became dominant
  in the mid-1900s as scientific productivity rapidly expanded in the
  post-war era. We think IMRaD style articles are a big improvement,
  even if it is nice to receive a letter every now and again.} This
structure has a narrative logic: what's the knowledge gap?
(introduction); how did you address it? (methods); what did you find?
(results); what do the results mean? (discussion).

Structure helps writers as well as readers. Try starting the writing
process with section headings as a structure, then flesh it out, layer
by layer. In each section, start by making a list of the key points you
want to convey, each representing the first sentence of a new paragraph.
Then add the content of each paragraph and you'll be well on your way to
having a full first draft of your article.

Imagine that the breadth of focus in the body of your article has an
``hourglass'' structure (Figure~\ref{fig-writing-imrad}). The start of
the introduction should have a broad focus, providing the reader with
the general context of your study. From there, the focus of the
introduction should get increasingly narrow until you are describing the
specific knowledge gap or problem you will address and (briefly how you
are going to address it. The methods and results sections are at the
center of the hourglass because they are tightly focused on your study
alone. In the discussion section, the focus shifts in the opposite
direction, from narrow to broad. Begin by summarizing the results of
your study, discuss limitations, then integrate the findings with
existing literature and describe practical and theoretical implications.

\begin{marginfigure}

{\centering \includegraphics{images/writing/imrad.png}

}

\caption{\label{fig-writing-imrad}Conventional structure of a research
article. The main body of the article consists of Introduction, Methods,
Results, and Discussion (IMRaD) sections.}

\end{marginfigure}

Research articles are often packed with complex information; it is easy
for readers to get lost. A ``cross reference'' is a helpful signpost
that tells readers where they can find relevant additional information
without disrupting the flow of your writing. For example, you can refer
the reader to data visualizations by cross referencing to figures or
tables (e.g., ``see Figure 1''), or additional methodological
information in the supplementary information (e.g., ``see Supplementary
Information A'').

One useful trick for structuring complex arguments is to cross reference
your research aims/hypotheses with your results. For example, you could
introduce numbered hypotheses in the introduction of an article and then
refer to them directly when reporting the relevant analyses and results.
These cross references can serve to remind readers how different results
or analyses relate back to your research goals.

\hypertarget{paragraphs-sentences-and-words}{%
\subsection{Paragraphs, sentences, and
words}\label{paragraphs-sentences-and-words}}

Writing an article is like drawing a human form. If you begin by
sketching the clothes, you risk adding beautiful textures onto an
impossible shape. Instead, you have to start by understanding the
underlying skeleton and then gradually adding layers until you can
visualize how cloth hangs on the body. The structure of an article is
the ``skeleton'' and the paragraphs and sentences are the ``flesh''.
Only start thinking about paragraphs and sentences once you have a solid
outline in place.

Ideally, each paragraph should correspond to a single point in the
article's outline, with the specifics necessary to convince the reader
embedded within. ``P-E-E-L'' (Point - Explain - Evidence - Link) is a
useful paragraph structure, particularly in the introduction and
discussion sections. First, state the paragraph's message succinctly in
the first sentence (P). The core of the paragraph is dedicated to
further explaining the point and providing evidence (E-E; you can also
include a third ``E'' -- an example). At the end of the paragraph, take
a couple of sentences to remind the reader of your point and set up a
link to the next paragraph.

Since each sentence in a paragraph has a purpose, you can compose and
edit the sentence by asking how its form serves that purpose. For
example, short sentences are great for making strong initial points. On
the other hand, if you only use short sentences your writing may come
across as monotonous and robotic. Try varying sentence lengths to give
your writing a more natural rhythm. Just avoid cramming too much
information into the same sentence; very long sentences can be confusing
and difficult to process.

You can also use sentence structure as a scaffold to support the
reader's thinking. Start sentences with something the reader already
knows. For example, rather than writing ``We performed a
between-subjects \(t\)-test comparing performance in the experimental
and control groups to address the cognitive dissonance hypothesis'',
write ``To address the cognitive dissonance hypothesis, we compared
performance in the experimental group and control group using a
between-subjects t-test.''

Human readers are good at processing narratives about people. Yet often
scientists compromise the research narrative by removing themselves from
the process, sometimes even using awkward grammatical constructions to
do so. For example, scientists sometimes write ``the data were
analysed'' or, worse, ``an analysis of the data was carried out.'' Many
of us were taught to write sentences like these, but it's much clearer
to say ``we analyzed the data.''

Similarly, many of us tend to hide our views with frames and caveats:
``{[}It is believed that/Research indicates that/Studies show that{]}
money leads to increased happiness (Frog \& Toad, 1963).'' If you truly
do believe that money causes happiness, simply assert it -- with a
citation if necessary. Save caveats for cases where \emph{someone}
believes that money causes happiness, but it's \emph{not} you. Emphasize
uncertainty where you in fact feel that uncertainty is warranted and
readers will take your doubts more seriously.

\hypertarget{advice}{%
\section{Advice}\label{advice}}

Scientific writing has a reputation for being dry, dull, and soulless.
While it's true that writing research articles is more constrained than
writing fiction, there are still ways to surprise and entertain your
reader with metaphor, alliteration, and even humor. As long as your
writing is clear and accurate, we see no reason why you cannot also make
it enjoyable. Enjoyable articles are easier to read and more fun to
write.\sidenote{\footnotesize One of our favorite examples of an enjoyable article is
  Cutler (\protect\hyperlink{ref-cutler1994}{1994}), a delightful piece
  that uses the form of the article to make a point about human language
  processing. Read it: you'll see!}

Here are a few more pieces of advice about expressing yourself clearly:

\textbf{Be explicit}. Avoid vagueness and ambiguity. The more you leave
the meaning of your writing to your reader's imagination the greater the
danger that different readers will imagine different things! So be
direct and specific.

\textbf{Be concise}. Maximize the signal to noise ratio in your writing
by omitting needless words and removing clutter
(\protect\hyperlink{ref-zinsser2006}{Zinsser 2006}). For example, say
\emph{we investigated} rather than \emph{we performed an investigation
of} and say \emph{if} rather than \emph{in the event that}. Don't try to
convey everything you know about a topic -- a research report is not an
essay. Include only what you need to achieve the purpose of the article
and exclude everything else.

\textbf{Be concrete}. Concrete examples make abstract ideas easier to
grasp. But some ideas are just hard to express in prose, and diagrams
can be very helpful in these cases. For example, it may be clearer to
illustrate a complex series of exclusion criteria using a flow chart
rather than text. You can even use photos, videos, and screenshots to
illustrate experimental tasks (\protect\hyperlink{ref-heycke2019}{Heycke
and Spitzer 2019}).

\textbf{Be consistent}. Referring to the same concept using different
words can be confusing because it may not be clear if you are referring
to a different concept or just using a synonym. For example, in everyday
conversation, ``replication'' and ``reproducibility'' may sound like two
different ways to refer to the same thing, but in scientific writing,
these two concepts have different technical definitions, so we should
not use them interchangeably. Define each technical term once and then
use the same term throughout the manuscript.

\textbf{Adjust to your audience}. Most of us adjust our conversation
style depending on who we're talking to; the same principle applies to
good writing. Knowing your audience is more difficult with writing,
because we cannot see the reader's reactions and adjust accordingly.
Nevertheless, we can make some educated guesses about who our readers
might be. For example, if you are writing an introductory review
article, you may need to pay more attention to explaining technical
termsn than if you are writing a research article for a specialty
journal.

\textbf{Check your understanding}. Unclear writing can be a symptom of
unclear thinking. If an idea doesn't make sense in your head, how will
it ever make sense on the page? In fact, trying to communicate something
in writing is an excellent way to probe your understanding and expose
logical gaps in your arguments. So if you are finding it difficult to
write clearly, stop and ask yourself \emph{do I know what I want to
say}? If the problem is unclear thinking, then it might be worth talking
out the ideas with a colleague or advisor before you try to write them
down.

\textbf{Use acronyms sparingly}. It's tempting to replace lengthy
terminology with short acronyms --- why say ``cognitive dissonance
theory'' when you can say ``CDT''? Unfortunately, acronyms can increase
the reader's cognitive burden and cause misunderstandings.\sidenote{\footnotesize Barnett
  and Doubleday (\protect\hyperlink{ref-barnett2020}{2020}) found that
  acronyms are widely used in research articles and argued that they
  undermine clear communication. Here is one example of text Barnett and
  Doubleday extracted from a 2019 publication to illustrate the point:
  ``Applying PROBAST showed that ADO, B-AE-D, B-AE-D-C, extended ADO,
  updated ADO, updated BODE, and a model developed by Bertens et
  al.~were derived in studies assessed as being at low risk of bias.''}
For example, if you shorten ``odds ratio'' to ``OR'', the reader has to
take the extra step of translating ``OR'' back to ``odds ratio'' every
time they encounter it. The problem multiplies as you introduce more
acronyms into your article. Worse, for some readers, ``OR'' tends to
mean ``operating room'', not ``odds ratio.'' Acronyms can be useful, but
usually only when they are widely used and understood.

\hypertarget{drafting-and-revision}{%
\subsection{Drafting and revision}\label{drafting-and-revision}}

The clearest and most effortless-seeming scientific writing has probably
gone through extensive revision to appear that way. It can surprise many
students to know the amount of revision that has gone into many
``breezy'' articles. For example, Tversky and Kahneman repeatedly
drafted and re-drafted each word of their famous (and highly readable)
articles on judgment and decision-making, hunched over the typewriter
together (\protect\hyperlink{ref-lewis2016undoing}{Lewis 2016}).

Think of the article you are writing as a garden. Your first draft may
be an unruly mess of intertwined fronds and branches. Several rounds of
pruning and sculpting will be needed before your writing reaches its
most effective form. You'll be amazed how often you find words you can
omit or elaborate sentences you can simplify.

It can be difficult to judge if your own writing has achieved its
telepathic goal, especially after several rounds of revision. Try to get
feedback from somebody in your target audience. Their comments -- even
if not wholly positive -- will give you a good sense of how much of your
argument they understood (and agreed with).\sidenote{\footnotesize Seek out people who
  are willing to tell you that your writing is not good! They may not
  make you feel good, but they will help you improve.}

\hypertarget{writing-reproducibly}{%
\section{Writing reproducibly}\label{writing-reproducibly}}

Many research results are not reproducible -\/--- that is, the numbers
and graphs that they report can't be recreated by repeating the original
analyses -- even on the original data. As we discussed in
\textbf{?@sec-replication}, a lack of reproducibility is a big problem
for the scientific literature; if you can't trust the numbers in the
articles you read, it's much harder to build on the literature.

Fortunately, there are number of tools and techniques available that you
can use to write fully reproducible research reports. The basic idea is
to create an unbroken chain that links every single part of the data
analysis pipeline, from the raw data through to the final numbers
reported in your research article. This linkage enables you -- and
hopefully others as well -- to trace the provenance of every number and
recreate (reproduce) it from scratch.

\hypertarget{why-write-reproducible-reports}{%
\subsection{Why write reproducible
reports?}\label{why-write-reproducible-reports}}

There are (at least) three reasons to write reproducible reports. First,
data analysis is an error-prone activity. Without safeguards in place,
it can be easy to accidentally overwrite data, mislabel experimental
conditions, or copy and paste the wrong statistics. As we discussed in
\textbf{?@sec-replication}, one study found that nearly half of a sample
of psychology papers contained obvious statistical reporting errors
(\protect\hyperlink{ref-nuijten2016}{Nuijten et al. 2016}). You can
reduce opportunities for error by adopting a reproducible analysis
workflow that avoids error-prone manual actions, like copying and
pasting.

Second, technical information about data analysis can be difficult to
communicate in writing. Prose is often ambiguous and authors can
inadvertently leave out important details
(\protect\hyperlink{ref-hardwicke2018b}{Hardwicke et al. 2018}). By
contrast, a reproducible workflow documents the entire analysis pipeline
from raw data to research report exactly as it was implemented,
describing the origin of any reported values and allowing readers to
assess, verify, and repeat the analysis process.

Finally, reproducible workflows are typically more efficient workflows.
For example, you may realize you forgot to perform data exclusions and
need to rerun the analysis. You may produce a graph and then decide
you'd prefer a different color scheme. Or perhaps you want to output the
same results table in a PDF document and in a PowerPoint slide. In a
reproducible workflow, all of the analysis steps are scripted, and can
be easily re-run at the click of a button. You (and others) can also
reuse parts of your code in other projects, rather than having to write
from scratch.

\hypertarget{principles-of-reproducible-writing}{%
\subsection{Principles of reproducible
writing}\label{principles-of-reproducible-writing}}

Below we outline some general principles of reproducible writing. These
can be put in practice in a number of different software ecosystems. We
recommend RMarkdown, a way of writing data analysis code in R so that it
compiles into spiffy documents or even websites. (This book was written
in RMarkdown). \textbf{?@sec-rmarkdown} gives an introduction to the
nuts and bolts of using RMarkdown to create scientific papers.

\begin{itemize}
\item
  \textbf{Never break the chain}. Every part of the analysis pipeline --
  from raw data\sidenote{\footnotesize Modulo the privacy concerns discussed in
    \textbf{?@sec-management}, of course.} to final product -- should be
  present in the project repository. By consulting the repository
  documentation, a reader should be able to follow the steps to go from
  the raw data to the final manuscript, including tables and figures.
\item
  \textbf{Script everything}. Try to ensure that each step of the
  analysis pipeline is executed by computer code rather than manual
  actions such as copying and pasting or directly editing spreadsheets.
  This practice ensures that every step is documented via executable
  code rather than ambiguous description, ensuring it can be reproduced.
  Imagine, for example, that you decided to re-code a variable in your
  dataset. You could use the ``find and replace'' function in Excel, but
  this action would not be documented -- you might even forget that you
  did it! A better option would be to write an R script. While a
  scripted pipeline can be a pain to set up the first time, by the third
  time you rerun it, it will save you time.
\item
  \textbf{Use literate programming}. The meaning of a chunk of computer
  code is not always obvious to another user, especially if they're not
  an expert. Indeed, we frequently look at our own code and scratch our
  heads, wondering what on earth it's doing. To avoid this problem, try
  to structure your code around plain language comments that explain
  what it should be doing, a technique known as ``literate programming''
  (\protect\hyperlink{ref-knuth1992}{Knuth 1992}).
\item
  \textbf{Use defensive programming}. Errors can still occur in scripted
  analyses. Defensive programming is a series of strategies to help
  anticipate, detect, and avoid errors in advance. A typical defensive
  programming tool is the inclusion of \textbf{tests} in your code,
  snippets that check if the code or data meet some assumptions. For
  example, you might test if a variable storing reaction times has taken
  on values below zero (which should be impossible). If the test passes,
  the analysis pipeline continues; if the test fails, the pipeline halts
  and an error message appears to alert you to the problem.
\item
  \textbf{Use free/open-source software and programming languages}. If
  possible, avoid using commercial software, like SPSS or Matlab, and
  instead use free, open-source software and programming languages, like
  JASP, Jamovi, R, or Python. This practice will make it easier for
  others to access, reuse, and verify your work -- including
  yourself!\sidenote{\footnotesize Several of us have libraries of old Matlab code.
    While discounted licenses are available to students, a full-price
    software license can be a major barrier to researchers with limited
    resources. if you move away from Matlab, it's also terrible to have
    to ask yourself whether it's worth the price of another year's
    license just to rerun one old analysis.}
\item
  \textbf{Use version control}. In \textbf{?@sec-management}, we
  introduced the benefits of version control -- a great way to save your
  analysis pipeline incrementally as you build it, allowing you to roll
  back to a previous version if you accidentally introduce errors.
\item
  \textbf{Preserve the computational environment}. Even if your analysis
  pipeline is entirely reproducible on your own computer, you still need
  to consider whether it will run on somebody else's computer, or even
  your own computer after software updates. You can address this issue
  by documenting and preserving the computational environment in which
  the analysis pipeline runs successfully. Various tools are available
  to help with this, including Docker, Code Ocean, renv (for R), and pip
  (for Python).\sidenote{\footnotesize If you are interested in going in this
    direction, we recommend Peikert and Brandmaier
    (\protect\hyperlink{ref-peikert2021}{2021}), which gives an advanced
    tutorial for complete computational reproducibility using Docker and
    \texttt{make} as tools to supplement git and R Markdown.}
\end{itemize}

\hypertarget{the-reproducibility-collaboration-trade-off}{%
\subsection{The reproducibility-collaboration
trade-off}\label{the-reproducibility-collaboration-trade-off}}

We would love to leave it there and watch you walk off into the sunset
with a spring in your step and a reproducible report under your arm.
Unfortunately, we have to admit that writing reproducibly can create a
few practical difficulties when it comes to collaboration.

A major aspect of collaboration is exchanging comments and inline text
edits with co-authors. You can do this exchange with R Markdown files
and Git, but these tools are not as user-friendly as, say, Word or
Google Docs, and some collaborators will be completely unfamiliar with
them. Most journals also expect articles to be submitted as Word
documents. Outputting R Markdown files to Word can often introduce
formatting issues, especially for moderately complex tables. So until
more user-friendly tools are introduced, some compromise between
reproducibility and collaboration may be necessary. Here are two
workflow styles for you to consider.

First, the \textbf{maximal reproducibility} approach. If your
collaborators are familiar with R Markdown and you don't mind exchanging
comments and edits via Git -- or if they don't mind giving you lists of
comments and changes that you implement in the R Markdown document --
then you can maintain a fully reproducible workflow for your project.
The journal submission and publication process may still introduce some
issues such as incorporating changes made by the copy editor, but at
least your submitted manuscript (and the preprint you have hopefully
posted) will be fully reproducible.

Second, the \textbf{two worlds} approach. This workflow is a bit clunky,
but it facilitates collaboration and maintains reproducibility. First,
write your results section in R Markdown and generate a Word document
(see \textbf{?@sec-rmarkdown}). Then, write the remainder of the
manuscript in Word, including incorporating comments and changes from
collaborators. When you have a final version, copy and paste the
abstract, introduction, methods, and discussion into the R Markdown
document.\sidenote{\footnotesize You can also incorporate Google Docs into this
  workflow -- we find that cloud platforms like Docs are especially
  useful when gathering comments from multiple collaborators on the same
  document. Unfortunately, you cannot generate a Google Doc from R
  Markdown, so you will need to import and convert or else copy and
  paste.} Integrating any changes made to the results section back into
the R Markdown requires a bit more effort, either using manual checking
or Word's ``compare documents'' feature. The advantage of this approach
is that you have a reproducible document and your collaborators have not
had to deviate from their preferred workflow. Unfortunately, it requires
more effort from you and is slightly more error-prone than the maximal
reproducibility approach.

\hypertarget{writing-responsibly}{%
\section{Writing responsibly}\label{writing-responsibly}}

As a scientific writer, you have both professional and ethical
responsibilities. You must communicate all relevant information about
your research so as to enable proper evaluation and verification by
other scientists. It is also important not to overstate your findings
and calibrate your conclusions to the available evidence
(\protect\hyperlink{ref-hoekstra2020}{Hoekstra and Vazire 2020}). If
errors are found in your work, you must respond and correct them when
possible (\protect\hyperlink{ref-bishop2018}{Bishop 2018}). Finally, you
must meet scholarly obligations with regards to authorship and citation
practices.

\hypertarget{responsible-disclosure-and-interpretation}{%
\subsection{Responsible disclosure and
interpretation}\label{responsible-disclosure-and-interpretation}}

Back in school, we all learned that getting the right answer is not
enough -- you need to demonstrate how you arrived at that answer in
order to get full marks. The same expectation applies to research
reports. Don't just tell the reader what you found, tell them how you
found it.\sidenote{\footnotesize It can be easy to overlook important details,
  especially when you reach the end of a project. Looking back at your
  study preregistration can be a helpful reminder. Reporting guidelines
  for different research designs can also provide useful checklists
  (\protect\hyperlink{ref-appelbaum2018}{Appelbaum et al. 2018}).} That
means describing the methods in full detail, as well as sharing data,
materials, and analysis scripts.

In a journal article, you typically have some flexibility in terms of
how much detail you provide in the main body of the article and how much
you relegate to the supplementary information. Readers have different
needs; some may just want to know the highlights, and some will need
detailed methodological information in order to replicate your study. As
a rule of thumb, try to make sure there is nothing relegated to the
supplementary information that might surprise the reader. You certainty
should not use the supplementary information to hide important details
deliberately or use it as a disorganized dumping ground -- the
principles of clear writing still apply!

Here are a few more guidelines for responsible writing:

\begin{itemize}
\item
  \textbf{Don't overclaim}. Scientists often feel they are (and
  unfortunately, often are) evaluated based on the \emph{results} of
  their research, rather than the \emph{quality} of their research.
  Consequently, it can be tempting to make bigger and bolder claims than
  are really justified by the evidence. Think carefully about the
  limitations of your research and calibrate your conclusions to the
  evidence, rather than what you wish you were able to claim. Ensure
  that your conclusions are appropriately stated throughout the
  manuscript, especially in the title and abstract.
\item
  \textbf{Acknowledge limitations}. Even if you calibrate your claims
  appropriately throughout, there are likely specific limitations that
  are worth discussing, either as you introduce the design of the study
  in the introduction or as you interpret it in the discussion section.
  For example, if your experiment used one particular manipulation to
  instantiate a construct of interest, you might discuss this limitation
  and how it might be addressed by future work. Think carefully about
  the limitations of your study, state them clearly, and consider how
  they impact your conclusions
  (\protect\hyperlink{ref-clarke2023}{Clarke et al. 2023}).\sidenote{\footnotesize Should
    you just make your claims more modest, and avoid writing about your
    study's limitations? The balance between claims and limitations is
    tricky. One way to navigate this issue is to ask yourself, ``is it
    OK to say X in the abstract of my article, if I later go on to say
    state a limitation relevant to that claim, or will the reader feel
    tricked?''}
\item
  \textbf{Discuss, don't debate}. The purpose of the discussion section
  is to help the reader interpret your research. Importantly, a journal
  article is not a debate -- don't feel the need to argue dogmatically
  for a particular position or interpretation. You should discuss the
  strengths and weaknesses of the evidence, and the relative merits of
  different interpretations. For example, perhaps there is a potential
  confounding variable that you were unable to eliminate with your
  research design. The reader might be able to spot this themselves, but
  regardless, its your responsibility to highlight it. Perhaps on
  balance you think the confound is unlikely to explain the results --
  that's fine, but you need to explain your reasoning to the reader.
\item
  \textbf{Disclose conflicts of interest and funding}. Researchers are
  usually personally invested in the outcomes of their research and this
  investment can lead to bias (for example, overclaiming or selective
  reporting). But sometimes your potential personal gains from a piece
  of research rise above a threshold and are considered
  \textbf{conflicts of interest}. Where this threshold lies is not
  always completely clear. The most obvious conflicts of interest occur
  when you stand to benefit financially from the outcomes of your
  research (for example a drug developer evaluating their own drug). If
  you are in doubt about whether you have a potential conflict of
  interest, then you should disclose it. You should also disclose any
  funding you received for the research, partly because this is often a
  requirement of the funder, and partly because it may represent a
  conflict of interest, for example if the funder has a particular stake
  in the outcome of the research. To avoid ambiguity, you should also
  disclose when you do \emph{not} have a conflict of interest or funding
  to declare.
\item
  \textbf{Report transparently}. In \textbf{?@sec-prereg}, you learned
  about the problem of selective reporting and how this practice can
  bias the research literature. There are several ways to avoid this
  issue in your own work. First, assuming you \emph{have} reported
  everything, include a statement in the methods section that explicitly
  says so. A statement suggested by Simmons, Nelson, and Simonsohn
  (\protect\hyperlink{ref-simmons2012}{2012}) is ``We report how we
  determined our sample size, all data exclusions (if any), all
  manipulations, and all measures in the study.'' If you have
  preregistered your study, clearly link to the preregistration and
  state whether you deviated from your original plan. You can include a
  detailed preregistration disclosure table in the supplementary
  information and highlight any major deviations in the methods section.
  In the results section, clearly identify (e.g., with sub-headings)
  which analyses were pre-planned and included in the preregistration
  (confirmatory) and which were not planned (exploratory).
\end{itemize}

\hypertarget{responsible-handling-of-errors}{%
\subsection{Responsible handling of
errors}\label{responsible-handling-of-errors}}

It is not your responsibility to never make mistakes. But it \emph{is}
your responsibility to respond to errors in a timely, transparent, and
professional manner (\protect\hyperlink{ref-bishop2018}{Bishop
2018}).\sidenote{\footnotesize As jazz musician Miles Davis once said, ``If you hit a
  wrong note, it's the next note that you play that determines if it's
  good or bad.''} Regardless of how the error was identified (e.g., by
yourself or by a reader), we recommend contacting the journal and
requesting that they publish a correction statement (sometimes called an
\textbf{erratum}). Several of us have corrected papers in the past. If
the error is serious and cannot be fixed, you should consider retracting
the article.

A correction/retraction statement should include the following
information:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Acknowledge the error}. Be clear that an error has occurred.
\item
  \textbf{Describe the error}. Readers need to know the exact nature of
  the error.
\item
  \textbf{Describe the implications of the error}. Readers need to know
  how the error might affect their interpretation of the results.
\item
  \textbf{Describe how the error occurred}. Knowing how the error
  happened may help others avoid the same error.
\item
  \textbf{Describe what you have done to address the error}. Others may
  learn from solutions you've implemented.
\item
  \textbf{Acknowledge the person who identified the error}. Identifying
  errors can take a lot of work; if the person is willing to be
  identified, give credit where credit is due.
\end{enumerate}

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

In 2018, at a crucial stage of her career, Julia Strand published an
important study in the prestigious journal \emph{Psychonomic Bulletin \&
Review}. She presented the work at conferences and received additional
funding to do follow-up studies. But several months later, her team
found that they could not replicate the result.

Puzzled, she began searching for the cause of the discrepant results.
Eventually, she found the culprit -- a programming error. As she sat
staring at her computer in horror, she realized that it was unlikely
anyone else would ever find the bug. Hiding the error must have seemed
like the easiest thing to do.

But she did the right thing. She spent the next day informing her
students, her co-authors, the funding officer, the department chair
overseeing her tenure review, and the journal -- to initiate a
retraction of the article. And\ldots{} it didn't ruin her career.
Everybody was understanding and appreciated that she was doing the right
thing. The journal corrected the article. She didn't lose her grant. She
got tenure. And a lot of scientists, including us, admire her for what
she did. Honest mistakes happen -- it's how you respond to them that
matters (\protect\hyperlink{ref-strand2021}{Strand 2021}).

\end{tcolorbox}

\hypertarget{responsible-citation}{%
\subsection{Responsible citation}\label{responsible-citation}}

Citing prior work that your study builds upon ensures that researchers
receive credit for their contributions and helps readers to verify the
basis of your claims. You should certainly avoid copying the work of
others and presenting it as your own (see \textbf{?@sec-ethics} for more
on plagiarism). Try to be explicit about why you are citing a source.
For example, does it provide evidence to support your point? Is it a
review paper that gives the reader useful background? Or is it a
description of a theory you are testing?

Make sure you read articles before you cite them. Stang, Jonas, and
Poole (\protect\hyperlink{ref-stang2018}{2018}) reports a cautionary
tale in which a commentary criticizing a methodological tool was
frequently cited as \emph{supporting} the use of that tool! It seems
that many authors had not read the paper they were citing, which is both
misleading and embarrassing.

Try to avoid selective or uncritical citation. It is misleading to cite
only research that supports your argument and ignoring research that
doesn't. You should provide a balanced account of prior work, including
contradictory evidence. Make sure to evaluate and integrate evidence
from prior studies, rather than simply describing them. Remember --
every study has limitations.

\hypertarget{responsible-authorship-practices}{%
\subsection{Responsible authorship
practices}\label{responsible-authorship-practices}}

It is an ethical responsibility to credit the individuals who worked on
a research project -- both so that they can reap the benefits if the
work is influential, but also so that they can take responsibility for
errors.\sidenote{\footnotesize In 1975, physicist and mathematician Jack H.
  Hetherington wrote a paper he intended to submit to the journal
  \emph{Physical Review Letters}. We're not sure why, but Hetherington
  wrote the paper in first person plural (i.e., referring to himself as
  ``we'' rather than ``I''). He subsequently discovered that the journal
  would not accept the use of ``we'' for single-authored articles.
  Hetherington had painstakingly tapped out the article on his
  typewriter, an exercise he was not keen to repeat. Instead, he opted
  for a less taxing solution and named his cat -- a feline by the name
  of F. D. C. Willard -- as a coauthor. The paper was accepted and
  published (\protect\hyperlink{ref-hetherington1975}{Hetherington and
  Willard 1975}).}

Currently in academia, the \emph{authorship model} is dominant. Under
this model, authorship and authorship order are important signals about
researchers contributions to a project. It is generally expected that to
qualify for authorship, an individual should have made a substantial
contribution to the research (e.g., design, data collection, analysis),
assisted with writing the research report, and takes joint
responsibility for the research along with the other co-authors.
Individuals who worked on the project who do not reach this threshold
are instead mentioned in a separate acknowledgements section and not
considered authors.

\textbf{Authorship order} is often understood to signal the nature and
extent of an author's contribution. In psychology (and neighboring
disciplines), the first author and last author are typically the project
leaders. Typically -- though certainly not always! -- the first author
is a junior colleague who implements the project and the last author is
a senior colleague who supervises the project.

It has been argued that the authorship model should be replaced with a
more inclusive \emph{contributorship} model in which all individuals who
worked on the project are acknowledged as `contributors'. Unlike the
authorship model, there is no arbitrary threshold for contributorship.
The actual contributions of each individual are explicitly described,
rather than relying on the implicit conventions of authorship order. The
contributorship model may facilitate collaboration and ensure student
assistants are properly credited.

You will probably find that most journals still expect you to use the
authorship model. Nevertheless, it is usually possible -- and sometimes
required -- to include a contributorship statement in your article that
describes what everybody did. For example, the CREDIT taxonomy provides
a structured taxonomy of research tasks, making for uniform
contributorship reporting.\sidenote{\footnotesize For larger projects, the tool
  Tenzing allows for CREDIT statements to be generated automatically
  from standardized forms (\protect\hyperlink{ref-holcombe2020}{Holcombe
  et al. 2020}).}

Because authorship is such an important signal in academia, it's
important to agree on an authorship plan with your collaborators
(particularly who will be the first and last authors) as early as
possible.\sidenote{\footnotesize If you have find yourself in a situation where all
  authors have contributed equally, you may have to draw inspiration
  from historical examples and determine authorship order based on a 25
  game croquet series (\protect\hyperlink{ref-hassell1974}{Hassell and
  May 1974}), rock, paper, scissors
  (\protect\hyperlink{ref-kupfer2004}{Kupfer, Webbeking, and Franklin
  2004}), or a brownie bake-off (\protect\hyperlink{ref-young1992}{Young
  and Young 1992}). Alternatively, you can adopt the method of Lakens,
  Scheel, and Isager (\protect\hyperlink{ref-lakens2018}{2018}) and
  randomize the authorship order in R!}

\hypertarget{chapter-summary-writing}{%
\section{Chapter summary: Writing}\label{chapter-summary-writing}}

Writing a scientific article can be a rewarding endpoint for the process
of doing experimental research. But writing is a craft, and writing
clearly -- especially about complex and technical topics -- can require
substantial practice and many drafts. Further, writing about research
comes with ethical and professional responsibilities that are different
than the burdens of other kinds of writing. A scientific author must
work to ensure the reproducibility of their findings and report on those
findings responsibly, noting limitations and weaknesses as well as
strengths.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find a writing buddy and exchange feedback on a short piece of writing
  (the abstract of a paper in progress, a conference abstract, or even a
  class project proposal would be good examples). Think about how to
  improve each other's writing using the advice offered in this chapter.
\item
  Identify a published research article with openly available data and
  see if you can reproduce an analysis in their paper by recovering the
  exact numerical values they report. You can find support for this
  exercise at the Social Science Reproduction Platform
  (\url{https://www.socialsciencereproduction.org}) or ReproHack
  (\url{https://www.reprohack.org}). Discuss with a friend what
  challenges you faced in this exercise and how they might be avoided in
  your own work.
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\item
  Zinsser, W. (2006). \emph{On writing well: The classic guide to
  writing nonfiction {[}7th ed{]}}. Harper Collins.
\item
  Gernsbacher, M. A. (2018). Writing empirical articles: Transparency,
  reproducibility, clarity, and memorability. \emph{Advances in Methods
  and Practices in Psychological Science}, \emph{1}, 403--14.
  \url{https://doi.org/10.1177/2515245918754485}.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-19}{%
\section*{References}\label{bibliography-19}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-19}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-appelbaum2018}{}}%
Appelbaum, Mark, Harris Cooper, Rex B. Kline, Evan Mayo-Wilson, Arthur
M. Nezu, and Stephen M. Rao. 2018. {``Journal Article Reporting
Standards for Quantitative Research in Psychology: {The} {APA}
{Publications} and {Communications} {Board} Task Force Report.''}
\emph{American Psychologist} 73 (1): 3.
\url{https://doi.org/10.1037/amp0000191}.

\leavevmode\vadjust pre{\hypertarget{ref-barnett2020}{}}%
Barnett, Adrian, and Zoe Doubleday. 2020. {``The Growth of Acronyms in
the Scientific Literature.''} \emph{eLife} 9 (July): e60080.
\url{https://doi.org/10.7554/eLife.60080}.

\leavevmode\vadjust pre{\hypertarget{ref-bishop2018}{}}%
Bishop, D. V. M. 2018. {``Fallibility in Science: Responding to Errors
in the Work of Oneself and Others.''} \emph{Advances in Methods and
Practices in Psychological Science} 1 (3): 432--38.
\url{https://doi.org/10.1177/2515245918776632}.

\leavevmode\vadjust pre{\hypertarget{ref-clarke2023}{}}%
Clarke, Beth, Lindsay Alley, Sakshi Ghai, Jessica Kay Flake, Julia M.
Rohrer, Joseph P. Simmons, Sarah R. Schiavone, and Simine Vazire. 2023.
{``Looking Our Limitations in the Eye: A Tutorial for Writing about
Research Limitations in Psychology.''} {PsyArXiv}.
\url{https://doi.org/10.31234/osf.io/386bh}.

\leavevmode\vadjust pre{\hypertarget{ref-cutler1994}{}}%
Cutler, Anne. 1994. {``The Perception of Rhythm in Language.''}

\leavevmode\vadjust pre{\hypertarget{ref-doumont2009}{}}%
Doumont, Jean-Luc. 2009. {``Trees, Maps, and Theorems.''}
\emph{Brussels: Principiae}.

\leavevmode\vadjust pre{\hypertarget{ref-gernsbacher2018}{}}%
Gernsbacher, Morton Ann. 2018. {``Writing Empirical Articles:
Transparency, Reproducibility, Clarity, and Memorability.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (3):
403--14. \url{https://doi.org/10.1177/2515245918754485}.

\leavevmode\vadjust pre{\hypertarget{ref-hardwicke2018b}{}}%
Hardwicke, Tom E, Maya B Mathur, Kyle Earl MacDonald, Gustav Nilsonne,
George Christopher Banks, Mallory Kidwell, Alicia Hofelich Mohr, et al.
2018. {``Data Availability, Reusability, and Analytic Reproducibility:
Evaluating the Impact of a Mandatory Open Data Policy at the Journal
Cognition.''}

\leavevmode\vadjust pre{\hypertarget{ref-hassell1974}{}}%
Hassell, M. P., and R. M. May. 1974. {``Aggregation of Predators and
Insect Parasites and Its Effect on Stability.''} \emph{Journal of Animal
Ecology} 43 (2): 567--94. \url{https://doi.org/10.2307/3384}.

\leavevmode\vadjust pre{\hypertarget{ref-heard2016}{}}%
Heard, Stephen B. 2016. \emph{The Scientist's Guide to Writing: How to
Write More Easily and Effectively Throughout Your Scientific Career}.
Princeton, New Jersey: Princeton University Press.

\leavevmode\vadjust pre{\hypertarget{ref-hetherington1975}{}}%
Hetherington, J. H., and F. D. C. Willard. 1975. {``Two-, Three-, and
Four-Atom Exchange Effects in Bcc
\${\^{}}\{3\}{\textbackslash{}}mathrm\{he\}\$.''} \emph{Physical Review
Letters} 35 (21): 1442--44.
\url{https://doi.org/10.1103/PhysRevLett.35.1442}.

\leavevmode\vadjust pre{\hypertarget{ref-heycke2019}{}}%
Heycke, Tobias, and Lisa Spitzer. 2019. {``Screen Recordings as a Tool
to Document Computer Assisted Data Collection Procedures.''}
\emph{Psychologica Belgica} 59 (1): 269--80.
\url{https://doi.org/10.5334/pb.490}.

\leavevmode\vadjust pre{\hypertarget{ref-hoekstra2020}{}}%
Hoekstra, Rink, and Simine Vazire. 2020. {``Intellectual Humility Is
Central to Science.''} Preprint. \url{https://osf.io/edh2s}.

\leavevmode\vadjust pre{\hypertarget{ref-holcombe2020}{}}%
Holcombe, Alex O., Marton Kovacs, Frederik Aust, and Balazs Aczel. 2020.
{``Documenting Contributions to Scholarly Articles Using {CRediT} and
Tenzing.''} Edited by Cassidy R. Sugimoto. \emph{PLOS ONE} 15 (12):
e0244611. \url{https://doi.org/10.1371/journal.pone.0244611}.

\leavevmode\vadjust pre{\hypertarget{ref-king2000}{}}%
King, Stephen. 2000. \emph{On Writing: A Memoir of the Craft}. Scribner.

\leavevmode\vadjust pre{\hypertarget{ref-knuth1992}{}}%
Knuth, Donald Ervin. 1992. \emph{Literate Programming}. no. 27. Center
for the Study of Language; Information.

\leavevmode\vadjust pre{\hypertarget{ref-kupfer2004}{}}%
Kupfer, John A, Amy L Webbeking, and Scott B Franklin. 2004. {``Forest
Fragmentation Affects Early Successional Patterns on Shifting
Cultivation Fields Near {Indian} {Church}, {Belize}.''}
\emph{Agriculture, Ecosystems \& Environment} 103 (3): 509--18.
\url{https://doi.org/10.1016/j.agee.2003.11.011}.

\leavevmode\vadjust pre{\hypertarget{ref-lakens2018}{}}%
Lakens, Danil, Anne M. Scheel, and Peder M. Isager. 2018.
{``Equivalence Testing for Psychological Research: A Tutorial.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (2):
259--69. \url{https://doi.org/10.1177/2515245918770963}.

\leavevmode\vadjust pre{\hypertarget{ref-lewis2016undoing}{}}%
Lewis, Michael. 2016. \emph{The Undoing Project: A Friendship That
Changed the World}. Penguin UK.

\leavevmode\vadjust pre{\hypertarget{ref-nuijten2016}{}}%
Nuijten, Michle B, Chris H J Hartgerink, Marcel A L M van Assen, Sacha
Epskamp, and Jelte M Wicherts. 2016. {``The Prevalence of Statistical
Reporting Errors in Psychology (1985--2013).''} \emph{Behav. Res.
Methods} 48 (4): 1205--26.

\leavevmode\vadjust pre{\hypertarget{ref-peikert2021}{}}%
Peikert, Aaron, and Andreas M Brandmaier. 2021. {``A Reproducible Data
Analysis Workflow with r Markdown, Git, Make, and Docker.''}
\emph{Quantitative and Computational Methods in Behavioral Sciences},
1--27.

\leavevmode\vadjust pre{\hypertarget{ref-savage2019}{}}%
Savage, Van, and Pamela Yeh. 2019. {``Novelist Cormac McCarthy's Tips on
How to Write a Great Science Paper.''} \emph{Nature} 574 (7777):
441--43.

\leavevmode\vadjust pre{\hypertarget{ref-simmons2012}{}}%
Simmons, Joseph P, Leif D Nelson, and Uri Simonsohn. 2012. {``A 21 Word
Solution.''} \emph{SSRN Electronic Journal}.
\url{https://doi.org/10.2139/ssrn.2160588}.

\leavevmode\vadjust pre{\hypertarget{ref-stang2018}{}}%
Stang, Andreas, Stephan Jonas, and Charles Poole. 2018. {``Case Study in
Major Quotation Errors: A Critical Commentary on the
{Newcastle}--{Ottawa} Scale.''} \emph{European Journal of Epidemiology}
33 (11): 1025--31. \url{https://doi.org/10.1007/s10654-018-0443-3}.

\leavevmode\vadjust pre{\hypertarget{ref-strand2021}{}}%
Strand, Julia. 2021. {``Error Tight: Exercises for Lab Groups to Prevent
Research Mistakes.''} PsyArXiv.
\url{https://doi.org/10.31234/osf.io/rsn5y}.

\leavevmode\vadjust pre{\hypertarget{ref-young1992}{}}%
Young, Helen J., and Truman P. Young. 1992. {``Alternative {Outcomes} of
{Natural} and {Experimental} {High} {Pollen} {Loads}.''} \emph{Ecology}
73 (2): 639--47. \url{https://doi.org/10.2307/1940770}.

\leavevmode\vadjust pre{\hypertarget{ref-zinsser2006}{}}%
Zinsser, William. 2006. \emph{On Writing Well: The Classic Guide to
Writing Nonfiction}. 30th anniversary ed., 7th ed., rev. and updated.
New York: HarperCollins.

\end{CSLReferences}

\hypertarget{sec-viz}{%
\chapter{Visualization}\label{sec-viz}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Analyze the principles behind informative visualizations
\item
  Incorporate visualization into an analysis workflow
\item
  Learn to make ``the design plot''
\item
  Select different visualizations of variability and distribution
\item
  Connect visualization concepts to measurement principles
\end{itemize}

\end{tcolorbox}

What makes visualizations so useful, and what role do they play in the
experimenter's toolkit? Simply put, data visualization is the act of
``making the invisible visible.'' Our visual systems are remarkably
powerful pattern detectors, and relationships that aren't at all clear
when scanning through rows of raw data can immediately jump out at us
when presented in an appropriate graphical form
(\protect\hyperlink{ref-zacks2020designing}{Zacks and Franconeri 2020}).
Good visualizations aim to deliberately harness this power and put it to
work at every stage of the research process, from the quick sanity
checks we run when first reading in our data to the publication-quality
figures we design when we are ready to communicate our findings. Yet our
powerful pattern detectors can also be a liability; if we're not
careful, we can easily be fooled into seeing patterns that are
unreliable or even misleading. As psychology moves into an era of bigger
data and more complex behaviors, we become increasingly reliant on
\textbf{data visualization literacy}
(\protect\hyperlink{ref-borner2019data}{Brner, Bueckle, and Ginda
2019}) to make sense of what is going on.

\begin{tcolorbox}[colframe=.blue, title=\faMicroscope \enspace Case study]

\hypertarget{mapping-a-pandemic}{%
\section*{Mapping a pandemic}\label{mapping-a-pandemic}}
\addcontentsline{toc}{section}{Mapping a pandemic}

\markright{Mapping a pandemic}

In 1854, a deadly outbreak of cholera was sweeping through London. The
scientific consensus at the time was that diseases like cholera spread
through breathing poisonous and foul-smelling vapors, an idea known as
the ``miasma theory''
(\protect\hyperlink{ref-halliday2001death}{Halliday 2001}). An
obstetrician and anesthesiologist named John Snow, however, had proposed
an alternative theory: rather than spreading through foul air, he
thought that cholera was spreading through a polluted water supply
(\protect\hyperlink{ref-snow1855mode}{Snow 1855}). To make a public case
for this idea, he started counting cholera deaths. He marked each case
on a map of the area, and indicated the locations of the water pumps for
reference. Furthermore, a line could be drawn representing the region
that was closest to each water pump, a technique which is now known as a
\href{https://en.wikipedia.org/wiki/Voronoi_diagram}{Voronoi diagram}.
The resulting illustration clearly reveals that cases clustered around
an area called Golden Square, which received water from a pump on Broad
Street (Figure~\ref{fig-viz-cholera}). Although the precise causal role
of these maps in Snow's own thinking is disputed, and it is likely that
he produced them well after the incident
(\protect\hyperlink{ref-brody2000map}{Brody et al. 2000}), they
nonetheless played a significant role in the history of data
visualization (\protect\hyperlink{ref-friendly2021history}{Friendly and
Wainer 2021}).

\begin{figure}[H]

{\centering \includegraphics[width=0.45\textwidth,height=\textheight]{images/viz/snow_cholera_voronoi.jpeg}

}

\caption{\label{fig-viz-cholera}Mapping out a cholera epidemic (1854).
Line shows region for which Broad Street pump is nearest.}

\end{figure}

Nearly two centuries later, as the COVID-19 pandemic swept through the
world, governmental agencies like the
\href{https://covid.cdc.gov/covid-data-tracker}{CDC} produced maps of
the outbreak that became much more familiar
(Figure~\ref{fig-viz-covid}).

\begin{figure}[H]

{\centering \includegraphics[width=0.6\textwidth,height=\textheight]{images/viz/covid-hot-spots.png}

}

\caption{\label{fig-viz-covid}Map showing the known locations of
cumulative coronavirus cases by share of the population in each county
(reproduced from the New York Times).}

\end{figure}

These maps make abstract statistics visible: By assigning higher
cumulative case rates to darker colors, we can see at a glance which
areas have been most affected. And we're not limited by the spatial
layout of a map. We're now also used to seeing the horizontal axis
correspond to \emph{time} and the vertical axis correspond to some value
at that time. Curves like the following, showing the 7-day average of
new cases, allow us to see other patterns, like the \emph{rate of
change}. Even though more and more cases accumulate every day, we can
see at a glance the different ``waves'' of cases, and when they peaked
(Figure~\ref{fig-viz-cases}).

\begin{figure}[H]

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{images/viz/covid-cases.png}

}

\caption{\label{fig-viz-cases}7-day average of new reported COVID cases
(reproduced from the New York Times).}

\end{figure}

While these visualizations capture purely descriptive statistics, we
often want our visualizations to answer more specific questions. For
example, we may ask about the effectiveness of vaccinations: how do case
rates differ across vaccinated and unvaccinated populations? In this
case, we may talk about ``breaking out'' a curve by some other variable,
like vaccination status (Figure~\ref{fig-viz-cases2}).

\begin{figure}[H]

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{images/viz/vaccination.png}

}

\caption{\label{fig-viz-cases2}Rates of COVID cases by vaccination
status (reproduced from
\href{https://covid.cdc.gov/covid-data-tracker/\#rates-by-vaccine-status}{\texttt{covid.cdc.gov}}).}

\end{figure}

From this visualization, we can see that unvaccinated individuals are
about 6x more likely to test positive. At the same time, these
visualizations were produced using \emph{observational} data, which
makes it challenging to draw causal inferences. For example, people were
not randomly assigned to vaccination conditions, and those who have
avoided vaccinations may differ in other ways than those who sought out
vaccinations. Additionally, you may have noticed that these
visualizations typically do not give a sense of the raw data, the sample
sizes of each group, or uncertainty about the estimates. In this
chapter, we will explore how to use visualizations to communicate the
results of carefully controlled psychology experiments, which license
stronger causal inferences.

\end{tcolorbox}

\hypertarget{basic-principles-of-confirmatory-visualization}{%
\section{Basic principles of (confirmatory)
visualization}\label{basic-principles-of-confirmatory-visualization}}

In this section, we begin by introducing a few simple guidelines to keep
in mind when making informative visualizations in the context of
experimental psychology.\sidenote{\footnotesize For the purposes of understanding the
  examples in this chapter, it should be sufficient to work through the
  tutorials on data manipulation and visualization in
  \textbf{?@sec-tidyverse} and \textbf{?@sec-ggplot}.} Remember that our
needs may be distinct from other fields, such as journalism or public
policy. You may have seen beautiful and engaging full-page graphics with
small print and a wealth of information. The art of designing and
producing these graphics is typically known as \textbf{infoviz} and
should be distinguished from what we call \textbf{statistical
visualization} (\protect\hyperlink{ref-gelman2013}{Gelman and Unwin
2013}).

Roughly, infoviz aims to construct rich and immersive worlds to visually
explore: a reader can spend hours pouring over the most intricate
graphics and continue to find new and intriguing patterns.
\textbf{Statistical visualization}, on the other hand, aims to crisply
convey the logic of a specific inference at a glance. These
visualizations are the production-ready figures that anchor the results
section of a paper and accompany the key, pre-registered analyses of
interest. In this section, we review several basic principles of making
statistical visualizations. We then return below to the role of
visualization in more exploratory analyses.

\begin{marginfigure}

{\centering \includegraphics{images/viz/viz_infoviz.jpeg}

}

\caption{\label{fig-viz-infoviz}Unlike statistical visualization, which
aims to clearly expose the logic of an experiment at a glance, infoviz
aims to provide a rich world of patterns to explore (reproduced from
\protect\hyperlink{ref-infoviz}{{``Relativity's Reach''} 2015}).}

\end{marginfigure}

\hypertarget{principle-1-show-the-design}{%
\subsection{Principle 1: Show the
design}\label{principle-1-show-the-design}}

There are so many different kinds of graphs (bar graphs, line graphs,
scatter plots, and pie charts) and so many different possible attributes
of those graphs (colors, sizes, line types). How do we begin to decide
how to navigate these decisions? The first principle guiding good
statistical visualizations is to \emph{show the design} of your
experiment.

The first confirmatory plot you should have in mind for your experiment
is the \textbf{design plot}. Analogous to the ``default model'' in
\textbf{?@sec-models}, the design plot should show the key dependent
variable of the experiment, broken down by all of the key manipulations.
Critically, design plots should neither omit particular manipulations
because they didn't yield an effect or include extra covariates because
they seemed interesting after looking at the data. Both of these steps
are the visual analogue of p-hacking! Instead, the design plot is the
``preregistered analysis'' of your visualization: it illustrates a first
look at the estimated causal effects from your experimental
manipulations. In the words of Coppock
(\protect\hyperlink{ref-coppock2019}{2019}), ``visualize as you
randomize''!\sidenote{\footnotesize It can sometimes be a challenge to represent the
  full pattern of manipulations from an experiment in a single plot.
  Below we give some tricks for maximizing the legible information in
  your plot. But if you have tried these and your design plot still
  looks crowded and messy, that could be an indication that your
  experiment is manipulating too many things at once!}

There are strong (unwritten) conventions about how your confirmatory
analysis is expected to map onto graphical elements, and following these
conventions can minimize confusion. Start with the variables you
manipulate, and make sure they are clearly visible. Conventionally, the
primary manipulation of interest (e.g.~condition) goes on the x-axis,
and the primary measurement of interest (e.g.~responses) goes on the
y-axis. Other critical variables of interest (e.g.~secondary
manipulations, demographics) are then assigned to ``visual variables''
(e.g.~color, shape, or size).

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

The visualization library \texttt{ggplot} (see \textbf{?@sec-ggplot})
makes the mapping of variables in the data to visual data. The first
part of a ggplot call is an \texttt{aesthetics} layer:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{aes}\NormalTok{(}
  \AttributeTok{x =}\NormalTok{ ...,}
  \AttributeTok{y =}\NormalTok{ ...,}
  \AttributeTok{color =}\NormalTok{ ...,}
  \AttributeTok{linetype =}\NormalTok{ ...,}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The aesthetics layer serves as a statement of how data are mapped to
``marks'' on the plot. This transparent mapping makes it very easy to
explore different plot types by changing that \texttt{aes} statement, as
we'll see below.

\end{tcolorbox}

As an example, we will consider the data from Stiller, Goodman, and
Frank (\protect\hyperlink{ref-stiller2015}{2015}) that we explored back
in \textbf{?@sec-models}. Because this experiment was a developmental
study, the primary independent variable of interest was the age group of
participants (ages 2, 3, or 4). So age gets assigned to the horizontal
(x) axis. The dependent variable is accuracy: the proportion of trials
that a participant made the correct response (out of 4 trials). So
accuracy goes on the vertical (y) axis. Now, we have two other variables
that we might want to show: the condition (experimental vs.~control) and
the type of stimuli (houses, beds, and plates of pasta). When we think
about it, though, only condition is central to exposing the design.
While we might be interested in whether some types of stimuli are
systematically easier or harder than others, condition is more central
for understanding the \emph{logic} of the study.

\hypertarget{principle-2-facilitate-comparison}{%
\subsection{Principle 2: Facilitate
comparison}\label{principle-2-facilitate-comparison}}

\begin{marginfigure}

{\centering \includegraphics{images/viz/viz_hierarchy.png}

}

\caption{\label{fig-viz-hierarchy}Principles of visual perception can
help guide visualization choices. Reproduced from
(\protect\hyperlink{ref-mackinlay1986automating}{\textbf{mackinlay1986automating?}})
(see also
(\protect\hyperlink{ref-cleveland1984graphical}{\textbf{cleveland1984graphical?}})).}

\end{marginfigure}

Now that you've mapped elements of your design to the figure's axes, how
do you decide which graphical elements to display? You might think:
well, in principle, these assignments are all arbitrary anyway. As long
as we clearly label our choices, it shouldn't matter whether we use
lines, points, bars, colors, textures, or shapes. It's true that there
are many ways to show the same data. But being thoughtful about our
choices can make it much easier for readers to interpret our findings.
The second principle of statistical visualizations is to
\emph{facilitate comparison} along the dimensions relevant to our
scientific questions. It is easier for our visual system to accurately
compare the location of elements (e.g.~noticing that one point is a
certain distance away from another) than to compare their areas or
colors (e.g.~noticing that one point is bigger or brighter than
another). Figure~\ref{fig-viz-hierarchy} shows an ordering of visual
variables based on how accurate our visual system is in making
comparisons.

For example, we \emph{could} start by plotting the accuracy of each age
group as colors (Figure~\ref{fig-viz-prepost2}).

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-prepost2-1.png}

}

\caption{\label{fig-viz-prepost2}A first visualization of the Stiller et
al.~(2015) data.}

\end{figure}

\clearpage

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

To make this (bad) visualization, we used a \texttt{ggplot} function
called \texttt{geom\_tile()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(condition, age\_group) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{rating =} \FunctionTok{mean}\NormalTok{(correct)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ condition, }\AttributeTok{fill =}\NormalTok{ rating)) }\SpecialCharTok{+}
    \FunctionTok{geom\_tile}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\texttt{geom\_tile()} is commonly used to make
\href{https://en.wikipedia.org/wiki/Heat_map}{heat maps}: for each value
of some pair of variables (x, y), it shows a color representing the
magnitude of a third variable (z). While a heat map is a silly way to
visualize the Stiller, Goodman, and Frank
(\protect\hyperlink{ref-stiller2015}{2015}) data, consider using
\texttt{geom\_tile()} when you have a pair of continuous variables, each
taking a large range of values. In these cases, bar plots and line plots
tend to get extremely cluttered, making it hard to see the relationship
between the variables. Heat maps help these relationships to pop out as
clear ``hot'' and ``cold'' regions. For example, in Barnett, Griffiths,
and Hawkins (\protect\hyperlink{ref-barnett2022}{2022}), a heatmap was
used to show a specific range of parameters where an effect of interest
emerged (see Figure~\ref{fig-viz-heatmap}).

\begin{figure}[H]

{\centering \includegraphics[width=0.4\textwidth,height=\textheight]{images/viz/viz_heatmap_example.png}

}

\caption{\label{fig-viz-heatmap}Heatmap showing a specific range of
continuous parameters where an effect emerged (reproduced from Barnett,
Griffiths, and Hawkins (\protect\hyperlink{ref-barnett2022}{2022})).}

\end{figure}

\vspace{3pt}

\end{tcolorbox}

\clearpage

Or we could plot the accuracy of each age group as sizes/areas
(Figure~\ref{fig-viz-prepost3}).

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-prepost3-1.png}

}

\caption{\label{fig-viz-prepost3}Iterating on the Stiller data.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

To make this (bad) visualization, we mapped the rating DV to the
\texttt{size} element in our \texttt{aes()} call.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(condition, age\_group) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{rating =} \FunctionTok{mean}\NormalTok{(correct)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ condition, }\AttributeTok{size =}\NormalTok{ rating)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

These plots allow us to see that one condition is (qualitatively) bigger
than others, but it's hard to tell how much bigger. Additionally, this
way of plotting the data places equal emphasis on age and condition, but
we may instead have in mind particular contrasts, like the \emph{change}
across ages and how that change differs across conditions. An
alternative is to show six bars: three on the left showing the
`experimental' phase and three on the right showing the `control' phase.
Maybe the age groups then are represented as different colors, as in
Figure~\ref{fig-viz-prepost4}.

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-prepost4-1.png}

}

\caption{\label{fig-viz-prepost4}A bar graph of the Stiller data.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

We make bar plots using the \texttt{ggplot} function
\texttt{geom\_bar()}. By default, it creates `stacked' bar plots, where
all values associated with the same x value (here, condition) get
stacked up on top of one another. Stacked bar plots can be useful if,
for example, you're plotting proportions that sum up to 1, or want to
show how some big count is broken down into subcategories. It's also
common to use \texttt{geom\_area()} for this purpose, which connects
adjacent regions. But the more common bar plot used in psychology puts
the bars next to one another, or `dodges' them. To accomplish this, we
use the \texttt{position="dodge"} parameter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(condition, age\_group) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{rating =} \FunctionTok{mean}\NormalTok{(correct)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ condition, }\AttributeTok{y =}\NormalTok{ rating, }\AttributeTok{fill =}\NormalTok{ age\_group)) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

This plot is slightly better: it's easier to compare the heights of bars
than the `blueness' of squares, and mapping age to color draws our eye
to those contrasts. However, we can do even better by noticing that our
experiment was designed to test an \emph{interaction}. That statistic of
interest is a difference of differences. To what extent does the
developmental change in performance on the experimental condition
different from developmental change in performance on the control
condition? Some researchers have gotten proficient at reading off
interactions from bar plots, but they also require a complex set of eye
movements. We have to look at the pattern across the bars on the left,
and then jump over to the bars on the right, and implicitly judge one
difference against the other: the actual statistic isn't explicitly
shown anywhere! What could help facilitate this comparison? Consider the
line plot in Figure~\ref{fig-viz-prepost5}.

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-prepost5-1.png}

}

\caption{\label{fig-viz-prepost5}A line graph of the Stiller data
promotes comparison.}

\end{figure}

The interaction contrast we want to interpret is highlighted visually in
this plot. It is much easier to compare the slopes of two lines than
mentally compute a difference of differences between four bars. A few
corollaries of this principle
\href{https://www.biostat.wisc.edu/~kbroman/presentations/IowaState2013/graphs_combined.pdf}{see
this helpful presentation from Karl Broman}:

\begin{itemize}
\item
  It is easier to compare values that are \emph{adjacent} to one
  another. This is especially important when there are many different
  conditions included on the same plot. If particular sets of conditions
  are of theoretical interest, place them close to one another.
  Otherwise, sort conditions by a meaningful value (rather than
  alphabetically, which is usually the default for plotting software).
\item
  When possible, color-code labels and place them directly next to data
  rather than in a separate legend. Legends force readers to glance back
  and forth to remember what different colors or lines mean.
\item
  When making histograms or density plots, it is challenging to compare
  distributions when they are placed side-by-side. Instead, facilitate
  comparison of distributions by vertically aligning them, or making
  them transparent and placed on the same axes.
\item
  If the scale makes it hard to see important differences, consider
  transforming the data (e.g.~taking the logarithm).
\item
  When making bar plots, be very careful about the vertical y-axis. A
  classic ``misleading visualization'' mistake is to cut off the bottom
  of the bars by placing the endpoint of the y-axis at some arbitrary
  value near the smallest data point. This is misleading because people
  interpret bar plots in terms of the relative \emph{area} of the bars
  (i.e.~the amount of ink taken up by the bar), not just their absolute
  y-values. If the difference between data points is very small relative
  to the overall scale (e.g.~means of 32 vs.~33 on a scale from 0 to
  100), then using a scale with limits of 31 and 33 would make one bar
  look twice as big as the other! Conversely, if plotting means from
  Likert scales with a minimum value of 1, then starting the scale at 0
  would shrink the effective difference! If you must use bars, use the
  natural end points of your measure (see \textbf{?@sec-measurement}).
  Otherwise, consider dropping the bars and allowing the data points to
  `float' with error bars.
\item
  If a key variable from your design is mapped to color, choose the
  color scale carefully. For example, if the variable is binary or
  categorical, choose visually distinct colors to maximize contrast
  (e.g.~black, blue, and orange). If the variable is ordinal or
  continuous, use a color gradient. If there is a natural midpoint
  (e.g.~if some values are negative and some are positive), consider
  using a diverging scale (e.g.~different colors at each extreme).
  Remember also that a portion of your audience may be color-blind.
  Palettes like
  \href{https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html}{viridis}
  have been designed to be colorblind-friendly and also perceptually
  uniform (i.e.~the perceived difference between 0.1 and 0.2 is
  approximately the same as the difference between 0.8 and 0.9).
  Finally, if the same manipulation or variable appears across multiple
  figures in your paper, keep the color mapping consistent: it is
  confusing if ``red'' means something different from figure to figure.
\end{itemize}

\hypertarget{principle-3-show-the-data}{%
\subsection{Principle 3: Show the
data}\label{principle-3-show-the-data}}

Looking at older papers, you may be alarmed to notice how little
information is contained in the graphs. The worst offenders might show
just two bars, representing average values for two conditions. This kind
of plot adds very little beyond a sentence in the text reporting the
means, but it can also be seriously misleading. It hides real variation
in the data, making a noisy effect based on a few data points look the
same as a more systematic one based on a larger sample. Additionally, it
collapses the \emph{distribution} of the data, making a multi-modal
distribution look the same as a unimodal one. The third principle of
modern statistical visualization is to \emph{show the data} and
visualize variability in some form.

The most minimal form of this principle is to \emph{always include error
bars}.\sidenote{\footnotesize And be sure to tell the reader what the error bars
  represent -- a 95\% confidence interval? a standard error of the mean?
  -- without this information, error bars are hard to interpret (see
  Depth box below).} Error bars turn a purely descriptive visualization
into an inferential one. They represent a minimal form of uncertainty
about the possible statistics that might have been observed, not just
the one that was actually observed. Figure~\ref{fig-viz-show-data1}
shows the Stiller data with error bars.

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-show-data1-1.png}

}

\caption{\label{fig-viz-show-data1}Error bars (95\% CIs) added to the
Stiller data line graph.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

A common problem arises when we want to add error bars to a dodged bar
plot. Naively, we'd expect we could just dodge the error bars in the
same way we dodged the bars themselves:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{) }\SpecialCharTok{+}
\FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ ci\_lower, }\AttributeTok{ymax =}\NormalTok{ ci\_upper), }\AttributeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

But this doesn't work! The rationale is kind of technical, but the width
of the error bars is much narrower than the width of the bars, so we
need to manually specify how much to dodge the error bars with the
\texttt{position\_dodge()} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{()) }\SpecialCharTok{+}
\FunctionTok{geom\_errorbar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ ci\_lower, }\AttributeTok{ymax =}\NormalTok{ ci\_upper), }
              \AttributeTok{position =} \FunctionTok{position\_dodge}\NormalTok{(}\AttributeTok{width =} \FloatTok{0.9}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

This does the trick!

\end{tcolorbox}

But we can do even better. By overlaying the distribution of the actual
data points on the same plot, we can give the reader information not
just about the statistical inferences but the underlying data supporting
those inferences. In the case of the Stiller, Goodman, and Frank
(\protect\hyperlink{ref-stiller2015}{2015}) study, data points for
individual trials are binary (correct or incorrect). It's technically
possible to show individual responses as dots at 0s and 1s, but this
doesn't tell us much (we'll just get a big clump of 0s and a big clump
of 1s). The question to ask yourself when `showing the data' is: what
are the theoretically meaningful \emph{units} of variation in the data?
This question is closely related to our discussion of mixed-effects
models in \textbf{?@sec-models}, when we considered which random effects
we should include. Here, a reader is likely to wonder how much variance
was found across \emph{different children} in a given age group. To show
such variation, we aggregate to calculate an accuracy score for each
participant.\sidenote{\footnotesize While participant-level variation is a good
  default, the relevant level of aggregation may differ across designs.
  For example, collective behavior studies may choose to show the data
  point for each \emph{group}. This choice of unit is also important
  when generating error bars: if you have a small number of participants
  but many observations per participant, you are faced with a choice.
  You may either bootstrap over the flat list of all individual
  observations (yielding very small error bars), or you may first
  aggregate within participants (yielding larger error bars that account
  for the fact that repeated observations from the same participant are
  not independent).}

There are many ways of showing the resulting distribution of
participant-level data. For example, a \textbf{boxplot} shows the median
(a horizontal line) in the center of a box extending from the lower
quartile (25\%) to the upper quartile (75\%). Lines then extend out to
the biggest and smallest values (excluding outliers, which are shown as
dots). Figure~\ref{fig-viz-show-data2} gives the boxplots for the
Stiller data, which don't look that informative -- perhaps because of
the coarseness of individual participant averages due to the small
number of trials.

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-show-data2-1.png}

}

\caption{\label{fig-viz-show-data2}Boxplot of the Stiller data.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

In \texttt{ggplot}, we can make box plots using the
\texttt{geom\_boxplot} command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{alpha =} \FloatTok{0.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

A common problem to run into is that \texttt{geom\_boxplot} requires the
variable assigned to \texttt{x} to discrete. If you have discrete levels
of a numeric variable (e.g.~age groups), make sure you've actually
converted that variable to a \texttt{factor}. Otherwise, if it's still
coded as \texttt{numeric}, \texttt{ggplot} will collapse all of the
levels together!

\end{tcolorbox}

It is also common to show the raw data as jittered values with low
transparency. In Figure~\ref{fig-viz-show-data3}), we jitter the points
because many participants have the same numbers (e.g.~50\% and if they
overlap it is hard to see how many points there are.

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-show-data3-1.png}

}

\caption{\label{fig-viz-show-data3}Jittered points representing the data
distribution of the Stiller data.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Adding the jittered points is simple using \texttt{geom\_jitter}, but we
are starting to have a fairly complex plot so maybe it's worth taking
stock of how we get there.

To plot both \emph{condition} means and \emph{participant} means, we
need to create two different data frames. Here \texttt{mss} is a data
frame of means for each participant; \texttt{ms} is a data frame with
means and confidence intervals \emph{across} participants. For this
purpose, we use the \texttt{tidyboot} package and the
\texttt{tidyboot\_mean} function, which gives us bootstrapped 95\%
confidence intervals for the means.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mss }\OtherTok{\textless{}{-}}\NormalTok{ d }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(condition, age\_group, subid) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{rating =} \FunctionTok{mean}\NormalTok{(correct)) }

\NormalTok{ms }\OtherTok{\textless{}{-}}\NormalTok{ mss }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(condition, age\_group) }\SpecialCharTok{|\textgreater{}}
\NormalTok{  tidyboot}\SpecialCharTok{::}\FunctionTok{tidyboot\_mean}\NormalTok{(rating) }
\end{Highlighting}
\end{Shaded}

With these two dataframes in hand, we can now make our \texttt{ggplot}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(ms, }
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ empirical\_stat,}
           \AttributeTok{color =}\NormalTok{ condition, }\AttributeTok{group =}\NormalTok{ condition)) }\SpecialCharTok{+}
    \FunctionTok{geom\_pointrange}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin =}\NormalTok{ ci\_lower, }\AttributeTok{ymax =}\NormalTok{ ci\_upper)) }\SpecialCharTok{+}
    \FunctionTok{geom\_line}\NormalTok{() }\SpecialCharTok{+}
    \FunctionTok{geom\_jitter}\NormalTok{(}\AttributeTok{data =}\NormalTok{ mss, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_group, }\AttributeTok{y =}\NormalTok{ rating, }\AttributeTok{color =}\NormalTok{ condition),}
                \AttributeTok{alpha =} \FloatTok{0.25}\NormalTok{, }\AttributeTok{width =} \FloatTok{0.1}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{ylab}\NormalTok{(}\StringTok{"mean response"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The most noteworthy aspect of this code is that the
\texttt{geom\_jitter} argument doesn't just take a different aesthetic,
it also takes a different dataframe altogether! Mixing dataframes can be
an important tool for creating complex plots.

\end{tcolorbox}

Perhaps the format that takes this principle the furthest is the
so-called ``raincloud plot''
(\protect\hyperlink{ref-allen2019raincloud}{Allen et al. 2019}) shown in
Figure~\ref{fig-viz-raincloud}. A raincloud plot combines the raw data
(the ``rain'') with a smoothed density (the ``cloud'') and a boxplot
giving the median and quartiles of the distribution.

\begin{figure}

{\centering \includegraphics{images/viz/raincloud.png}

}

\caption{\label{fig-viz-raincloud}Example of a raincloud plot,
reproduced from Allen et al.
(\protect\hyperlink{ref-allen2019raincloud}{2019}).}

\end{figure}

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{visualizing-uncertainty-with-error-bars}{%
\section*{Visualizing uncertainty with error
bars}\label{visualizing-uncertainty-with-error-bars}}
\addcontentsline{toc}{section}{Visualizing uncertainty with error bars}

\markright{Visualizing uncertainty with error bars}

One common misconception is that error bars are a measure of variance
\emph{in the data}, like the standard deviation of the response
variable. Instead, they typically represent a measure of precision
extracted from the statistical model. In older papers, for example, it
was common to use the standard error of the mean (SEM; see
\textbf{?@sec-inference}). Remember that this is not the standard
deviation of the data distribution but of the \emph{sampling
distribution} of the mean that is being estimated. Given the central
limit theorem, which tells us that this sampling distribution is
asymptotically normal, it was straightforward to estimate the standard
error analytically using the empirical standard deviation of the data
divided by the square root of the sample size:
\texttt{sd(x)\ /\ sqrt(length(x))}. Error bars based on the SEM often
looked misleadingly small, as they only represent a 68\% interval of the
sampling distribution and go to zero quickly as a function of sample
size. As a result, it became more common to show the 95\% confidence
interval instead: {[}-1.96 \(\times\) SEM, 1.96 \(\times\) SEM{]}.

While these analytic equations remain common, an increasingly popular
alternative is to \emph{bootstrap} confidence intervals. A deep
theoretical understanding of the bootstrap technique is outside the
scope of this text, but you can think of it as a way of deriving a
sampling distribution from your dataset using \emph{simulations} instead
of mathematical derivations about the properties of the sampling
distribution. The bootstrap is a powerfully generic technique,
especially when you want to show error bars for summary statistics that
are more complex than means, where we do not have such convenient
asymptotic guarantees and ``closed-form'' equations. For example,
suppose you are working with a skewed response variable or a dataset
with clear outliers, and you want to estimate medians and quartiles.

Or suppose you want to estimate proportions from categorical data, or a
more \emph{ad hoc} statistic like the AUC (area underneath the curve) in
a hierarchical design where it is not clear how to aggregate across
items or participants in a mixed-effects model. Analytic estimators of
confidence intervals can in principle be derived for these statistics,
subject to different assumptions, but it is often more transparent and
reliable in practice to use the bootstrap. As long as you can write a
code snippet to compute a value from a dataset, you can use the
bootstrap.

\begin{figure}[H]

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{015-viz_files/figure-pdf/fig-viz-boot-1.png}

}

\caption{\label{fig-viz-boot}Three different error bars for the Stiller
data: bootstrapped 95\% confidence intervals (left), standard error of
the mean (middle), and analytically computed confidence intervals
(right).}

\end{figure}

As we can see, the bootstrapped 95\% CI looks similar to the analytic
95\% CI derived from the standard error, except the upper and lower
limits are slightly asymmetric (reflecting outliers in one direction or
another). Of course, the bootstrap is not a silver bullet and can be
abused in particularly small samples. This is because the bootstrap is
fundamentally limited to the sample we run it on. It can be expected to
be reasonably accurate if the sample is reasonably representative of the
population. But at the end of the day, as they say, ``there's no such
thing as a free lunch.'' In other words, we cannot magically pull more
information out of a small sample without making additional assumptions
about the data generating process.

\end{tcolorbox}

\hypertarget{principle-4-maximize-information-minimize-ink}{%
\subsection{Principle 4: Maximize information, minimize
ink}\label{principle-4-maximize-information-minimize-ink}}

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{images/viz/bad-viz1.png}

}

\caption{\label{fig-viz-bad}This figure uses a lot to ink to show three
numbers, for a ``ddi'' of 0.2 (from the Washington Post, 1978; see
Wainer (\protect\hyperlink{ref-wainer1984display}{1984}) for other
examples).}

\end{marginfigure}

Now that we have the basic graphical elements in place to show our
design and data, it might seem like the rest is purely a matter of
aesthetic preference, like choosing a pretty color scheme or font. Not
so.

There are well-founded principles to make the difference between an
effective visualization and a confusing or obfuscating one. Simply put,
we should try to use the simplest possible presentation of the maximal
amount of information: we should maximize the ``data-ink ratio''. To
calculate the amount of information shown, Tufte
(\protect\hyperlink{ref-tufte2001visual}{1983}) suggested a measure
called the ``data density index,'' the ``numbers plotted per square
inch''. The worst offenders have a very low density while also using a
lot of excess ink (e.g., Figure~\ref{fig-viz-bad} and
Figure~\ref{fig-viz-bad2})

\begin{marginfigure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{images/viz/roeder_badviz.jpeg}

}

\caption{\label{fig-viz-bad2}This figure uses complicated 3D ribbons to
compare distributions across four countries (from Roeder
(\protect\hyperlink{ref-roeder1994dna}{1994})). How could the same data
have been presented more legibly?}

\end{marginfigure}

The defaults in modern visualization libraries like \texttt{ggplot}
prevent some of the worst offenses, but are still often suboptimal. For
example: consider whether the visual complexity introduced by the
default grey background and grid lines in Figure~\ref{fig-viz-themes0})
is justified, or whether a more minimal theme would be sufficient (see
the
\href{https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/}{\texttt{ggthemes}}
package for a good collection of themes.

\begin{figure}

{\centering \includegraphics[width=0.8\textwidth,height=\textheight]{015-viz_files/figure-pdf/fig-viz-themes0-1.png}

}

\caption{\label{fig-viz-themes0}Standard ``gray'' themed Stiller
figure.}

\end{figure}

Figure~\ref{fig-viz-themes} shows a slightly more ``styled'' version of
the same plot with labels directly on the plot and a lighter-weight
theme.

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-themes-1.png}

}

\caption{\label{fig-viz-themes}Custom themed Stiller figure with direct
labels.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

To produce the plot above, we've added a few styling elements including:

\begin{itemize}
\tightlist
\item
  The nice and minimal \texttt{theme\_few} from the \texttt{ggthemes}
  package.
\item
  A corresponding color palette (\texttt{scale\_colour\_few}) from the
  same package.
\item
  Manual specification of the aspect ratio
  (\texttt{theme(aspect.ratio\ =\ 1)}).
\item
  Direct labels using the \texttt{geom\_text\_repel} call from
  \texttt{ggrepel}.
\end{itemize}

This last item is a bit trickier to implement. To do so, we select one
set of points to label and feed these in as a filtered data frame. A few
manual arguments customize the placement of the labels (you can spend
hours tweaking this sort of thing by hand).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{geom\_text\_repel}\NormalTok{(}\AttributeTok{data =}\NormalTok{ ms }\SpecialCharTok{|\textgreater{}} \FunctionTok{filter}\NormalTok{(age\_group }\SpecialCharTok{==} \StringTok{"(3,4]"}\NormalTok{),}
                \FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ condition, }\AttributeTok{colour =}\NormalTok{ condition), }
                \AttributeTok{nudge\_y =}\NormalTok{ .}\DecValTok{1}\NormalTok{, }\AttributeTok{nudge\_x =}\NormalTok{ .}\DecValTok{5}\NormalTok{, }\AttributeTok{segment.color =} \ConstantTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

Here are a few final tips for making good confirmatory visualizations:

\begin{itemize}
\item
  Make sure the font size of all text in your figures is legible and no
  smaller than other text in your paper (e.g.~10pt). This change may
  require, for example, making the axis breaks sparser, rotating text,
  or changing the aspect ratio of the figure.
\item
  Another important tool to keep in your visualization arsenal is the
  \textbf{facet plot}. When your experimental design becomes more
  complex, consider breaking variables out into a \emph{grid} of facets
  instead of packing more and more colors and line-styles onto the same
  axis. In other words, while higher information density is typically a
  good thing, you want to aim for the sweet spot before it becomes too
  dense and confusing. Remember Principle 2. When there is too much
  going on in every square inch, it is difficult to guide your reader's
  eye to the comparisons that actually matter, and spreading it out
  across facets gives you additional control over the salient patterns.
\item
  Sometimes these principles come into conflict, and you may need to
  prioritize legibility over, for example, showing all of the data. For
  example, suppose there is an outlier orders of magnitude away from the
  summary statistics. If the axis limits are zoomed out to show that
  point, then most of the plot will be blank space! It is reasonable to
  decide that it is not worth compressing the key statistical question
  of your visualization into the bottom centimeter just to show one
  point. It may suffice to truncate the axes and note in the caption
  that a single point was excluded.
\item
  Fix the axis labels. A common mistake is to keep the default shorthand
  you used to name variables in your plotting software instead of more
  descriptive labels (e.g., ``RT'' instead of ``Reaction Time''). Use
  consistent terminology for different manipulations and measures in the
  main text and figures. If anything might be unclear, explain it in the
  caption.
\item
  Different audiences may require different levels of detail. Sometimes
  it is better to collapse over secondary variables (even if they are
  included in your statistical models) in order to control the density
  of the figure and draw attention to the key question of interest.
\end{itemize}

\hypertarget{exploratory-visualization}{%
\section{Exploratory visualization}\label{exploratory-visualization}}

So far in this chapter we have focused on principles of
\emph{confirmatory} data visualization: how to make production-quality
figures that convey the key pre-registered analyses without hiding
sources of variability or misleading readers about the reliability of
the results. Yet this is only one role that data visualization plays
when doing science. An equally important role is called
\emph{exploratory visualization}: the more routine practice of
understanding one's own data by visualizing it. This role is analogous
to the sense of exploratory data analyses discussed in
\textbf{?@sec-prereg}. We typically do not pre-register exploratory
visualizations, and when we decide to include them in a paper they are
typically in the service of a secondary argument (e.g., checking the
robustness of an effect or validating that some assumption is
satisfied).

This kind of visualization plays a ubiquitous role in a researcher's
day-to-day activities. While confirmatory visualization is primarily
audience-driven and concerned with visual communication, exploratory
visualization is first and foremost a ``cognitive tool'' for the
researcher. The first time we load in a new dataset, we start up a new
feedback loop --- we ask ourselves questions and answer them by making
visualizations. These visualizations then raise further questions and
are often our best tool for debugging our code. In this section, we
consider some best practices for exploratory visualization.

\hypertarget{examining-distributional-information}{%
\subsection{Examining distributional
information}\label{examining-distributional-information}}

\begin{figure}

\sidecaption{\label{fig-viz-anscombe}Anscombe's quartet
(\protect\hyperlink{ref-anscombe1973graphs}{Anscombe 1973}).}

{\centering \includegraphics{images/viz/viz_anscombe.png}

}

\end{figure}

The primary advantage of exploratory visualization -- the reason it is
uniquely important for data science -- is that it gives us access to
holistic information about the distribution of the data, that cannot be
captured in any single summary statistic. The most famous example is
known as ``Anscombe's quartet,'' a set of four datasets with identical
statistics (Figure~\ref{fig-viz-anscombe}). They have the same means,
the same variances, the same correlation, the same regression line, and
the same \(R^2\) value. Yet when they are plotted, they reveal striking
structural differences. The first looks like a noisy linear relationship
-- the kind of idealized relationship we imagine when we imagine a
regression line. But the second is a perfect quadratic arc, the third is
a perfectly noiseless line with a single outlier, and the fourth is
nearly categorical: every observation except one shares exactly the same
x-value.

If our analyses are supposed to help us distinguish between different
data-generating processes, corresponding to different psychological
theories, it is clear that these four datasets would correspond to
dramatically different theories even though they share the same
statistics. Of course, there are arbitrarily many datasets with the same
statistics, and most of these differences don't matter (this is why they
are called ``summary'' statistics, after all!).
Figure~\ref{fig-viz-datasaurus} shows just how bad things can get when
we rely on summary statistics. When we operationalize a theory's
predictions in terms of a single statistic (e.g., a difference between
groups or a regression coefficient) we can lose track of everything else
that may be going on. Good visualizations force us to zoom out and take
in the bigger picture.

\begin{figure}

\sidecaption{\label{fig-viz-datasaurus}Originally inspired by a figure
constructed by
\href{http://www.thefunctionalart.com/2016/08/download-datasaurus-never-trust-summary.html}{Cairo
(\protect\hyperlink{ref-datasaurus}{n.d.})} using the
\href{http://robertgrantstats.co.uk/drawmydata.html}{drawMyData} tool,
we can actually construct an arbitrary number of different graphs with
exactly the same statistics
(\protect\hyperlink{ref-matejka2017same}{Matejka and Fitzmaurice 2017};
\protect\hyperlink{ref-murray2021generating}{Murray and Wilson 2021}).
This set, known as the
\href{https://www.autodesk.com/research/publications/same-stats-different-graphs}{The
Datasaurus Dozen}, even has the same set of boxplots.}

{\centering \includegraphics{images/viz/datasaurus.png}

}

\end{figure}

\hypertarget{data-diagnostics}{%
\subsection{Data diagnostics}\label{data-diagnostics}}

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{distributional-gorillas-in-our-midst.}{%
\section*{{[}Distributional{]} gorillas in our
midst.}\label{distributional-gorillas-in-our-midst.}}
\addcontentsline{toc}{section}{{[}Distributional{]} gorillas in our
midst.}

\markright{{[}Distributional{]} gorillas in our midst.}

Many data scientists don't bother checking what their data looks like
before proceeding to test specific hypotheses. Yanai and Lercher
(\protect\hyperlink{ref-yanai2020}{2020}) cleverly designed an
artificial dataset for their students to test for such oversight. Each
row of the dataset contained an individual's body mass index (BMI) and
the number of steps they walked on a given day. While the spreadsheet
looked innocuous, the data was constructed such that simply plotting the
raw data revealed a picture of a gorilla. One group of 19 students was
given an explicit set of hypotheses to test (e.g.~about the relationship
between BMI and steps). Fourteen of these students failed to notice a
gorilla, suggesting that they evaluated these hypotheses without ever
visualizing their data. Another group of 14 students were simply asked
what, if anything, they could conclude (without being given explicit
hypotheses). More of these students apparently made the visualization,
but five of them still failed to notice the gorilla
(Figure~\ref{fig-viz-gorilla})!

\begin{figure}[H]

{\centering \includegraphics[width=0.5\textwidth,height=\textheight]{images/viz/gorilla.png}

}

\caption{\label{fig-viz-gorilla}A dataset was constructed by Yanai and
Lercher (\protect\hyperlink{ref-yanai2020}{2020}) which revealed a
picture of a gorilla when the raw data were plotted.}

\end{figure}

While it may not be surprising that a group of students would take the
shortest path to completing their assignment, similar concerns have been
raised in much more serious cases concerning how experienced researchers
could fail to notice obviously fraudulent data. For example, when
{``Evidence of Fraud in an Influential Field Experiment about
Dishonesty''} (\protect\hyperlink{ref-datacolada}{2021}) made a simple
histogram of the car mileage data reported in Shu et al.
(\protect\hyperlink{ref-shu2012signing}{2012}; released publicly by
\protect\hyperlink{ref-kristal2020signing}{Kristal et al. 2020}), they
were immediately able to observe that it followed a perfectly uniform
distribution, truncated at exactly 50,000 miles
(Figure~\ref{fig-viz-uniform}). Given a little thought, this pattern
should be extremely puzzling. Over a given period of time, we would
typically expect something more bell-shaped: a small number of people
will drive very little (e.g., 1000 miles), a small number of people will
drive a lot (e.g., 50,000 miles), and most people will fall between
these tails. So it is highly surprising to find exactly the same number
of drivers in every mileage bin. While further specialized analyses
revealed additional evidence of fraud (e.g.~based on patterns of
rounding and pairs of duplicated data points), this humble histogram was
already enough to set off alarm bells. A recurring regret raised by the
co-authors of this paper is that they never thought to make this
visualization before reporting their statistical tests.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\textwidth,height=\textheight]{images/viz/data_colada_uniform.png}

}

\caption{\label{fig-viz-uniform}A suspiciously uniform distribution
abruptly cutting off at 50k miles. Ring the alarm!}

\end{figure}

Our data are always messier than we expect. There might be a bug in our
coding scheme, a column might be mislabeled, or might contain a range of
values that we didn't expect. Maybe our design wasn't perfectly
balanced, or something went wrong with a particular participant's
keyboard presses. Most of the time, it's not tractable to manually
scroll through our raw data looking for such problems. Visualization is
our first line of defense for the all-important process of running
``data diagnostics.'' If there is a weird artifact in our data, it will
pop out if we just make the right visualizations.

\end{tcolorbox}

So which visualizations should we start with? The best practice is to
always start by making histograms of the raw data. As an example, let's
consider the rich and interesting dataset shared by Blake, McAuliffe,
and colleagues (\protect\hyperlink{ref-blake2015ontogeny}{2015}) in
their article ``Ontogeny of fairness in seven societies.'' This article
studies the emergence of children's reasoning about fairness -- both
when it benefits them and when it harms them -- across cultures.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

If you want to follow along with this example at home, you can load the
data from our repository!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{repo }\OtherTok{\textless{}{-}} \StringTok{"https://raw.githubusercontent.com/langcog/experimentology/main/"}
\NormalTok{fairness }\OtherTok{\textless{}{-}}\NormalTok{ readr}\SpecialCharTok{::}\FunctionTok{read\_csv}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(repo, }\StringTok{"data/viz/ontogeny\_of\_fairness.csv"}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{trial\_num =} \FunctionTok{as.numeric}\NormalTok{(}\FunctionTok{gsub}\NormalTok{(}\StringTok{".(}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{d+)"}\NormalTok{, }\StringTok{"}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{1"}\NormalTok{, trial)),}
         \AttributeTok{trial\_type =} \FunctionTok{factor}\NormalTok{(eq.uneq,}
                             \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"E"}\NormalTok{,}\StringTok{"U"}\NormalTok{),}
                             \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Equal"}\NormalTok{,}\StringTok{"Unequal"}\NormalTok{)),}
         \AttributeTok{condition =} \FunctionTok{factor}\NormalTok{(condition,}
                            \AttributeTok{levels =} \FunctionTok{c}\NormalTok{(}\StringTok{"AI"}\NormalTok{,}\StringTok{"DI"}\NormalTok{),}
                            \AttributeTok{labels =} \FunctionTok{c}\NormalTok{(}\StringTok{"Advantageous"}\NormalTok{,}\StringTok{"Disadvantageous"}\NormalTok{))) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{arrange}\NormalTok{(country, condition, actor.id, trial\_num)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

In this study, pairs of children played the ``inequity game'': they sat
across from one another and were given a particular allocation of
snacks. On some trials, each participant was allocated the same amount
(Equal trials) and on some trials they were allocated different amounts
(Unequal trials). One participant was chosen to be the ``actor'' and got
to choose whether to accept or reject the allocation: in the case of
rejection, neither participant got anything. The critical manipulation
was between two forms of inequity. Some pairs were assigned to the
Disadvantageous condition, where the actor was allocated less than their
partner on Unequal trials (e.g.~1 vs.~4). Others were assigned to the
Advantageous condition, where they were allocated more (e.g.~4 vs.~1).

The confirmatory \textbf{design plot} for this study would focus on
contrasting developmental trajectories for Advantageous
vs.~Disadvantageous inequality. However, this is a complex, multivariate
dataset, including 866 pairs from different age groups and different
testing sites across the world which used subtly different protocols.
How might we go about the process of exploratory visualization for this
dataset?

\hypertarget{plot-data-collection-details}{%
\subsection{Plot data collection
details}\label{plot-data-collection-details}}

Let's start by getting a handle on some of the basic sample
characteristics. For example, how many participants were in each age bin
(Figure~\ref{fig-viz-blake3})?

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-blake3-1.png}

}

\caption{\label{fig-viz-blake3}Participants by age in the Blake data.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Exploratory histograms are often a combination of an aggregation step
and a plotting step. In the aggregation step, we make use of the
convenience \texttt{count} function, which gives the number (\texttt{n})
of rows in a particular grouping. Here we \texttt{count} twice in order
to get first one row per participant and then count the number of
participants within each age group.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ps\_by\_age }\OtherTok{\textless{}{-}}\NormalTok{ d }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age =} \FunctionTok{floor}\NormalTok{(actor.age.years)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(age, actor.id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(age) }
\end{Highlighting}
\end{Shaded}

And then we plot using \texttt{ggplot}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(ps\_by\_age, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ n)) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{18}\NormalTok{) }\SpecialCharTok{+}
\NormalTok{    ggthemes}\SpecialCharTok{::}\FunctionTok{theme\_few}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

An alternative (perhaps more elegant) workflow here would be to use a
histogram:

\begin{verbatim}
ps_by_age <- d |>
  mutate(age = floor(actor.age.years)) |>
  count(age, actor.id) 
  
ggplot(ps_by_age, aes(x = age)) +
    geom_histogram(binwidth = 1)
\end{verbatim}

Histograms are intended by \texttt{ggplot} to be for continuous data,
however, and so they don't give the discrete bars that our earlier
workflow did.

\end{tcolorbox}

How many participants were included from each country
(Figure~\ref{fig-viz-blake5})?

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-blake5-1.png}

}

\caption{\label{fig-viz-blake5}Participants by country in the Blake
data.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Here we are going to make things even terser and use a pipe chain that
\emph{includes} the ggplot, just so we are writing only a single call to
produce our plot. It's up to you whether you think this enhances the
readability of your code or decreases it. We find that it's sometimes
useful when you don't plan on keeping the intermediate data frame for
any other use than plotting.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(country, actor.id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(country) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{country =} \FunctionTok{fct\_reorder}\NormalTok{(country, }\SpecialCharTok{{-}}\NormalTok{n)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ country, }\AttributeTok{y =}\NormalTok{ n)) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

If you use this technique, be careful to pipe
(\texttt{\textbar{}\textgreater{}} or \texttt{\%\textgreater{}\%})
between \texttt{tidyverse} operators but add (\texttt{+}) the
\texttt{ggplot} operators!

The only other trick to point out here is that we use the
\texttt{fct\_reorder} call to order the levels of the \texttt{country}
factor in descending order. This function is found in the very useful
\texttt{forcats} package of the \texttt{tidyverse}, which contains all
sorts of functions for working with factors.

\end{tcolorbox}

Are ages roughly similar across each country
(Figure~\ref{fig-viz-blake6})?

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-blake6-1.png}

}

\caption{\label{fig-viz-blake6}Age distribution across countries in the
Blake data.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

This next plot simply combines the grouping factors of each of the last
two plots, and uses \texttt{facet\_wrap} to show a separate histogram by
country:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age =} \FunctionTok{floor}\NormalTok{(actor.age.years)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(country, age, actor.id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(country, age) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{country =} \FunctionTok{fct\_reorder}\NormalTok{(country, }\SpecialCharTok{{-}}\NormalTok{n)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ n)) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{facet\_wrap}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ country) }\SpecialCharTok{+}
    \FunctionTok{xlim}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{18}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

These exploratory visualizations help us read off some descriptive
properties of the sample. For example, we can see that age ranges differ
somewhat across sites: the maximum age is 11 in India but 15 in Mexico.
We can also see that age groups are fairly imbalanced: in Canada, there
are 18 11-year-olds but only 5 6-year-olds.

None of these properties are problematic, but seeing them gives us a
degree of awareness that could shape our downstream analytic decisions.
For example, if we did not appropriately model random effects, our
estimates would be dominated by the countries with larger sample sizes.
And if we were planning to compare specific groups of 6-year-olds (for
some reason), this analysis would be underpowered.

\hypertarget{explorating-distributions}{%
\subsection{Explorating distributions}\label{explorating-distributions}}

Now that we have a handle on the sample, let's get a sense of the
dependent variable: the participant's decision to accept or reject the
allocation. Before we start taking means, let's look at how the
``rejection rate'' variable is distributed. We'll aggregate at the
participant level, and check the frequency of different rejection rates,
overall (Figure~\ref{fig-viz-blake7}).

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-blake7-1.png}

}

\caption{\label{fig-viz-blake7}Rejection rates in the Blake data.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Rejection rate is a continuous variable, so we switch to using a
histogram in this case, choosing .05 as a reasonable bin width to see
the distribution.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(eq.uneq)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(actor.id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{reject =} \FunctionTok{mean}\NormalTok{(reject, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ reject)) }\SpecialCharTok{+}
    \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =}\NormalTok{ .}\DecValTok{05}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

We notice that many participants (27\%) never reject in the entire
experiment. This kind of ``zero-inflated'' distribution is not uncommon
in psychology, and may warrant special consideration when designing the
statistical model. We also notice that there is clumping around certain
values. This clumping leads us to check how many trials each participant
is completing (Figure~\ref{fig-viz-blake8}).

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-blake8-1.png}

}

\caption{\label{fig-viz-blake8}Trials per participant in the Blake
data.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

This histogram is very similar to the ones above; however, we now
\texttt{count} twice, first getting the trial counts for each
participant and then counting how many times each count occurs overall!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(eq.uneq)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(actor.id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{count}\NormalTok{(n) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ n, }\AttributeTok{y =}\NormalTok{ nn)) }\SpecialCharTok{+}
    \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"\# trials"}\NormalTok{, }\AttributeTok{y =} \StringTok{"proportion of participants"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

There's some variation here: most participants completed 17 trials, but
some participants completed 8 trials, and a small number of participants
have 14 or 15. Given the logistical complexity of large multi-site
studies, it is common to have some changes in experimental protocol
across data collection. Indeed, looking at the supplement for the study,
we see that while India and Peru had 12 trials, additional trials were
added at the other sites. In a design where the number of trials was
carefully controlled, seeing unexpected numbers here (like the 14 or 15
trial bins) are clues that something else may be going on in the data.
In this case, it was a small number of trials with missing data. More
generally, seeing this kind of signal in a visualization of our own data
typically leads us to look up the participant IDs in these bins and
manually inspect their data to see what might be going on.

\hypertarget{hypothesis-driven-exploration}{%
\subsection{Hypothesis-driven
exploration}\label{hypothesis-driven-exploration}}

Finally, we can make a few versions of the design plot that are broken
out by different variables. Let's start by just looking at the data from
the largest site (Figure~\ref{fig-viz-blake9}).

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-blake9-1.png}

}

\caption{\label{fig-viz-blake9}Rejection rates in the US data from
Blake, plotted by age.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Here, we are using \texttt{geom\_smooth()} to overlay regression trends
over the raw data. \texttt{geom\_smooth()} takes a number of different
options corresponding to different smoothing techniques. Non-parametric
smoothing can be a good choice for exploratory visualizations if you
have a lot of data and want to make minimal assumptions about the form
of the trend.

Here, however, we show the linear regression trend,
\texttt{geom\_smooth(method\ =\ "lm")}, which better corresponds to the
predictions of the study and the statistical model being used (see
\textbf{?@sec-models}). Other regression forms can be specified with the
\texttt{formula} argument. For example, we could show quadratic
smoothing with
\texttt{geom\_smooth(method\ =\ "lm",\ formula\ =\ y\ \textasciitilde{}\ poly(x,\ 2))}.
The form of smoothing you use may differ across exploratory and
confirmatory visualizations. In a confirmatory visualization --- if you
are going to include a smoothing curve --- it is typically best to use
the one specified by your statistical model, as the slopes will
correspond to the inferences being testing.

We begin by making a summary dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ms }\OtherTok{\textless{}{-}}\NormalTok{ d }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(reject)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(country, trial\_type, condition, age, actor.id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{reject =} \FunctionTok{mean}\NormalTok{(reject, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(country, trial\_type, condition, age) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{reject =} \FunctionTok{mean}\NormalTok{(reject, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{n =} \FunctionTok{n}\NormalTok{()) }
\end{Highlighting}
\end{Shaded}

Then we can create the visualization (labels and styling omitted for
clarity):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{filter}\NormalTok{(ms, country }\SpecialCharTok{==} \StringTok{"US"}\NormalTok{),}
       \FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age, }\AttributeTok{y =}\NormalTok{ reject, }\AttributeTok{col =}\NormalTok{ country)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{size =}\NormalTok{ n)) }\SpecialCharTok{+}
    \FunctionTok{facet\_grid}\NormalTok{(trial\_type }\SpecialCharTok{\textasciitilde{}}\NormalTok{ condition) }\SpecialCharTok{+}
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We often find it convenient to filter the summary dataset in the
plotting call, so that we can reuse it again.

\end{tcolorbox}

Figure~\ref{fig-viz-blake9} is not a figure we'd put in a paper, but it
helps us get a sense of the pattern in the data. There appears to be an
age trend that's specific to the Unequal trials, with rejection rates
rising over time (compared to roughly even or decreasing rates in the
Equal trials). Meanwhile, rejection rates for the Disadvantageous group
also seem slightly higher than those in the Advantageous group. Now
let's re-bin the data into two-year age groups so that individual point
estimates are a bit more reliable, and add the other countries back
in.\sidenote{\footnotesize Binning data is a trick that we often use for reducing
  complexity in a plot when data are noisy. It should be used with care,
  however, since different binning decisions can sometimes lead to
  different conclusions. Here we tried several binning intervals and
  decided that two-year age bins showed the underlying trends pretty
  well.}

\begin{figure}

{\centering \includegraphics{015-viz_files/figure-pdf/fig-viz-blake10-1.png}

}

\caption{\label{fig-viz-blake10}Rejection rates by age for all data in
the Blake dataset.}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Despite the difference between the plot above and this one, the code to
produce them is actually very similar. The only difference is the
creation of the binned variable and a slight shift of aesthetic and
faceting variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ms }\OtherTok{\textless{}{-}}\NormalTok{ d }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(reject)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{age\_binned =} \FunctionTok{floor}\NormalTok{(actor.age.years}\SpecialCharTok{/}\DecValTok{2}\NormalTok{)}\SpecialCharTok{*}\DecValTok{2}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(country, trial\_type, condition, age\_binned, actor.id) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{reject =} \FunctionTok{mean}\NormalTok{(reject, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{group\_by}\NormalTok{(country, trial\_type, condition, age\_binned) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{reject =} \FunctionTok{mean}\NormalTok{(reject, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{),}
            \AttributeTok{n =} \FunctionTok{n}\NormalTok{()) }

\FunctionTok{ggplot}\NormalTok{(ms, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ age\_binned, }\AttributeTok{y =}\NormalTok{ reject, }\AttributeTok{col =}\NormalTok{ condition)) }\SpecialCharTok{+}
    \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{size =}\NormalTok{ n), }\AttributeTok{alpha =}\NormalTok{ .}\DecValTok{5}\NormalTok{) }\SpecialCharTok{+}
    \FunctionTok{facet\_grid}\NormalTok{(trial\_type }\SpecialCharTok{\textasciitilde{}}\NormalTok{ country) }\SpecialCharTok{+}
    \FunctionTok{geom\_smooth}\NormalTok{(}\AttributeTok{method=}\StringTok{"lm"}\NormalTok{, }\AttributeTok{se =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

Figure~\ref{fig-viz-blake10} is now looking much closer to a
quick-and-dirty version of a ``design plot'' we might include in a
paper. The DV (rejection rate) is on the y-axis, and the primary
variable of interest (age) is on the x-axis. Other elements of the
design (country and trial type) are mapped to color and facets,
respectively.

\hypertarget{visualization-as-debugging}{%
\subsection{Visualization as
debugging}\label{visualization-as-debugging}}

The point of exploratory visualization is to converge toward a better
understanding of what's going on in your data. As you iterate through
different exploratory visualizations, \emph{stay vigilant}! Think about
what you expect to see before making the plot, then ask yourself whether
you got what you expected. You can think of this workflow as a form of
``visual debugging''. You might notice a data point with an impossible
value, such as a proportion greater than 1 or a reaction time less than
0. Or you might notice weird clusters or striations, which might
indicate heterogeneity in data entry (perhaps different coders used
slightly different rubrics or rounded in different ways). You might
notice that an attribute is missing for some values, and trace it back
to a bug reading in the data or merging data frames (maybe there was a
missing comma in our \texttt{csv} file). If you see anything that looks
weird, track it down until you understand why it's happening. Bugs that
are subtle and invisible in other parts of the analysis pipeline will
often pop out as red flags in visualizations.

\hypertarget{chapter-summary-visualization}{%
\section{Chapter summary:
Visualization}\label{chapter-summary-visualization}}

This chapter has given a short review of the principles of data
visualization, especially focusing on the needs of experimental
psychology, which are often quite different than those of other fields.
We particularly focused on the need to make visualization part of the
experimenter's analytic workflow. Picking up the idea of a ``default
model'' from \textbf{?@sec-models}, we discussed a default ``design
plot'' that reflects the key choices made in the experimental design.
Within this framework, we then discussed different visualizations of
distribution and variability that better align our graphics with the
principles of measurement and attention to raw data that we have been
advocating throughout.

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Choose a recent piece of research that you've heard about and try to
  sketch the ``design plot'' with pencil and paper. What does and
  doesn't work? How does your sketch differ from the visualizations in
  the paper?
\item
  The ``design plot'' idea that we've discussed here can run into
  problems when an experimental design is too complex to show on a
  single plot. Imagine you had data from a trial of attention deficit
  hyperactivity disorder (ADHD) treatment that manipulated both whether
  a medication was given and whether patients received therapy in a
  crossed design. The researchers measured two different outcomes:
  parent report symptom severity and teacher report symptom severity in
  four different time-points (baseline, 3 months, 6 months, and 9
  months). How could you show the data from such an experiment in a
  transparent way?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

There are many good introductions to data visualization. Here are two
social-science focused books whose advice we agree with and that also
contain a lot of practical information and helpful R code for the same
packages we use here.

\begin{itemize}
\item
  Healy, K. (2019). \emph{Data Visualization: A Practical Introduction}.
  Princeton University Press. Princeton University Press. Available free
  online at \url{https://socviz.co}.
\item
  Wilke, C. O. (2019). \emph{Fundamentals of Data Visualization}.
  O'Reilly Media. Available free online at
  \url{https://clauswilke.com/dataviz/}.
\end{itemize}

For a more classical treatment, see:

\begin{itemize}
\item
  Tukey, J. W. (1977). \emph{Exploratory data analysis}. Pearson.
\item
  Tufte, E. R. (1997). \emph{The Visual Display of Quantitative
  Information}. Graphics Press.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-20}{%
\section*{References}\label{bibliography-20}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-20}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-allen2019raincloud}{}}%
Allen, Micah, Davide Poggiali, Kirstie Whitaker, Tom Rhys Marshall, and
Rogier A Kievit. 2019. {``Raincloud Plots: A Multi-Platform Tool for
Robust Data Visualization.''} \emph{Wellcome Open Research} 4.

\leavevmode\vadjust pre{\hypertarget{ref-anscombe1973graphs}{}}%
Anscombe, Francis J. 1973. {``Graphs in Statistical Analysis.''}
\emph{The American Statistician} 27 (1): 17--21.

\leavevmode\vadjust pre{\hypertarget{ref-barnett2022}{}}%
Barnett, Samuel A, Thomas L Griffiths, and Robert D Hawkins. 2022. {``A
Pragmatic Account of the Weak Evidence Effect.''} \emph{Open Mind},
1--14.

\leavevmode\vadjust pre{\hypertarget{ref-blake2015ontogeny}{}}%
Blake, PR, K McAuliffe, J Corbit, TC Callaghan, O Barry, A Bowie, L
Kleutsch, et al. 2015. {``The Ontogeny of Fairness in Seven
Societies.''} \emph{Nature} 528 (7581): 258--61.

\leavevmode\vadjust pre{\hypertarget{ref-borner2019data}{}}%
Brner, Katy, Andreas Bueckle, and Michael Ginda. 2019. {``Data
Visualization Literacy: Definitions, Conceptual Frameworks, Exercises,
and Assessments.''} \emph{Proceedings of the National Academy of
Sciences} 116 (6): 1857--64.

\leavevmode\vadjust pre{\hypertarget{ref-brody2000map}{}}%
Brody, Howard, Michael Russell Rip, Peter Vinten-Johansen, Nigel Paneth,
and Stephen Rachman. 2000. {``Map-Making and Myth-Making in Broad
Street: The London Cholera Epidemic, 1854.''} \emph{The Lancet} 356
(9223): 64--68.

\leavevmode\vadjust pre{\hypertarget{ref-datasaurus}{}}%
Cairo, A. n.d. {``Download the Datasaurus: Never Trust Summary
Statistics Alone; Always Visualize Your Data.''}
\url{http://www.thefunctionalart.com/2016/08/downloaddatasaurus-never-trust-summary.html}.

\leavevmode\vadjust pre{\hypertarget{ref-coppock2019}{}}%
Coppock, Alexander. 2019. {``Visualize as You Randomize: Design-Based
Statistical Graphs for Randomized Experiments.''} In, edited by James N.
Druckman and Donald P. Green.

\leavevmode\vadjust pre{\hypertarget{ref-datacolada}{}}%
{``Evidence of Fraud in an Influential Field Experiment about
Dishonesty.''} 2021. \url{https://datacolada.org/98}.

\leavevmode\vadjust pre{\hypertarget{ref-friendly2021history}{}}%
Friendly, Michael, and Howard Wainer. 2021. \emph{A History of Data
Visualization and Graphic Communication}. Harvard University Press.

\leavevmode\vadjust pre{\hypertarget{ref-gelman2013}{}}%
Gelman, Andrew, and Antony Unwin. 2013. {``Infovis and Statistical
Graphics: Different Goals, Different Looks.''} \emph{J. Comput. Graph.
Stat.} 22 (1): 2--28.

\leavevmode\vadjust pre{\hypertarget{ref-halliday2001death}{}}%
Halliday, Stephen. 2001. {``Death and Miasma in Victorian London: An
Obstinate Belief.''} \emph{British Medical Journal} 323 (7327):
1469--71.

\leavevmode\vadjust pre{\hypertarget{ref-kristal2020signing}{}}%
Kristal, Ariella S, Ashley V Whillans, Max H Bazerman, Francesca Gino,
Lisa L Shu, Nina Mazar, and Dan Ariely. 2020. {``Signing at the
Beginning Versus at the End Does Not Decrease Dishonesty.''}
\emph{Proceedings of the National Academy of Sciences} 117 (13):
7103--7.

\leavevmode\vadjust pre{\hypertarget{ref-matejka2017same}{}}%
Matejka, Justin, and George Fitzmaurice. 2017. {``Same Stats, Different
Graphs: Generating Datasets with Varied Appearance and Identical
Statistics Through Simulated Annealing.''} In \emph{Proceedings of the
2017 CHI Conference on Human Factors in Computing Systems}, 1290--94.

\leavevmode\vadjust pre{\hypertarget{ref-murray2021generating}{}}%
Murray, Lori L, and John G Wilson. 2021. {``Generating Data Sets for
Teaching the Importance of Regression Analysis.''} \emph{Decision
Sciences Journal of Innovative Education} 19 (2): 157--66.

\leavevmode\vadjust pre{\hypertarget{ref-infoviz}{}}%
{``Relativity's Reach.''} 2015. \emph{Scientific American}.

\leavevmode\vadjust pre{\hypertarget{ref-roeder1994dna}{}}%
Roeder, Kathryn. 1994. {``DNA Fingerprinting: A Review of the
Controversy.''} \emph{Statistical Science}, 222--47.

\leavevmode\vadjust pre{\hypertarget{ref-shu2012signing}{}}%
Shu, Lisa L, Nina Mazar, Francesca Gino, Dan Ariely, and Max H Bazerman.
2012. {``Signing at the Beginning Makes Ethics Salient and Decreases
Dishonest Self-Reports in Comparison to Signing at the End.''}
\emph{Proceedings of the National Academy of Sciences} 109 (38):
15197--200.

\leavevmode\vadjust pre{\hypertarget{ref-snow1855mode}{}}%
Snow, John. 1855. \emph{On the Mode of Communication of Cholera}. John
Churchill.

\leavevmode\vadjust pre{\hypertarget{ref-stiller2015}{}}%
Stiller, Alex J, Noah D Goodman, and Michael C Frank. 2015. {``Ad-Hoc
Implicature in Preschool Children.''} \emph{Language Learning and
Development} 11 (2): 176--90.

\leavevmode\vadjust pre{\hypertarget{ref-tufte2001visual}{}}%
Tufte, E. R. 1983. \emph{The Visual Display of Quantitative
Information}. Graphics Press.

\leavevmode\vadjust pre{\hypertarget{ref-wainer1984display}{}}%
Wainer, Howard. 1984. {``How to Display Data Badly.''} \emph{The
American Statistician} 38 (2): 137--47.

\leavevmode\vadjust pre{\hypertarget{ref-yanai2020}{}}%
Yanai, I, and M Lercher. 2020. {``A Hypothesis Is a Liability.''}
\emph{Genome Biology} 21 (1): 231--31.

\leavevmode\vadjust pre{\hypertarget{ref-zacks2020designing}{}}%
Zacks, Jeffrey M, and Steven L Franconeri. 2020. {``Designing Graphs for
Decision-Makers.''} \emph{Policy Insights from the Behavioral and Brain
Sciences} 7 (1): 52--63.

\end{CSLReferences}

\hypertarget{sec-meta}{%
\chapter{Meta-analysis}\label{sec-meta}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Discuss the benefits of synthesizing evidence across studies
\item
  Conduct a simple fixed- or random-effects meta analysis
\item
  Reason about the role of within- and across-study biases in
  meta-analysis
\end{itemize}

\end{tcolorbox}

Throughout this book, we have focused on how to design individual
experiments that maximize the precision of our effect size estimates and
minimize their bias. But even when we do our best to get a precise,
unbiased estimate in an individual experiment, one study can never be
definitive. Variability in participant demographics, stimuli, and
experimental methods may limit the generalizability of our findings.
Additionally, even well-powered individual studies have some amount of
statistical error, limiting their precision. Synthesizing evidence
across studies is critical for developing a balanced and appropriately
evolving view of the overall evidence on an effect of interest and for
understanding sources of variation in the effect.

Synthesizing evidence rigorously takes more than putting a search term
into Google Scholar, downloading articles that look topical or
interesting, and qualitatively summarizing your impressions of those
studies. While this ad-hoc method can be an essential first step in
performing a literature review
(\protect\hyperlink{ref-grant2009typology}{Grant and Booth 2009}), it is
not systematic and doesn't provide a \emph{quantitative} summary of a
particular effect. Further, it doesn't tell you anything about potential
biases in the literature -- for example, a bias for the publication of
positive effects.

To address these issues, a more systematic, quantitative review of the
literature is often more informative. This chapter focuses on a specific
type of quantitative review called \textbf{meta-analysis}: a method for
combining effect sizes across different studies. (If you need a
refresher on effect size, see \textbf{?@sec-estimation}, where we
introduce the concept).\sidenote{\footnotesize We'll primarily be using Cohen's \(d\),
  the standardized difference between means, which we introduced in
  \textbf{?@sec-estimation}. There are many more varieties of effect
  size available, but we focus here on \(d\) because it's common and
  easy to reason about in the context of the statistical tools we
  introduced in the earlier sections of the book.}

By combining information from multiple studies, meta-analysis often
provides more precise estimates of an effect size than any single study.
In addition, meta-analysis also allows the researcher to look at the
extent to which an effect varies across studies. If an effect does vary
across studies, meta-analysis also can be used to test whether certain
study characteristics systematically produce different results (e.g.,
whether an effect is larger in certain populations).

\begin{tcolorbox}[colframe=.blue, title=\faMicroscope \enspace Case study]

\hypertarget{towel-reuse-by-hotel-guests}{%
\section*{Towel reuse by hotel
guests}\label{towel-reuse-by-hotel-guests}}
\addcontentsline{toc}{section}{Towel reuse by hotel guests}

\markright{Towel reuse by hotel guests}

Imagine you are staying in a hotel and you have just taken a shower. Do
you throw the towels on the floor or hang them back up again? In a
widely-cited study on the power of social norms, Goldstein, Cialdini,
and Griskevicius (\protect\hyperlink{ref-goldstein2008room}{2008})
manipulated whether a sign encouraging guests to reuse towels focused on
environmental impacts (e.g., ``help reduce water use'') or social norms
(e.g., ``most guests re-use their towels''). Across two studies, they
found that guests were significantly more likely to reuse their towels
after receiving the social norm message (Study 1: odds ratio {[}OR{]} =
1.46, 95\% CI {[}1.00, 2.16{]}, \(p = .05\); Study 2: OR = 1.35, 95\% CI
{[}1.04, 1.77{]}, \(p = .03\)).

However, five subsequent studies by other researchers did not find
significant evidence that social norm messaging increased towel reuse.
(ORs ranged from 0.22 to 1.34, and no hypothesis-consistent \(p\)-value
was less than .05). This caused many researchers to wonder if there is
any effect at all. To examine this question, Scheibehenne, Jamil, and
Wagenmakers (\protect\hyperlink{ref-scheibehenne2016}{2016})
statistically combined evidence across the studies via meta-analysis.
This meta-analysis indicated that using social norm messages did
significantly increase hotel towel reuse, on average (OR = 1.26, 95\% CI
{[}1.07, 1.46{]}, \(p < .005\)). This case study demonstrates an
important strength of meta-analysis: by pooling evidence from multiple
studies, meta-analysis can generate more powerful insights than any one
study alone. We will also see how meta-analysis can be used to assess
variability in effects across studies.

\end{tcolorbox}

Meta-analysis often teaches us something about a body of evidence that
we do not intuitively grasp when we casually read through a bunch of
articles. In the above case study, merely reading the individual studies
might give the impression that social norm messages do not increase
hotel towel re-use. But meta-analysis indicated that the average effect
is beneficial, although there might be substantial variation in effect
sizes across studies.\sidenote{\footnotesize Given the billions of hotel bookings
  worldwide every year, even a small effect might have lead to a
  substantial environmental impact!}

\hypertarget{the-basics-of-evidence-synthesis}{%
\section{The basics of evidence
synthesis}\label{the-basics-of-evidence-synthesis}}

As we explore the details of conducting a meta-analysis, we'll turn to
another running example: a meta-analysis of studies investigating the
``contact hypothesis'' on intergroup relations.

According to the contact hypothesis, prejudice towards members of
minority groups can be reduced through intergroup contact interventions,
in which members of majority and minority groups work together to pursue
a common goal (\protect\hyperlink{ref-allport1954nature}{Allport 1954}).
To aggregate the evidence on the contact hypothesis, Paluck, Green, and
Green (\protect\hyperlink{ref-paluck2019contact}{2019}) meta-analyzed
studies that tested the effects of intergroup contact interventions on
long-term prejudice-related outcomes.

Using a systematic literature search, Paluck, Green, and Green
(\protect\hyperlink{ref-paluck2019contact}{2019}) searched for all
papers that tested these effects and then extracted effect size
estimates from each paper.\sidenote{\footnotesize This book will not cover the process
  of conducting a systematic literature search and extracting effect
  sizes, but these topics are critical to understand if you plan to
  conduct a meta-analysis or other evidence synthesis. Our experience is
  that extracting effect sizes from papers with inconsistent reporting
  standards can be especially tricky, so it can be helpful to talk to
  someone with experience in meta-analysis to get advice about this.}
Because not every paper reports standardized effect sizes -- or even
means and standard deviations for every group -- this process can often
involve scraping information from plots, tables, and statistical tests
to try to reconstruct effect sizes.\sidenote{\footnotesize For example, if the outcome
  variable is continuous, we could estimate Cohen's \(d\) from group
  means and standard deviations reported in the paper, even without
  having access to raw data.}

Following best practices for meta-analysis (where there are almost never
privacy concerns to worry about), Paluck, Green, and Green
(\protect\hyperlink{ref-paluck2019contact}{2019}) shared their data
openly. The first few lines are shown in Table~\ref{tbl-meta-dataset}.
We'll use these data as our running example throughout.

\hypertarget{tbl-meta-dataset}{}
\begin{longtable}[]{@{}lrlrrr@{}}
\caption{\label{tbl-meta-dataset}First few lines of extracted effect
sizes (d) and their variances (var\_d) in the Paluck, Green, and Green
(2019) meta-analysis.}\tabularnewline
\toprule\noalign{}
name\_short & pub\_date & target & n\_total & d & var\_d \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
name\_short & pub\_date & target & n\_total & d & var\_d \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Boisjoly 06 B & 2006 & race & 1243 & 0.030 & 0.006 \\
Sorensen 10 & 2010 & race & 597 & 0.302 & 0.007 \\
Scacco 18 & 2018 & religion & 474 & 0.000 & 0.010 \\
Finseraas 2017 & 2017 & foreigners & 577 & 0.000 & 0.011 \\
Sheare 74 & 1974 & disability & 400 & 1.059 & 0.011 \\
Barnhardt 09 & 2009 & religion & 312 & 0.395 & 0.015 \\
\end{longtable}

As we've seen throughout this book, visualizing data before and after
analysis helps benchmark and check our intuitions about the formal
statistical results. In a meta-analysis, a common way to plot effect
sizes is the \textbf{forest plot}, which depicts individual studies'
estimates and confidence intervals.\sidenote{\footnotesize You can ignore for now the
  column of percentages and the final line, ``RE Model''; we will return
  to these later.} In the forest plot in Figure~\ref{fig-meta-forest},
the larger squares correspond to more precise studies; notice how much
narrower their confidence intervals are than the confidence intervals of
less precise studies.

\begin{figure}

\sidecaption{\label{fig-meta-forest}Forest plot for Paluck, Green, and
Green (\protect\hyperlink{ref-paluck2019contact}{2019}) meta-analysis.
Studies are ordered from smallest to largest standard error.}

{\centering \includegraphics{016-meta_files/figure-pdf/fig-meta-forest-1.png}

}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

In this chapter, we use the wonderful \texttt{metafor} package
(\protect\hyperlink{ref-viechtbauer2010}{Viechtbauer 2010}). With this
package, you must first fit your meta-analytic model. But once you've
fit your model \texttt{mod}, you can simply call \texttt{forest(mod)} to
create a plot like the one above.

\end{tcolorbox}

\hypertarget{how-not-to-synthesize-evidence}{%
\subsection{How not to synthesize
evidence}\label{how-not-to-synthesize-evidence}}

Many people's first instinct in evidence synthesis is to count how many
studies supported versus did not support the hypothesis under
investigation. This technique usually amounts to counting the number of
studies with ``significant'' \(p\)-values, since -- for better or for
worse -- ``significance'' is largely what drives the take-home
conclusions researchers report
(\protect\hyperlink{ref-mcshane2017statistical}{McShane and Gal 2017};
\protect\hyperlink{ref-nelson1986interpretation}{Nelson, Rosenthal, and
Rosnow 1986}). In meta-analysis, we call this practice of counting the
number of significant \(p\)-values \textbf{vote-counting}
(\protect\hyperlink{ref-borenstein2021introduction}{Borenstein et al.
2021}). For example, in the Paluck, Green, and Green
(\protect\hyperlink{ref-paluck2019contact}{2019}) meta-analysis, almost
all studies had a positive effect size, but only 12 of 27 were
significant. So, based on this vote-count, we would have the impression
that most studies do not support the contact hypothesis.

Many qualitative literature reviews use this vote-counting approach,
although often not explicitly. Despite its intuitive appeal,
vote-counting can be very misleading because it characterizes evidence
solely in terms of dichotomized \(p\)-values, while entirely ignoring
effect sizes. In \textbf{?@sec-replication}, we saw how fetishizing
statistical significance can mislead us when we consider individual
studies. These problems also apply when considering multiple studies.

For example, small studies may consistently produce non-significant
effects due to their limited power. But when many such studies are
combined in a meta-analysis, the meta-analysis may provide strong
evidence of a positive average effect. Inversely, many studies might
have statistically significant effects, but if their effect sizes are
small, then a meta-analysis might indicate that the average effect size
is too small to be practically meaningful. In these cases, vote-counting
based on statistical significance can lead us badly astray
(\protect\hyperlink{ref-borenstein2021introduction}{Borenstein et al.
2021}). To avoid these pitfalls, meta-analysis combines the effect size
estimates from each study (not just their \(p\)-values), weighting them
in a principled way.

\hypertarget{fixed-effects-meta-analysis}{%
\subsection{Fixed-effects
meta-analysis}\label{fixed-effects-meta-analysis}}

If vote-counting is a bad idea, how should we combine results across
studies? Another intuitive approach might be to average effect sizes
from each study. For example, in Paluck et al.'s meta-analysis, the mean
of the studies' effect size estimates is 0.44. This averaging approach
is a step in the right direction, but it has an important limitation:
averaging effect size estimates gives equal weight to each study. A
small study (e.g.,
\protect\hyperlink{ref-clunies1989changing}{Clunies-Ross and O'meara
1989} with N=30) contributes as much to the mean effect size as a large
study (e.g., \protect\hyperlink{ref-boisjoly2006empathy}{Boisjoly et al.
2006} with N=1243). Larger studies provide more precise estimates of
effect sizes than small studies, so weighting all studies equally is not
ideal. Instead, larger studies should carry more weight in the analysis.

To address this issue, \textbf{fixed-effects meta-analysis} uses a
\textbf{weighted average} approach. Larger, more precise studies are
given more weight in the calculation of the overall effect size.
Specifically, each study is weighted by the inverse of its variance
(i.e., the inverse of its squared standard error). This makes sense
because larger, more precise studies have smaller variances, and thus
get more weight in the analysis.

In general terms, the fixed-effect pooled estimate is:

\[\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}\]
where \(k\) is the number of studies, \(\widehat{\theta}_i\) is the
point estimate of the \(i^{th}\) study, and
\(w_i = 1/\widehat{\sigma}^2_i\) is study \(i\)'s weight in the analysis
(i.e., the inverse of its variance).\sidenote{\footnotesize If you are curious, the
  standard error of the fixed-effect \(\widehat{\mu}\) is
  \(\frac{1}{\sum_{i=1}^k w_i}\). This standard error can be used to
  construct a confidence interval or \(p\)-value, as described in
  \textbf{?@sec-inference}.}

Using the fixed-effects formula, we can estimate that the overall effect
size in Paluck et al.'s meta-analysis is a standardized mean difference
of \(\widehat{\mu}\) = 0.28; 95\% confidence interval {[}0.23, 0.34{]};
\(p < .001\). Because Cohen's \(d\) is our effect size index, this
estimate would suggest that intergroup contact decreased prejudice by
0.28 standard deviations.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Fitting meta-analytic models in \texttt{metafor} is quite simple. For
example, for the fixed effects model above, we simply the \texttt{rma}
function and specified that we wanted a fixed effects analysis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fe.m }\OtherTok{\textless{}{-}} \FunctionTok{rma}\NormalTok{(}\AttributeTok{yi =}\NormalTok{ d,}
            \AttributeTok{vi =}\NormalTok{ var\_d,}
            \AttributeTok{data =}\NormalTok{ paluck,}
            \AttributeTok{method =} \StringTok{"FE"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then \texttt{summary(fe.m)} gives us the relevant information about the
fitted model.

\end{tcolorbox}

\hypertarget{limitations-of-fixed-effects-meta-analysis}{%
\subsection{Limitations of fixed-effects
meta-analysis}\label{limitations-of-fixed-effects-meta-analysis}}

One of the limitations of fixed-effect meta-analysis is that it assumes
that the true effect size is, well, \emph{fixed}! In other words,
fixed-effect meta-analysis assumes that there is a single effect size
that all studies are estimating. This is a stringent assumption. It's
easy to imagine that it could be violated. Imagine, for example, that
intergroup contact decreased prejudice when the group succeeded at its
joint goal, but \emph{increased} prejudice when the group failed. If we
meta-analyzed two studies under these conditions -- one in which
intergroup contact substantially increased prejudice, and one in which
intergroup contact substantially decreased prejudice -- it might appear
that the true effect of intergroup contact was close to zero, when in
fact both of the meta-analyzed studies had large effects.

In Paluck et al.'s meta-analysis, studies differed in several ways that
could lead to different true effects. For example, some studies
recruited adult participants while others recruited children. If
intergroup contact is more or less effective for adults versus children,
then it is misleading to talk about a single (i.e., ``fixed'')
intergroup contact effect. Instead, we would say that the effects of
intergroup contact vary across studies, an idea called
\textbf{heterogeneity}.

Does the concept of heterogeneity remind you of anything from when we
analyzed repeated-measures data in \textbf{?@sec-models} on models?
Recall that, with repeated-measures data, we had to deal with the
possibility of heterogeneity across participants -- and of the ways we
did so was by introducing participant-level random intercepts to our
regression model. It turns out that we can do a similar thing in
meta-analysis to deal with heterogeneity across studies.

\hypertarget{random-effects-meta-analysis}{%
\subsection{Random-effects
meta-analysis}\label{random-effects-meta-analysis}}

While fixed-effect meta-analysis essentially assumes that all studies in
the meta-analysis have the same population effect size, \(\mu\),
random-effects meta-analysis instead assumes that study effects come
from a normal distribution with mean \(\mu\) and standard deviation
\(\tau\).\sidenote{\footnotesize Technically, other specifications of random-effects
  meta-analysis are possible. For example, robust variance estimation
  does not require making assumptions about the distribution of effects
  across studies (\protect\hyperlink{ref-hedges2010robust}{Hedges,
  Tipton, and Johnson 2010}). These approaches also have other
  substantial advantages, like their ability to handle effects that are
  clustered {[}e.g., because some papers contribute multiple estimates;
  Hedges, Tipton, and Johnson
  (\protect\hyperlink{ref-hedges2010robust}{2010}); Pustejovsky and
  Tipton (\protect\hyperlink{ref-pustejovsky2021meta}{2021}){]} and
  their ability to provide better inference in meta-analyses with
  relatively few studies (\protect\hyperlink{ref-tipton2015small}{Tipton
  2015}). For these reasons, we often use these robust methods.} The
larger the standard deviation, \(\tau\), the more heterogeneous the
effects are across studies. A random-effects model then estimates both
\(\mu\) and \(\tau\), for example by maximum likelihood
(\protect\hyperlink{ref-dersimonian1986meta}{DerSimonian and Laird
1986}; \protect\hyperlink{ref-brockwell2001comparison}{Brockwell and
Gordon 2001}).

Like fixed-effect meta-analysis, the random-effects estimate of
\(\widehat{\mu}\) is still a weighted average of studies' effect size
estimates:
\[\widehat{\mu} = \frac{ \sum_{i=1}^k w_i \widehat{\theta}_i}{\sum_{i=1}^k w_i}\]

However, in random-effects meta-analysis, the inverse-variance weights
now incorporate heterogeneity:
\(w_i = 1/\left(\widehat{\tau}^2 + \widehat{\sigma}^2_i \right)\). Where
before we had one term in our weights, now we have two. That is because
these weights represent the inverse of studies' \emph{marginal}
variances, taking into account both statistical error due to their
finite sample sizes (\(\widehat{\sigma}^2_i\) as before) and also
genuine effect heterogeneity (\(\widehat{\tau}^2\)).

Conducting a random-effects meta-analysis of Paluck et al.'s dataset
yields \(\widehat{\mu}\) = 0.4; 95\% confidence interval {[}0.2,
0.61{]}; \(p < .001\). That is, \emph{on average across studies},
intergroup contact was associated with a decrease in prejudice of 0.4
standard deviations, substantially larger than the estimate from the
fixed effects model. This meta-analytic estimate is shown as the bottom
line of Figure~\ref{fig-meta-forest}.

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

Fitting a random effects model requires only a small change to the
methods argument of \texttt{rma}. (We also include the \texttt{knha}
flag that adds a correction to the computation of standard errors and
p-values).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{re.m }\OtherTok{\textless{}{-}} \FunctionTok{rma}\NormalTok{(}\AttributeTok{yi =}\NormalTok{ d,}
            \AttributeTok{vi =}\NormalTok{ var\_d,}
            \AttributeTok{data =}\NormalTok{ paluck,}
            \AttributeTok{method =} \StringTok{"REML"}\NormalTok{,}
            \AttributeTok{knha =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

Based on the random effects model, intergroup contact effects appear to
differ across studies. Paluck et al.~estimated that the standard
deviation of effects across studies was \(\widehat{\tau}\) = 0.44 ; 95\%
confidence interval {[}0.25, 0.57{]}. This estimate indicates a
substantial amount of heterogeneity! To visualize these results, we can
plot the estimated density of the population effects, which is just a
normal distribution with mean \(\widehat{\mu}\) and standard deviation
\(\widehat{\tau}\) (Figure~\ref{fig-meta-densities}).

\begin{marginfigure}

{\centering \includegraphics{016-meta_files/figure-pdf/fig-meta-densities-1.png}

}

\caption{\label{fig-meta-densities}Estimated distribution of population
effects from random-effects meta-analysis of Paluck et. al's dataset
(heavy red curve) and estimated density of studies' point estimates
(thin black curve).}

\end{marginfigure}

This meta-analysis highlights an important point:that the overall effect
size estimate \(\widehat{\mu}\) represents only the \emph{mean}
population effect across studies. It tells us nothing about how much the
effects \emph{vary} across studies. Thus, we recommend always reporting
the heterogeneity estimate \(\widehat{\tau}\), preferably along with
other related metrics that help summarize the distribution of effect
sizes across studies
(\protect\hyperlink{ref-riley2011interpretation}{Riley, Higgins, and
Deeks 2011}; \protect\hyperlink{ref-wang2019simple}{Wang and Lee 2019};
\protect\hyperlink{ref-mathur_mam}{Mathur and VanderWeele 2019},
\protect\hyperlink{ref-npphat}{2020a}). Reporting the heterogeneity
helps readers know how consistent or inconsistent the effects are across
studies, which may point to the need to investigate \emph{moderators} of
the effect (i.e., factors that are associated with larger or smaller
effects, such as whether participants were adults or
children).\sidenote{\footnotesize One common approach to investigating moderators in
  meta-analysis is meta-regression, in which moderators are included as
  covariates in a random-effects meta-analysis model
  (\protect\hyperlink{ref-thompson2002should}{Thompson and Higgins
  2002}). As in standard regression, coefficients can then be estimated
  for each moderator, representing the mean difference in population
  effect between studies with versus without the moderator.}

\begin{tcolorbox}[colframe=.violet, title=\faMagnifyingGlassPlus \enspace Depth]

\hypertarget{single-paper-meta-analysis-and-pooled-analysis}{%
\section*{Single-paper meta-analysis and pooled
analysis}\label{single-paper-meta-analysis-and-pooled-analysis}}
\addcontentsline{toc}{section}{Single-paper meta-analysis and pooled
analysis}

\markright{Single-paper meta-analysis and pooled analysis}

Thus far, we have described meta-analysis as a tool for summarizing
results reported across multiple papers. However, some people have
argued that meta-analysis should also be used to summarize the results
of multiple studies reported in a single paper
(\protect\hyperlink{ref-goh2016mini}{Goh, Hall, and Rosenthal 2016}).
For instance, in a paper where you describe 3 different experiments on a
hypothesis, you could (1) extract summary information (e.g., \emph{M}'s
and \emph{SD}'s) from each study, (2) compute the effect size, and then
(3) combine the effect sizes in a meta-analysis.

Single-paper meta-analyses come with many of the same strengths and
weaknesses we have discussed thus far. One unique weakness, though, is
that having a small number of studies means that you typically have low
power to detect heterogeneity and moderators. This lack of power
sometimes leads researchers to claim that there are no significant
differences between their studies. But an alternative explanation is
that there simply wasn't enough power to detect those differences!

As an alternative, you can also pool the actual data from the three
studies, as opposed to just pooling summary statistics. For example, if
you have data from 10 participants in each of the 3 experiments, you
could pool them into a single dataset with 30 participants and include
random effects of your condition manipulation across experiments (as
described in \textbf{?@sec-models}). This strategy is often referred to
as \textbf{pooled} or \textbf{integrative} data analysis (and
occasionally as ``mega-analysis'', which sounds cool).

\begin{figure}[H]

{\centering \includegraphics{images/meta/meta_v_ipd.png}

}

\caption{\label{fig-meta-v-ipd}Meta-analysis vs.~pooled data analysis.}

\end{figure}

One of the benefits of pooled data analysis is that it can give you more
power to detect moderators. For instance, imagine that the effect of an
intergroup contact treatment is moderated by age. If we performed a
traditional meta-analysis, we would only have three observations in our
data set, yielding very low power. However, we have many more
observations (and much more variation in the moderator) in the pooled
data analysis, which can lead to higher power
(Figure~\ref{fig-meta-v-ipd}).

Pooled data analysis is not without its own limitations
(\protect\hyperlink{ref-cooper2009relative}{Cooper and Patall 2009}).
And, of course, sometimes it doesn't make as much sense to pool datasets
(e.g., when measures are different from one another). Nonetheless, we
believe that pooled data analysis and meta-analysis are both useful
tools to keep in mind in a paper reporting multiple experiments!

\end{tcolorbox}

\hypertarget{bias-in-meta-analysis}{%
\section{Bias in meta-analysis}\label{bias-in-meta-analysis}}

Meta-analysis is a great tool for synthesizing evidence across studies,
but the accuracy of a meta-analysis can be compromised by bias. We'll
talk about two categories of bias here: \textbf{within-study} and
\textbf{across-study} biases. Either type can lead to meta-analytic
estimates that are too large, too small, or even in the wrong direction
altogether.

\hypertarget{within-study-biases}{%
\subsection{Within-study biases}\label{within-study-biases}}

Within-study biases -- such as demand characteristics, confounds, and
order effects, all discussed in \textbf{?@sec-design} -- not only impact
the validity of individual studies, but also any attempt to synthesize
those studies. And of course, if individual study results are affected
by analytic flexibility (\(p\)-hacking), meta-analyzing these will
result in inflated effect size estimates. In other words: garbage in,
garbage out.

For example, Paluck, Green, and Green
(\protect\hyperlink{ref-paluck2019contact}{2019}) noted that early
studies on intergroup contact almost exclusively used non-randomized
designs. Imagine a hypothetical study where researchers studied a
completely ineffective intergroup contact intervention, and non-randomly
assigned low-prejudice people to the intergroup contact condition and
high-prejudice people to the control condition. In a scenario like this,
the researcher would of course find that the prejudice was lower in the
intergroup contact condition. But the effect would not be a true contact
intervention effect, but rather a spurious effect of non-random
assignment (i.e., confounding). Now imagine meta-analyzing many studies
with similarly poor designs. The meta-analyst might find impressive
evidence of an intergroup contact effect, even if none existed.

To mitigate this problem, meta-analysts often exclude studies that may
be especially affected by within-study bias. (For example,
\protect\hyperlink{ref-paluck2019contact}{Paluck, Green, and Green 2019}
excluded non-randomized studies). Of course, these decisions can't be
made on the basis of their effects on the meta-analytic estimate or else
this post-hoc exclusion itself will lead to bias! For this reason,
inclusion and exclusion criteria for meta-analyses should be
preregistered whenever possible.

Sometimes certain sources of bias cannot be eliminated by excluding
studies -- often because studies in a particular domain share certain
fundamental limitations (for example, attrition in drug trials). After
data have been collected, meta-analysts should also assess studies'
risks of bias qualitatively using established rating tools
(\protect\hyperlink{ref-sterne2016robins}{Sterne et al. 2016}). Doing so
allows the meta-analyst to communicate how much within-study bias there
may be.\sidenote{\footnotesize If you're interested in assessing within-study bias,
  you can take a look at the
  \href{https://methods.cochrane.org/bias/resources/rob-2-revised-cochrane-risk-bias-tool-randomized-trials}{Risk
  of Bias tool} developed by Cochrane, an organization devoted to
  evidence synthesis.}

Meta-analysts can also conduct sensitivity analyses to assess how much
results might be affected by different within-study biases or by
excluding certain types of studies (\protect\hyperlink{ref-art}{Mathur
and VanderWeele 2022}). For example, if nonrandom assignment is a
concern, a meta-analyst may run the analyses including only randomized
studies, versus including all studies, in order to determine how much
including nonrandomized studies changes the meta-analytic estimate.
These two options parallel our discussion of experimental
preregistration in \textbf{?@sec-prereg}: To allay concerns about
results-dependent meta-analysis, researchers can either pre-register
their analyses ahead of time or else be transparent about their choices
after the fact. Sensitivity analyses can allay concerns that a specific
choice of exclusion criteria is critically related to the reported
results.

\hypertarget{across-study-biases}{%
\subsection{Across-study biases}\label{across-study-biases}}

Across-study biases occur if, for example, researchers
\textbf{selectively report} certain types of findings or selectively
publish certain types of findings (publication bias, as discussed in
\textbf{?@sec-replication} and \textbf{?@sec-prereg}). Often, these
across-study biases favor statistically-significant positive results,
which means the meta-analytic estimate based on those studies will be
inflated relative to the true effect.

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{quantifying-publication-bias-in-the-social-sciences}{%
\section*{Quantifying publication bias in the social
sciences}\label{quantifying-publication-bias-in-the-social-sciences}}
\addcontentsline{toc}{section}{Quantifying publication bias in the
social sciences}

\markright{Quantifying publication bias in the social sciences}

It's typically very hard to quantify publication bias because you don't
know how many studies are out there in researchers' ``file drawers'' --
unpublished studies are by definition not available. But a recent study
took advantage of a unique opportunity to try and quantify publication
bias within a known pool of studies.

Time-sharing Experiments in the Social Sciences (TESS) is an innovative
project that lets researchers apply to run experiments on
nationally-representative samples in the U.S. In 2014, Franco, Malhotra,
and Simonovits (\protect\hyperlink{ref-franco2014}{2014}) and colleagues
took advantage of this application process by examining the entire
population of 221 studies conducted through TESS.

Using this information, Franco and colleagues examined the records of
these studies to determine whether the researchers found statistically
significant results, a mixture of statistically significant and
non-significant results, or only non-significant results. Then, they
examined the likelihood that these results were published in the
scientific literature.

Over 60\% of studies with statistically significant results were
published, compared to a mere 25\% of studies that produced only
statistically non-significant results. This finding was important
because it quantified how strong publication bias actually was, at least
in one particular population of studies. This estimate may not be
general: for example, perhaps TESS studies were easier to put in the
file drawer because they cost nothing for the researchers to run. But
even a lower level of publication bias can have a substantial effect on
a meta-analysis, meaning that it is crucial to check for -- and
potentially, correct for -- publication bias.

\end{tcolorbox}

Like within-study biases, meta-analysts often try to mitigate
across-study biases by being careful about what studies make it into the
meta-analysis. Meta-analysts don't only want to capture high-profile,
published studies on their effect of interest, but also studies
published in low-profile journals and the so-called ``gray literature''
{[}i.e., unpublished dissertations and theses; Lefebvre et al.
(\protect\hyperlink{ref-lefebvre2019searching}{2019}){]}.\sidenote{\footnotesize Evidence
  is mixed regarding whether including gray literature actually reduces
  across-study biases in meta-analysis
  (\protect\hyperlink{ref-tsuji2020addressing}{Tsuji et al. 2020};
  \protect\hyperlink{ref-sapbe}{Mathur and VanderWeele 2021}), but it is
  still common practice to try to include this literature.}

There are also statistical methods to help assess how robust the results
may be to across-study biases. Among the most popular tools to assess
and correct for publication bias is the \textbf{funnel plot}
(\protect\hyperlink{ref-duval2000trim}{Duval and Tweedie 2000};
\protect\hyperlink{ref-egger1997bias}{Egger et al. 1997}). A funnel plot
shows the relationship between studies' effect estimates and their
precision (usually their standard error). These plots are called
``funnel plots'' because if there is no publication bias, then as
precision increases, the effects ``funnel'' towards the meta-analytic
estimate. As the precision is smaller, they spread out more because of
greater measurement error. Figure~\ref{fig-meta-funnel-unbiased} is an
example of one type of funnel plot (\protect\hyperlink{ref-sapb}{Mathur
and VanderWeele 2020b}) for a simulated meta-analysis of 100 studies
with no publication bias.

\begin{figure}

\sidecaption{\label{fig-meta-funnel-unbiased}Significance funnel plot
for a meta-analysis simulated to have no publication bias. Orange
points: studies with \(p < 0.05\) and positive estimates. Grey points:
studies with \(p\) \(\ge\) \(0.05\) or negative estimates. Black
diamond: random-effects estimate of \(\widehat{\mu}\).}

{\centering \includegraphics{016-meta_files/figure-pdf/fig-meta-funnel-unbiased-1.png}

}

\end{figure}

\begin{tcolorbox}[colframe=.base0, title=\faCode \enspace Code]

For this plot, we use the \texttt{PublicationBias} package and the
\texttt{significance\_funnel} function. (An alternative function is the
\texttt{metafor} function \texttt{funnel}, which results in the figure
in the margins, a more ``classic'' funnel plot.) We use our fitted model
\texttt{re.m} and feed the relevant parameters into the fucntion:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{significance\_funnel}\NormalTok{(}\AttributeTok{yi =}\NormalTok{ re.m}\SpecialCharTok{$}\NormalTok{yi,}
                    \AttributeTok{vi =}\NormalTok{ re.m}\SpecialCharTok{$}\NormalTok{vi,}
                    \AttributeTok{favor.positive =} \ConstantTok{TRUE}\NormalTok{,}
                    \AttributeTok{plot.pooled =} \ConstantTok{TRUE}\NormalTok{,}
                    \AttributeTok{est.all =}\NormalTok{ re.m}\SpecialCharTok{$}\NormalTok{b,}
                    \AttributeTok{est.N =} \ConstantTok{NA}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

As you can see, because meta-analysis is such a well-established method,
many of the relevant operations are ``plug and play.''

\end{tcolorbox}

As implied by the ``funnel'' moniker, our plot looks a little like a
funnel. Larger studies (those with smaller standard errors) cluster more
closely around the mean of 0.34 than do smaller studies, but large and
small studies alike have point estimates centered around the mean. That
is, the funnel plot is symmetric.\sidenote{\footnotesize Classic funnel plots look
  more like Figure~\ref{fig-meta-classic-funnel}). Our version is
  different in a couple of ways. Most prominently, we don't have the
  vertical axis reversed (which we think is confusing). We also don't
  have the left boundary highlighted, because we think folks don't
  typically select for negative studies.}

\begin{figure}

\sidecaption{\label{fig-meta-classic-funnel}Classic funnel plot.}

{\centering \includegraphics{016-meta_files/figure-pdf/fig-meta-classic-funnel-1.png}

}

\end{figure}

Not all funnel plots are symmetric! Figure~\ref{fig-meta-funnel-biased}
is what happens to our hypothetical meta-analysis if all studies with
\(p<0.05\) and positive estimates are published, but only 10\% of
studies with \(p \ge 0.05\) or with negative estimates are published.
The introduction of publication bias dramatically inflates the pooled
estimate from 0.34 to 1.15. Also, there appears to be a correlation
between studies' estimates and their standard errors, such that smaller
studies tend to have larger estimates than do larger studies. This
correlation is often called \textbf{funnel plot asymmetry} because the
funnel plot starts to look like a right triangle rather than a funnel.
Funnel plot asymmetry \emph{can} be a diagnostic for publication bias,
though it isn't always a perfect indicator, as we'll see in the next
subsection.

\begin{figure}

\sidecaption{\label{fig-meta-funnel-biased}Significance funnel plot for
the same simulated meta-analysis after publication bias has occurred.
Orange points: studies with \(p < 0.05\) and positive estimates. Grey
points: studies with \(p\) \(\ge\) \(0.05\) or negative estimates. Black
diamond: random-effects estimate of \(\widehat{\mu}\).}

{\centering \includegraphics{016-meta_files/figure-pdf/fig-meta-funnel-biased-1.png}

}

\end{figure}

\hypertarget{across-study-bias-correction}{%
\subsection{Across-study bias
correction}\label{across-study-bias-correction}}

How do we identify and correct bias across studies? Given that some
forms of publication bias induce a correlation between studies' point
estimates and their standard errors, several popular statistical
methods, such as Trim-and-Fill
(\protect\hyperlink{ref-duval2000trim}{Duval and Tweedie 2000}) and
Egger's regression (\protect\hyperlink{ref-egger1997bias}{Egger et al.
1997}) are designed to quantify funnel plot asymmetry.

Funnel plot asymmetry does not always imply that there is publication
bias, though. Nor does publication bias always cause funnel plot
asymmetry. Sometimes funnel plot asymmetry is driven by genuine
differences in the effects being studied in small and large studies
(\protect\hyperlink{ref-egger1997bias}{Egger et al. 1997};
\protect\hyperlink{ref-lau2006case}{Lau et al. 2006}). For example, in a
meta-analysis of intervention studies, if the most effective
interventions are also the most expensive or difficult to implement,
these highly effective interventions might be used primarily in the
smallest studies (``small study effects'').

Funnel plots and related methods are best suited to detecting
publication bias in which (1) small studies with large positive point
estimates are more likely to be published than small studies with small
or negative point estimates; and (2) the largest studies are published
regardless of the magnitude of their point estimates. That model of
publication bias is sometimes what is happening, but not always!

A more flexible approach for detecting publication bias uses
\textbf{selection models}. These models can detect other forms of
publication bias that funnel plots may not detect, such as publication
bias that favors \emph{significant} results. We won't cover these
methods in detail here, but we think they are a better approach to the
question, along with related sensitivity analyses.\sidenote{\footnotesize High-level
  overviews of selection models are given in McShane, Bckenholt, and
  Hansen (\protect\hyperlink{ref-mcshane2016adjusting}{2016}) and Maier,
  VanderWeele, and Mathur (\protect\hyperlink{ref-smt}{in press}). For
  more methodological detail, see Hedges
  (\protect\hyperlink{ref-hedges1984estimation}{1984}), Iyengar and
  Greenhouse (\protect\hyperlink{ref-iyengar1988}{1988}), and Vevea and
  Hedges (\protect\hyperlink{ref-vevea1995}{1995}). For a tutorial on
  fitting and interpreting selection models, see Maier, VanderWeele, and
  Mathur (\protect\hyperlink{ref-smt}{in press}). For sensitivity
  analyses, see Mathur and VanderWeele
  (\protect\hyperlink{ref-sapb}{2020b}).}

You may also have heard of ``\(p\)-methods'' to detect across-study
biases such as \(p\)-curve and \(p\)-uniform
(\protect\hyperlink{ref-simonsohn2014p}{Simonsohn, Nelson, and Simmons
2014}; \protect\hyperlink{ref-van2015meta}{Van Assen, Aert, and Wicherts
2015}). These methods essentially assess whether the significant
\(p\)-values ``bunch up'' just under 0.05, which is taken to indicate
publication bias. These methods are increasingly popular in psychology
and have their merits. However, they are actually simplified versions of
selection models (e.g.,
\protect\hyperlink{ref-hedges1984estimation}{Hedges 1984}) that work
only under considerably more restrictive settings than the original
selection models {[}for example, when there is not heterogeneity across
studies; McShane, Bckenholt, and Hansen
(\protect\hyperlink{ref-mcshane2016adjusting}{2016}){]}. For this
reason, it is usually (although not always) better to use selection
models in place of the more restrictive \(p\)-methods.

Going back to our running example, Paluck et al.~used a regression-based
approach to assess and correct for publication bias. This approach
provided significant evidence of a relationship between the standard
error and effect size (i.e., an asymmetric funnel plot). Again, this
asymmetry could reflect publication bias or other sources of correlation
between studies' estimates and their standard errors. Paluck et al.~also
used this same regression-based approach to try to correct for potential
publication bias. Results from this model indicated that the
bias-corrected effect size estimate was close to zero. In other words,
even though all studies estimated that intergroup contact decreased
prejudice, it is still possible that there are unpublished studies that
did not find this (or found that intergroup contact increased
prejudice).

\begin{tcolorbox}[colframe=.orange, title=\faPersonFallingBurst \enspace Accident report]

\hypertarget{garbage-in-garbage-out-meta-analyzing-potentially-problematic-research}{%
\section*{Garbage in, garbage out? Meta-analyzing potentially
problematic
research}\label{garbage-in-garbage-out-meta-analyzing-potentially-problematic-research}}
\addcontentsline{toc}{section}{Garbage in, garbage out? Meta-analyzing
potentially problematic research}

\markright{Garbage in, garbage out? Meta-analyzing potentially
problematic research}

Botox can help eliminate wrinkles. But some researchers have suggested
that, when used to paralyze the muscles associated with frowning, Botox
may also help treat clinical depression. As surprising as this claim may
sound, a quick examination of the literature would lead many to conclude
that this treatment works. Studies that randomly assign depressed
patients to receive either Botox or saline injections do indeed find
that Botox recipients show decreased depression. And when you combine
all available evidence in a meta-analysis, you find that this effect is
quite large: \emph{d} = 0.83, 95\% CI {[}0.52, 1.14{]}.

As Coles et al. (\protect\hyperlink{ref-coles2019does}{2019}) argued
though, this estimated effect may be impacted by both within- and
between-study bias. First, participants are not supposed to know whether
they have been randomly assigned to receive Botox or a control saline
injections. But only one of these treatments leads the upper half of
your face to be paralyzed! After a couple weeks, you're pretty likely to
know whether you received the Botox treatment or control saline
injection. Thus, the apparent effect of Botox on depression could
instead be a placebo effect.

Second, only 50\% of the outcomes that researchers measured were
reported in the final publications, raising concerns about selective
reporting. Perhaps researchers examining the effects of Botox on
depression only reported the measures that showed a positive effect, not
the ones that didn't.

Taken together, these two criticisms suggest that, despite the
impressive meta-analytic estimate, the effect of Botox on depression is
far from certain.

\end{tcolorbox}

\hypertarget{chapter-summary-meta-analysis}{%
\section{Chapter summary:
Meta-analysis}\label{chapter-summary-meta-analysis}}

Taken together, Paluck and colleagues' use of meta-analysis provided
several important insights that would have been easy to miss in a
non-quantitative review. First, despite a preponderance of
non-significant findings, intergroup contact interventions were
estimated to decrease prejudice by on average 0.4 standard deviations.
On the other hand, there was considerable heterogeneity in intergroup
contact effects, suggesting important moderators of the effectiveness of
these interventions. And finally, publication bias was a substantial
concern, indicating a need for follow-up research using a registered
report format that will be published regardless of whether the outcome
is positive (\textbf{?@sec-prereg}).

Overall, meta-analysis is a key technique for aggregating evidence
across studies. Meta-analysis allows researchers to move beyond the bias
of naive techniques like vote counting and towards a more quantitative
summary of an experimental effect. Unfortunately, a meta-analysis is
only as good as the literature it's based on, so the aspiring
meta-analyst must be aware of both within- and between-study biases!

\begin{tcolorbox}[colframe=.cyan, title=\faComments \enspace Discussion questions]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Imagine that you read the following result in the abstract of a
  meta-analysis: ``In 83 randomized studies of middle school children,
  replacing one hour of class time with mindfulness meditation
  significantly improved standardized test scores (standardized mean
  difference \(\widehat{\mu} = 0.05\); 95\% confidence interval:
  {[}\(0.01, 0.09\){]}; \(p<0.05\)).'' Why is this a problematic way to
  report on meta-analysis results? Suggest a better sentence to replace
  this one.
\item
  As you read the rest of the meta-analysis, you find that the authors
  conclude that ``These findings demonstrate robust benefits of
  meditation for children, suggesting that test scores improve even when
  the meditation is introduced as a replacement for normal class time.''
  You recall that the heterogeneity estimate was
  \(\widehat{\tau} = 0.90\). Do you think that this result regarding the
  heterogeneity tends to support, or rather tends to undermine, the
  concluding sentence of the meta-analysis? Why?
\item
  What kinds of within-study biases would concern you in the
  meta-analysis described in the prior two questions? How might you
  assess the credibility of the meta-analyzed studies and of the
  meta-analysis as whole in light of these possible biases?
\item
  Imagine you conduct a meta-analysis on a literature in which
  statistically significant results in either direction are much more
  likely to be published that non-significant results. Draw the funnel
  plot you would expect to see. Is the plot symmetric or asymmetric?
\item
  Why do you think small studies receive more weight in random-effects
  meta-analysis than in fixed-effects meta-analysis? Can you see why
  this is true mathematically based on the equations given above, and
  can you also explain the intuition in simple language?
\end{enumerate}

\end{tcolorbox}

\begin{tcolorbox}[colframe=.magenta, title=\faBook \enspace Readings]

\begin{itemize}
\tightlist
\item
  A nice, free textbook with lots of good code examples: Harrer, M.,
  Cuijpers, P., Furukawa, T., \& Ebert, D. (2022). Doing Meta-Analysis
  with R: A Hands-On Guide. Chapman \& Hall/CRC Press. Available free
  online at
  \href{}{https://bookdown.org/MathiasHarrer/Doing\_Meta\_Analysis\_in\_R/}.
\end{itemize}

\end{tcolorbox}

\hypertarget{bibliography-21}{%
\section*{References}\label{bibliography-21}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-21}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-allport1954nature}{}}%
Allport, Gordon Willard. 1954. {``The Nature of Prejudice.''}

\leavevmode\vadjust pre{\hypertarget{ref-boisjoly2006empathy}{}}%
Boisjoly, Johanne, Greg J Duncan, Michael Kremer, Dan M Levy, and Jacque
Eccles. 2006. {``Empathy or Antipathy? The Impact of Diversity.''}
\emph{American Economic Review} 96 (5): 1890--1905.

\leavevmode\vadjust pre{\hypertarget{ref-borenstein2021introduction}{}}%
Borenstein, Michael, Larry V Hedges, Julian PT Higgins, and Hannah R
Rothstein. 2021. \emph{Introduction to Meta-Analysis}. John Wiley \&amp;
Sons.

\leavevmode\vadjust pre{\hypertarget{ref-brockwell2001comparison}{}}%
Brockwell, Sarah E, and Ian R Gordon. 2001. {``A Comparison of
Statistical Methods for Meta-Analysis.''} \emph{Statistics in Medicine}
20 (6): 825--40.

\leavevmode\vadjust pre{\hypertarget{ref-clunies1989changing}{}}%
Clunies-Ross, Graham, and Kris O'meara. 1989. {``Changing the Attitudes
of Students Towards Peers with Disabilities.''} \emph{Australian
Psychologist} 24 (2): 273--84.

\leavevmode\vadjust pre{\hypertarget{ref-coles2019does}{}}%
Coles, Nicholas A, Jeff T Larsen, Joyce Kuribayashi, and Ashley Kuelz.
2019. {``Does Blocking Facial Feedback via Botulinum Toxin Injections
Decrease Depression? A Critical Review and Meta-Analysis.''}
\emph{Emotion Review} 11 (4): 294--309.

\leavevmode\vadjust pre{\hypertarget{ref-cooper2009relative}{}}%
Cooper, Harris, and Erika A Patall. 2009. {``The Relative Benefits of
Meta-Analysis Conducted with Individual Participant Data Versus
Aggregated Data.''} \emph{Psychological Methods} 14 (2): 165.

\leavevmode\vadjust pre{\hypertarget{ref-dersimonian1986meta}{}}%
DerSimonian, Rebecca, and Nan Laird. 1986. {``Meta-Analysis in Clinical
Trials.''} \emph{Controlled Clinical Trials} 7 (3): 177--88.

\leavevmode\vadjust pre{\hypertarget{ref-duval2000trim}{}}%
Duval, Sue, and Richard Tweedie. 2000. {``Trim and Fill: A Simple
Funnel-Plot--Based Method of Testing and Adjusting for Publication Bias
in Meta-Analysis.''} \emph{Biometrics} 56 (2): 455--63.
\url{https://doi.org/10.1111/j.0006-341X.2000.00455.x}.

\leavevmode\vadjust pre{\hypertarget{ref-egger1997bias}{}}%
Egger, Matthias, George Davey Smith, Martin Schneider, and Christoph
Minder. 1997. {``Bias in Meta-Analysis Detected by a Simple, Graphical
Test.''} \emph{BMJ} 315 (7109): 629--34.
\url{https://doi.org/10.1136/bmj.315.7109.629}.

\leavevmode\vadjust pre{\hypertarget{ref-franco2014}{}}%
Franco, A, N Malhotra, and G Simonovits. 2014. {``Publication Bias in
the Social Sciences: Unlocking the File Drawer.''} \emph{Science}.

\leavevmode\vadjust pre{\hypertarget{ref-goh2016mini}{}}%
Goh, Jin X, Judith A Hall, and Robert Rosenthal. 2016. {``Mini
Meta-Analysis of Your Own Studies: Some Arguments on Why and a Primer on
How.''} \emph{Social and Personality Psychology Compass} 10 (10):
535--49.

\leavevmode\vadjust pre{\hypertarget{ref-goldstein2008room}{}}%
Goldstein, Noah J, Robert B Cialdini, and Vladas Griskevicius. 2008.
{``A Room with a Viewpoint: Using Social Norms to Motivate Environmental
Conservation in Hotels.''} \emph{Journal of Consumer Research} 35 (3):
472--82.

\leavevmode\vadjust pre{\hypertarget{ref-grant2009typology}{}}%
Grant, Maria J, and Andrew Booth. 2009. {``A Typology of Reviews: An
Analysis of 14 Review Types and Associated Methodologies.''}
\emph{Health Information \& Libraries Journal} 26 (2): 91--108.

\leavevmode\vadjust pre{\hypertarget{ref-hedges1984estimation}{}}%
Hedges, Larry V. 1984. {``Estimation of Effect Size Under Nonrandom
Sampling: The Effects of Censoring Studies Yielding Statistically
Insignificant Mean Differences.''} \emph{Journal of Educational
Statistics} 9 (1): 61--85.
\url{https://doi.org/10.3102/10769986009001061}.

\leavevmode\vadjust pre{\hypertarget{ref-hedges2010robust}{}}%
Hedges, Larry V, Elizabeth Tipton, and Matthew C Johnson. 2010.
{``Robust Variance Estimation in Meta-Regression with Dependent Effect
Size Estimates.''} \emph{Research Synthesis Methods} 1 (1): 39--65.

\leavevmode\vadjust pre{\hypertarget{ref-iyengar1988}{}}%
Iyengar, Satish, and Joel B Greenhouse. 1988. {``Selection Models and
the File Drawer Problem.''} \emph{Statistical Science}, 109--17.

\leavevmode\vadjust pre{\hypertarget{ref-lau2006case}{}}%
Lau, Joseph, John PA Ioannidis, Norma Terrin, Christopher H Schmid, and
Ingram Olkin. 2006. {``The Case of the Misleading Funnel Plot.''}
\emph{BMJ} 333 (7568): 597--600.
\url{https://doi.org/10.1136/bmj.333.7568.597}.

\leavevmode\vadjust pre{\hypertarget{ref-lefebvre2019searching}{}}%
Lefebvre, Carol, Julie Glanville, Simon Briscoe, Anne Littlewood, Chris
Marshall, Maria-Inti Metzendorf, Anna Noel-Storr, et al. 2019.
{``Searching for and Selecting Studies.''} \emph{Cochrane Handbook for
Systematic Reviews of Interventions}, 67--107.

\leavevmode\vadjust pre{\hypertarget{ref-smt}{}}%
Maier, Maximilian, Tyler J VanderWeele, and Maya B Mathur. in press.
{``Using Selection Models to Assess Sensitivity to Publication Bias: A
Tutorial and Call for More Routine Use.''} \emph{Campbell Systematic
Reviews}, in press.

\leavevmode\vadjust pre{\hypertarget{ref-mathur_mam}{}}%
Mathur, Maya B, and Tyler J VanderWeele. 2019. {``New Metrics for
Meta-Analyses of Heterogeneous Effects.''} \emph{Statistics in Medicine}
38 (8): 1336--42.

\leavevmode\vadjust pre{\hypertarget{ref-npphat}{}}%
---------. 2020a. {``Robust Metrics and Sensitivity Analyses for
Meta-Analyses of Heterogeneous Effects.''} \emph{Epidemiology} 31 (3):
356--58.

\leavevmode\vadjust pre{\hypertarget{ref-sapb}{}}%
---------. 2020b. {``Sensitivity Analysis for Publication Bias in
Meta-Analyses.''} \emph{Journal of the Royal Statistical Society: Series
C} 5 (69): 1091--1119.

\leavevmode\vadjust pre{\hypertarget{ref-sapbe}{}}%
---------. 2021. {``Estimating Publication Bias in Meta-Analyses of
Peer-Reviewed Studies: A Meta-Meta-Analysis Across Disciplines and
Journal Tiers.''} \emph{Research Synthesis Methods} 12 (2): 176--91.

\leavevmode\vadjust pre{\hypertarget{ref-art}{}}%
---------. 2022. {``Methods to Address Confounding and Other Biases in
Meta-Analyses: Review and Recommendations.''} \emph{Annual Review of
Public Health} 1 (43).

\leavevmode\vadjust pre{\hypertarget{ref-mcshane2016adjusting}{}}%
McShane, Blakeley B, Ulf Bckenholt, and Karsten T Hansen. 2016.
{``Adjusting for Publication Bias in Meta-Analysis: An Evaluation of
Selection Methods and Some Cautionary Notes.''} \emph{Perspectives on
Psychological Science} 11 (5): 730--49.
\url{https://doi.org/10.1177/1745691616662243}.

\leavevmode\vadjust pre{\hypertarget{ref-mcshane2017statistical}{}}%
McShane, Blakeley B, and David Gal. 2017. {``Statistical Significance
and the Dichotomization of Evidence.''} \emph{Journal of the American
Statistical Association} 112 (519): 885--95.
\url{https://doi.org/10.1080/01621459.2017.1289846}.

\leavevmode\vadjust pre{\hypertarget{ref-nelson1986interpretation}{}}%
Nelson, Nanette, Robert Rosenthal, and Ralph L Rosnow. 1986.
{``Interpretation of Significance Levels and Effect Sizes by
Psychological Researchers.''} \emph{American Psychologist} 41 (11):
1299.

\leavevmode\vadjust pre{\hypertarget{ref-paluck2019contact}{}}%
Paluck, Elizabeth Levy, Seth A Green, and Donald P Green. 2019. {``The
Contact Hypothesis Re-Evaluated.''} \emph{Behavioural Public Policy} 3
(2): 129--58.

\leavevmode\vadjust pre{\hypertarget{ref-pustejovsky2021meta}{}}%
Pustejovsky, James E, and Elizabeth Tipton. 2021. {``Meta-Analysis with
Robust Variance Estimation: Expanding the Range of Working Models.''}
\emph{Prevention Science}, 1--14.

\leavevmode\vadjust pre{\hypertarget{ref-riley2011interpretation}{}}%
Riley, Richard D, Julian PT Higgins, and Jonathan J Deeks. 2011.
{``Interpretation of Random Effects Meta-Analyses.''} \emph{BMJ} 342.

\leavevmode\vadjust pre{\hypertarget{ref-scheibehenne2016}{}}%
Scheibehenne, Benjamin, Tahira Jamil, and Eric-Jan Wagenmakers. 2016.
{``Bayesian Evidence Synthesis Can Reconcile Seemingly Inconsistent
Results: The Case of Hotel Towel Reuse.''} \emph{Psychol. Sci.} 27 (7):
1043--46.

\leavevmode\vadjust pre{\hypertarget{ref-simonsohn2014p}{}}%
Simonsohn, Uri, Leif D Nelson, and Joseph P Simmons. 2014. {``P-Curve: A
Key to the File-Drawer.''} \emph{Journal of Experimental Psychology:
General} 143 (2): 534.

\leavevmode\vadjust pre{\hypertarget{ref-sterne2016robins}{}}%
Sterne, Jonathan AC, Miguel A Hernn, Barnaby C Reeves, Jelena Savovi,
Nancy D Berkman, Meera Viswanathan, David Henry, et al. 2016.
{``ROBINS-i: A Tool for Assessing Risk of Bias in Non-Randomised Studies
of Interventions.''} \emph{Bmj} 355.

\leavevmode\vadjust pre{\hypertarget{ref-thompson2002should}{}}%
Thompson, Simon G, and Julian PT Higgins. 2002. {``How Should
Meta-Regression Analyses Be Undertaken and Interpreted?''}
\emph{Statistics in Medicine} 21 (11): 1559--73.

\leavevmode\vadjust pre{\hypertarget{ref-tipton2015small}{}}%
Tipton, Elizabeth. 2015. {``Small Sample Adjustments for Robust Variance
Estimation with Meta-Regression.''} \emph{Psychological Methods} 20 (3):
375.

\leavevmode\vadjust pre{\hypertarget{ref-tsuji2020addressing}{}}%
Tsuji, Sho, Alejandrina Cristia, Michael C Frank, and Christina
Bergmann. 2020. {``Addressing Publication Bias in Meta-Analysis.''}
\emph{Zeitschrift f{}r Psychologie}.

\leavevmode\vadjust pre{\hypertarget{ref-van2015meta}{}}%
Van Assen, Marcel ALM, Robbie van Aert, and Jelte M Wicherts. 2015.
{``Meta-Analysis Using Effect Size Distributions of Only Statistically
Significant Studies.''} \emph{Psychological Methods} 20 (3): 293.

\leavevmode\vadjust pre{\hypertarget{ref-vevea1995}{}}%
Vevea, Jack L, and Larry V Hedges. 1995. {``A General Linear Model for
Estimating Effect Size in the Presence of Publication Bias.''}
\emph{Psychometrika} 60 (3): 419--35.

\leavevmode\vadjust pre{\hypertarget{ref-viechtbauer2010}{}}%
Viechtbauer, Wolfgang. 2010. {``Conducting Meta-Analyses in r with the
Metafor Package.''} \emph{Journal of Statistical Software} 36 (3):
1--48.

\leavevmode\vadjust pre{\hypertarget{ref-wang2019simple}{}}%
Wang, Chia-Chun, and Wen-Chung Lee. 2019. {``A Simple Method to Estimate
Prediction Intervals and Predictive Distributions: Summarizing
Meta-Analyses Beyond Means and Confidence Intervals.''} \emph{Research
Synthesis Methods} 10 (2): 255--66.

\end{CSLReferences}

\hypertarget{sec-conclusions}{%
\chapter{Conclusions}\label{sec-conclusions}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Synthesize the viewpoint of this book
\item
  Discuss the outlook for psychology
\end{itemize}

\end{tcolorbox}

You've made it to the end of Experimentology, our (sometimes
opinionated) guide to how to run psychology experiments.

\hypertarget{summarizing-our-approach}{%
\section{Summarizing our approach}\label{summarizing-our-approach}}

In this book we've tried to present a single unified approach to the why
and how of running experiments. This approach begins with the goal of
doing experiments:

\begin{quote}
Experiments are intended to make maximally unbiased, generalizable, and
precise estimates of specific causal effects.
\end{quote}

This formulation isn't exactly how experiments are talked about in the
broader field, but we hope you've started to see some of the rationale
behind this approach.

\begin{itemize}
\item
  The emphasis on causal effects stems from an acknowledgement of the
  key role of experiments in establishing causal inferences
  (\textbf{?@sec-experiments}) and the importance of causal
  relationships to theories (\textbf{?@sec-theories}).
\item
  A desire for specificity informs our preference for simple
  experimental designs (\textbf{?@sec-design}).
\item
  We select our sample in order to maximize the generalizability of our
  findings to some target population (\textbf{?@sec-sampling}).
\item
  We also try to minimize the possibility of bias through our decisions
  about data collection (\textbf{?@sec-collection}) and data analysis
  (\textbf{?@sec-prereg}).
\item
  A focus on estimation (\textbf{?@sec-estimation}) helps us avoid some
  of the fallacies that come along with dichotomous inference
  (\textbf{?@sec-inference}), and allows us to think meta-analytically
  (\textbf{?@sec-meta}) about the overall evidence for a particular
  effect.
\item
  The desire for precision informs our choice of reliable and valid
  measures (\textbf{?@sec-measurement}).
\end{itemize}

Woven throughout this narrative is the hope that embracing open science
values throughout the experimental process will help you maximize your
work. Not only is sharing your work openly an ethical responsibility
(\textbf{?@sec-ethics}), it's also a great way to minimize errors while
creating valuable products that both advance scientific progress and
accelerate your own career (\textbf{?@sec-management}).

\hypertarget{forward-the-field}{%
\section{Forward the field}\label{forward-the-field}}

As we've learned at various points in this book, there's a replication
crisis (\protect\hyperlink{ref-osc2015}{Open Science Collaboration
2015}), a theory crisis (\protect\hyperlink{ref-oberauer2019}{Oberauer
and Lewandowsky 2019}), and a generalizability crisis
(\protect\hyperlink{ref-yarkoni2020}{Yarkoni 2020}) in psychology. Based
on all these crises, you might think that we are pessimistic about the
future of psychology. Not so.

There have been tremendous changes in psychological methods in the ten
years since we began teaching our course (2012 -- 2022). When we began,
it was common for incoming graduate students to describe the rampant
\(p\)-hacking they had been encouraged to do in their undergraduate
labs. Now, students join the class aware of new practices like
preregistration and cognizant of problems of generalizability and theory
building. It takes a long time for a field to change, but we have seen
tremendous progress at every level -- from US government policies
requiring transparency in the sciences all the way down to individual
researchers' adoption of tools and practices that increase the
efficiency of their work while also decreasing the chances of error.

One of the most exciting trends has been the rise of meta-science, in
which researchers use the tools of science to understand how to make
science better (\protect\hyperlink{ref-hardwicke2020b}{Tom E. Hardwicke
et al. 2020}). Reproducibility and replicability projects (reviewed in
\textbf{?@sec-replication}) can help us measure the robustness of the
scientific literature. In addition, studies that evaluate the impacts of
new policies (e.g., \protect\hyperlink{ref-hardwicke2018b}{Tom E.
Hardwicke et al. 2018}) -- can help stakeholders like journal editors
and funders make informed choices about how to push the field towards
more robust science.

In addition to changes that correct methodological issues, the last ten
years have seen the rise of ``big team science'' efforts that advance
the field in new ways (\protect\hyperlink{ref-coles2022}{Coles et al.
2022}). Collaborations such as the Psychological Science Accelerator
(\protect\hyperlink{ref-moshontz2018}{Moshontz et al. 2018}) and
ManyBabies (\protect\hyperlink{ref-frank2017b}{Frank et al. 2017}) allow
hundreds of researchers from around the world to come together to run
shared projects. These projects are enabled by open science practices
like data and code sharing, and they provide a way for researchers to
learn best practices via participating. In addition, by including
broader and more diverse samples they can help address challenges around
generalizability (\protect\hyperlink{ref-klein2018b}{Klein et al.
2018}).

Finally, the last ten years have seen huge progress in the use of
statistical models both for understanding data
(\protect\hyperlink{ref-mcelreath2018}{McElreath 2018}) and for
describing specific psychological mechanisms
(\protect\hyperlink{ref-ma2022}{Ma, Krding, and Goldreich 2022}). In
our own work we have used these models extensively and we believe that
they provide an exciting toolkit for building quantitative theories that
allow us to explain and to predict the human mind.

\hypertarget{final-thoughts}{%
\section{Final thoughts}\label{final-thoughts}}

Doing experiments is a craft, one that requires practice and attention.
The first experiment you run will have limitations and issues. So will
the 100th. But as you refine your skills, the quality of the studies you
design will get better. Further, your own ability to judge others'
experiments will improve as well, making you a more discerning consumer
of empirical results. We hope you enjoy this journey!

\leavevmode\vadjust pre{\hypertarget{appendices}{}}%
\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\hypertarget{bibliography-22}{%
\section*{References}\label{bibliography-22}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-22}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-coles2022}{}}%
Coles, Nicholas A, J Kiley Hamlin, Lauren L Sullivan, Timothy H Parker,
and Drew Altschul. 2022. {``Build up Big-Team Science.''} Nature
Publishing Group.

\leavevmode\vadjust pre{\hypertarget{ref-frank2017b}{}}%
Frank, Michael C, Elika Bergelson, Christina Bergmann, Alejandrina
Cristia, Caroline Floccia, Judit Gervain, J Kiley Hamlin, et al. 2017.
{``A Collaborative Approach to Infant Research: Promoting
Reproducibility, Best Practices, and Theory-Building.''} \emph{Infancy}
22 (4): 421--35.

\leavevmode\vadjust pre{\hypertarget{ref-hardwicke2018b}{}}%
Hardwicke, Tom E, Maya B Mathur, Kyle Earl MacDonald, Gustav Nilsonne,
George Christopher Banks, Mallory Kidwell, Alicia Hofelich Mohr, et al.
2018. {``Data Availability, Reusability, and Analytic Reproducibility:
Evaluating the Impact of a Mandatory Open Data Policy at the Journal
Cognition.''}

\leavevmode\vadjust pre{\hypertarget{ref-hardwicke2020b}{}}%
Hardwicke, Tom E., Stylianos Serghiou, Perrine Janiaud, Valentin
Danchev, Sophia Crwell, Steven N. Goodman, and John P. A. Ioannidis.
2020. {``Calibrating the Scientific Ecosystem Through Meta-Research.''}
\emph{Annual Review of Statistics and Its Application} 7 (1): 11--37.
\url{https://doi.org/10.1146/annurev-statistics-031219-041104}.

\leavevmode\vadjust pre{\hypertarget{ref-klein2018b}{}}%
Klein, Richard A, Michelangelo Vianello, Fred Hasselman, Byron G Adams,
Reginald B Adams Jr, Sinan Alper, Mark Aveyard, et al. 2018. {``Many
Labs 2: Investigating Variation in Replicability Across Samples and
Settings.''} \emph{Advances in Methods and Practices in Psychological
Science} 1 (4): 443--90.

\leavevmode\vadjust pre{\hypertarget{ref-ma2022}{}}%
Ma, Wei Ji, K Krding, and Daniel Goldreich. 2022. \emph{Bayesian Models
of Perception and Action: An Introduction}. unpublished.

\leavevmode\vadjust pre{\hypertarget{ref-mcelreath2018}{}}%
McElreath, Richard. 2018. \emph{Statistical Rethinking: A Bayesian
Course with Examples in r and Stan}. Chapman; Hall/CRC.

\leavevmode\vadjust pre{\hypertarget{ref-moshontz2018}{}}%
Moshontz, Hannah, Lorne Campbell, Charles R Ebersole, Hans IJzerman,
Heather L Urry, Patrick S Forscher, Jon E Grahe, et al. 2018. {``The
Psychological Science Accelerator: Advancing Psychology Through a
Distributed Collaborative Network.''} \emph{Adv Methods Pract Psychol
Sci} 1 (4): 501--15.

\leavevmode\vadjust pre{\hypertarget{ref-oberauer2019}{}}%
Oberauer, Klaus, and Stephan Lewandowsky. 2019. {``Addressing the Theory
Crisis in Psychology.''} \emph{Psychonomic Bulletin \& Review} 26 (5):
1596--1618.

\leavevmode\vadjust pre{\hypertarget{ref-osc2015}{}}%
Open Science Collaboration. 2015. {``Estimating the Reproducibility of
Psychological Science.''} \emph{Science} 349 (6251).

\leavevmode\vadjust pre{\hypertarget{ref-yarkoni2020}{}}%
Yarkoni, Tal. 2020. {``The Generalizability Crisis.''} \emph{Behav.
Brain Sci.} 45: 1--37.

\end{CSLReferences}

\hypertarget{sec-instructors}{%
\chapter{Instructor's guide}\label{sec-instructors}}

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

This is an instructor's guide to conducting replication projects in
courses. In addition to benefiting the field in ways that have been
previously discussed by some of the authors of this book (e.g., Hawkins
et al. (\protect\hyperlink{ref-hawkins2018}{2018}), Frank and Saxe
(\protect\hyperlink{ref-frank2012}{2012})), replication-based courses
can additionally benefit students in these courses. In this guide, we
will describe these benefits, explore different ways in which courses
may be modified depending on student level and resources, and provide
some guidelines and examples to help you set up the logistics of your
course.

\hypertarget{why-teach-a-project-based-course}{%
\section{Why Teach a Project-Based
Course?}\label{why-teach-a-project-based-course}}

Over the years, we have observed many ways in which our
replication-based courses benefited students above and beyond a more
traditional lecture and problem set-based course. Some of these benefits
include:

\begin{itemize}
\tightlist
\item
  \textbf{Student interest}: Since each student will be free to
  replicate a study that is aligned with their research interests, this
  freedom facilitates a more direct application of course methods and
  lessons to a project that is interesting to each student.
\item
  \textbf{Usefulness}: If this course is taught in the first year of the
  program (as recommended), students may use their replication project
  as a way to establish robustness of a phenomenon before building
  studies on top of it.
\item
  \textbf{Realism}: Practice datasets that are typically provided for
  course exercises lack the complexity and messiness of real data. By
  conducting a replication project and dealing with real data, students
  learn to apply the tools provided in the course in a way that more
  closely demonstrates their usefulness beyond the course.
\item
  \textbf{Intuition}: Presentations of replication outcomes across the
  class along with a discussion of what factors seemed to predict these
  outcomes helps students develop a better intuition when reading the
  literature for how likely studies are to replicate.
\item
  \textbf{Perspective}: Frustrating experiences with ambiguity (whether
  regarding experimental methods, materials, or analyses) can motivate
  students to adopt best practices for their own future studies.
\end{itemize}

A project-based course may look very different depending on student
level (undergraduate vs.~graduate/post-doc level) and availability of
resources at your institution for a course like this, namely in terms of
TA support and course funding (for data collection). For most of this
guide, we will assume that you have a similar setup to ours (i.e.,
teaching at the graduate/post-doc level and have course funding and TAs
to support the course), but we have also spent some time considering
ways to adjust the course to fit different student levels and
availability of resources (see ``Scenarios for different course
layouts'').

\hypertarget{logistics}{%
\section{Logistics}\label{logistics}}

\hypertarget{syllabus-considerations}{%
\subsection{Syllabus considerations}\label{syllabus-considerations}}

If it is your first time teaching this course, you may want to decide
ahead of time whether your course will mainly focus on content, or
whether you will cover \emph{both} content and relevant practical
skills. For instance, if the course is for undergraduate students, you
may decide to focus mainly on content, whereas if the course is for
graduate students, they may find it more useful if the course covers
both content and practical skills they can use in their research.

Another important consideration is how long your course will be.
Depending on whether your university operates on quarters or semesters,
the pace of the course will differ. For Psych 251, since we are on the
quarter system, we use the 10-week schedule shown below. However, we
have also adapted this schedule to a 16-week course given that it better
represents a majority of other institutions' academic calendars. At the
end of this chapter, we give a set of sample class schedules.

\hypertarget{grading}{%
\subsection{Grading}\label{grading}}

Depending on your course format and teaching philosophy, you may have
preferred grading criteria. As a point of reference, in Psych 251, we
wanted to encompass both the assignments (problemsets and project
components) as well as actual course attendance and participation. In
addition, because the replication project is a central part of the
course, we weighted the project components slightly more than the
problem sets:

\begin{itemize}
\tightlist
\item
  40\%: Problem sets (four, at 10\% each)
\item
  50\%: Final project components, including presentations, data
  collection, analysis, and writeup
\item
  10\%: Attendance and participation in class
\end{itemize}

\hypertarget{course-budget}{%
\subsection{Course budget}\label{course-budget}}

For our course, we usually receive around US\$1,000 for course funding
from the Psychology Department. In addition, when students from other
departments are enrolled, we have been lucky to receive additional
funding from those departments as well, to further support the course.
Still, making sure that the course funds cover all students' projects is
one of the most challenging parts of the course. Assuming you have a
budget to work with, here are some lessons we've learned along the way
regarding budgeting (and if you don't have any funding, please refer to
the section titled ``Course Funding'' under ``Scenarios for different
course layouts''):

\begin{itemize}
\tightlist
\item
  Before students pick their study to replicate, provide them with an
  estimate of how many participant hours they will be able to receive
  for their project
\item
  As soon as students pick a study for their replication project, help
  each student run a power analysis to confirm that replicating the
  study would be within the budget (TAs can help with this)
\item
  If a student feels strongly about a study that does not fit within the
  budget, consider the following ways to adjust the study: 1) can the
  study be made shorter by cutting out unnecessary measures? 2) if it is
  a multi-trial study, can the number of trials be reduced? 3) would
  their advisors be willing to provide additional funding? 4) can the
  study be run on university participant pools?
\item
  As mentioned above, if there are students from other departments who
  are enrolled in your course, one possibility to obtain more funding is
  to reach out to the heads of those departments to see whether they
  would be willing to help support your course.
\end{itemize}

Once all projects have been approved as within-budget, we encourage you
to create a shared spreadsheet containing each student's name, so that
they can fill in the details of their replication project. Ultimately,
this will help ensure that students are paying fair wages to their
participants and keep track of how the course funds are being divided
up.

\hypertarget{course-related-institutional-review-board-application}{%
\subsection{Course-related Institutional Review Board
application}\label{course-related-institutional-review-board-application}}

While it may be possible to apply for individual IRB approval for each
student's project, we recommend applying for course-wide standard IRB
approval for all replication projects that are conducted in your class.
Contacting your review board early in the planning stages of the course
should clarify what options you have available.

One important thing to remember when students run their individual
projects is that they should have the course-wide consent form at the
beginning of their studies (and TAs should check this when they review
the paradigms). For reference, this is the consent form that each
student is required to post at the beginning of their study:

``By answering the following questions, you are participating in a study
being performed by cognitive scientists in the Stanford Department of
Psychology. If you have questions about this research, please contact us
at stanfordpsych251@gmail.com. You must be at least 18 years old to
participate. Your participation in this research is voluntary. You may
decline to answer any or all of the following questions. You may decline
further participation, at any time, without adverse consequences. Your
anonymity is assured; the researchers who have requested your
participation will not receive any personal information about you.''

\hypertarget{scenarios-for-different-course-layouts}{%
\section{Scenarios for different course
layouts}\label{scenarios-for-different-course-layouts}}

Now that we have covered the standard format of the course, we want to
now turn our attention to ways in which this format can be tweaked in
order to fit different needs and resources. We have organized this
section into two main categories: student level, and course resources
(such as TAs and course funding).

\hypertarget{student-level}{%
\subsection{Student level}\label{student-level}}

While Psych 251 at Stanford is geared towards graduate students (and is
currently a required class for entering first-year graduate students in
the Psychology Department), we also accept advanced undergraduate
students as well as graduate students from other departments (e.g.,
Education, Human-Computer Interaction, Philosophy, Computer Science). On
the first day of our course, we tell students that they should be
comfortable with two of the three following topics:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Some knowledge of psychological experimentation \& subject matter
\item
  Statistical programming: things like functions and variables
\item
  Basic statistics like ANOVA and t-test
\end{enumerate}

If students are only comfortable with one of the three topics above, we
warn them ahead of time that the course may demand more time from them
than the average student.

Now, if you are planning on catering this course for undergraduate
students, chances are that they have had less exposure to these topics
overall, so there are multiple ways to calibrate the course accordingly:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \textbf{Prerequisites}: Require students to have completed courses
  that cover at least two of the three topics mentioned above (i.e., a
  psychology class, a class that covers statistical programming, a class
  that covers basic statistics, any two of the three).
\item
  \textbf{Pace}: unlike Psych 251, where the entire course only lasts 10
  weeks, a class for undergraduates may benefit from a slower pace,
  allowing more time to cover the foundational principles before diving
  into the project. For instance, the course could be held over multiple
  academic semesters/quarters, with the project goal of Course \#1 being
  choosing and planning the replication study, and the project goal of
  Course \#2 being the execution and interpretation of the replication.
\item
  \textbf{Pair-Group-Based Projects}: In our course, each student is
  required to conduct their own replication project. However, this
  structure may be overwhelming for undergraduate students who may have
  less confidence taking on an entire replication project by themselves.
  One option that may alleviate this pressure is to have students
  conduct these projects as pairs or as small teams, so that they can
  collectively draw on each others' strengths. When assigning these
  pairs or teams, it may be especially helpful to try to ensure a
  relatively even balance of students who are confident in each of the
  three areas outlined above (psychology, statistical programming, basic
  statistics).
\end{enumerate}

Now that we've offered a few suggestions to address different student
levels, let's dive into the issue of course resources.

\hypertarget{course-resources}{%
\subsection{Course resources}\label{course-resources}}

We think there are two main ways in which your course may have different
resources from our model: In terms of course assistance (i.e., teaching
assistants), and in terms of course funding for student projects. We'll
explore ways to work around each of these in this section:

\textbf{Teaching assistants}

As a point of comparison, in general, 2-3 teaching assistants are
allocated to Psych 251, which enrolls about 36 students, which comes out
to about 12-18 students per TA. Since a project-based course requires
individual attention and feedback, we would recommend against a
student-TA ratio that is much higher than that. That means that if you
know you will have just one TA for the class, you should think about
reducing the enrollment cap accordingly. But what if you have \emph{no}
TAs? With some adjustments, there are still ways you can make the course
work sans-TA; we outline a few ideas below:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \textbf{Peer grading}: As an instructor with no TAs, the area that
  will require the biggest lift in terms of time and attention is
  grading. One way to overcome this is to introduce a peer-grading
  system, in which students grade each others' work. If you choose this
  route, two things that may encourage fair grading among your students
  is to 1) distribute a clear and specific rubric that reduces the
  amount of subjectivity in the grading process as much as possible, and
  2) anonymize the assignments so that students do not know whose
  assignment they are grading. If possible, it may again be beneficial
  to assign grading pairs that consist of students that are relatively
  knowledgeable in different areas, so that they can provide feedback
  that address weak points in each others' work.
\item
  \textbf{Collective troubleshooting}: The second most time intensive
  area you will have to make up for is the amount of troubleshooting you
  may have to do for students who run into issues implement their
  projects, anywhere from getting GitHub and RMarkdown up and running on
  their devices, to trouble with data collection on Mechanical Turk. One
  way to encourage communal support among your students is to set up a
  central discussion board for the course (e.g., Piazza or a course
  channel on Slack) where students can publicly (but anonymously, if
  desired) post issues they are running into. Then, you can offer extra
  credit to students who help troubleshoot these issues, in order to
  further incentivize collective troubleshooting. There will likely
  still be issues that cannot be addressed by the students, but this
  system at least frees up your time to focus your attention on those
  that only \emph{you} can address.
\item
  \textbf{Single class-wide project}: Finally, if the collective grading
  and troubleshooting methods outlined above do not cut down on enough
  time, you could consider walking through a single replication project
  as a class.\sidenote{\footnotesize This approach does cancel out some of the
    benefits of a project-based course we mentioned at the start --
    namely, the project will likely no longer fit each student's
    specific research interest, so there may be less benefit in terms of
    specific student interest and usefulness for their program of
    research, but the other two benefits of realism and intuition
    (especially if the project is discussed in the context of other
    replication findings) still stand.} To make a single-project course
  work, you could have students nominate studies they would like to
  replicate as a class, and then have them vote on the final choice.
  Once the target study has been selected, every student can
  individually carry out all the steps of the project, including
  preregistering and writing up the analysis script. Then, setting up
  and running the data collection phase can happen during class, and
  once data has been collected, you can distribute it to the students
  for them to run it through their analysis script and interpret the
  result. Whether you choose to have students grade each others' work or
  whether you grade their work yourself, the fact that the project is
  standardized should cut down on a lot of the time you would otherwise
  spend learning about the details of every individual project.
\end{enumerate}

\textbf{Course funding}

In addition to availability of TAs, another way in which your course may
be different from ours is in terms of course funding. If you have little
or not funding for your course (even after reaching out to relevant
members of your department or institution), we suggest the following
adjustments:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  \textbf{Pair-Group-Based Projects}: Similarly to suggestion \#3 for
  addressing different student levels, one option for limited course
  budgets is to have students conduct the replication projects as pairs
  or teams to reduce the cost of data collection. This structure may
  have the added benefit of encouraging students to problem-solve
  together. Alternatively, each student in the pairs or teams could
  complete each step of the replication individually (e.g., writing up
  the report, analyzing the data, interpreting the result), which would
  ensure that each student takes full responsibility for every step of
  the project. This structure may also provide opportunities for
  interesting discussions at the end of the course around analytic
  reproducibility, especially if students in the same teams (with the
  same dataset) differed in the conclusions they drew about the
  replication outcome.
\item
  \textbf{Funding from Advisors}: In some cases, students come to us
  with target studies that require more funding than we are able to
  allocate, but that they feel particularly invested in (e.g., because
  of how relevant the study is to their line of research). Once we rule
  out other ways of making the study fit our budget (e.g., dropping
  extra control conditions, running a subset of the study), we often ask
  students whether their advisor would be willing to fund the study. We
  have found that advisors are often willing to do this, especially if
  the replication could serve an important role in the development of
  the student's research program. Similarly, one way to reduce the
  burden on a limited course budget would be to encourage all students
  to first ask their advisors about whether they would be willing to
  fund part or all of the data collection for the replication. While
  chances are that some advisors will be unwilling or unable to do this,
  there should still be a meaningful reduction in the number of projects
  the course will need to fund.
\item
  \textbf{Reproduce a Replication}: The suggestions above apply if you
  at least have \emph{some} amount of course funding, but what if you
  have \emph{no} funding at all? While there are obvious limitations to
  this solution, one suggestion is to have students reproduce past
  public replications. For instance, our
  \href{https://github.com/psych251}{course Github page}, contains
  public repositories of all past replication projects that have been
  conducted in our course. Since the data for each replication project
  is available in these repositories, you could provide each of your
  students with a dataset and the original paper associated with it, and
  assign them to reproduce the results of the replication. Students
  should then be able to follow each step of the replication project
  described below (e.g., writing the report, identifying the key
  analysis, running the analysis). This format will only work if
  students do not view the original final replication reports that are
  posted publicly for their project, so it may be necessary to be clear
  about this at the beginning of the course.
\end{enumerate}

For those of you who are working with a different course format (whether
in terms of student level or course resources), we hope these
suggestions were useful. If you try out a new idea in your course that
you found helpful, we would be thrilled if you shared them with us!

\hypertarget{sample-course-schedules}{%
\section{Sample course schedules}\label{sample-course-schedules}}

The sample syllabi laid out below are categorized along the following
decisions: 1) Material: whether the course focuses on just content or
both content and skills, and 2) Duration: whether the course is 10-weeks
long or 16-weeks long.

For undergraduate instructors, we have labelled advanced topics in
purple. We expect that these topics are best suited for advanced
undergraduate students. As for content around statistics (e.g.,
Estimation, Inference), instructors should decide how much of this
content to teach, depending on how prepared students have been in
previous classes.

\clearpage

\subsection{10 weeks}

\hypertarget{tbl-skills-content-10}{}
\begin{longtable}{lllll}
\caption{\label{tbl-skills-content-10}A sample 10-week syllabus with both skills and content materials. }\tabularnewline

\toprule
Week & Day & Topic & Chapter & Appendix\\
\midrule
1 & M & Class Introduction & 1 & \\
1 & W & Theories & 2 & \\
1 & F & Version Control &  & B\\
\hline
2 & M & Reproducible reports & 14 & C\\
\cellcolor[HTML]{ebe8fc}{2} & \cellcolor[HTML]{ebe8fc}{W} & \cellcolor[HTML]{ebe8fc}{Tidyverse Tutorial} & \cellcolor[HTML]{ebe8fc}{} & \cellcolor[HTML]{ebe8fc}{D}\\
\cellcolor[HTML]{ebe8fc}{2} & \cellcolor[HTML]{ebe8fc}{F} & \cellcolor[HTML]{ebe8fc}{Tidyverse Tutorial continued (with TAs)} & \cellcolor[HTML]{ebe8fc}{} & \cellcolor[HTML]{ebe8fc}{}\\
\hline
3 & M & Measurement, Reliability, and Validity & 8 & \\
3 & W & Design of Experiments & 9 & \\
3 & F & Sampling & 10 & \\
\hline
4 & M & Project Management & 13 & \\
4 & W & Experiments 1: Simple survey experiments using Qualtrics &  & \\
4 & F & Experiments 2: Project-specific Implementation (TAs) &  & \\
\hline
5 & M & Estimation & 5 & \\
5 & W & Inference & 6 & \\
5 & F & Sample Size Planning &  & \\
\hline
6 & M & Survey Design &  & \\
6 & W & Midterm Presentations 1 &  & \\
6 & F & Midterm Presentations 2 &  & \\
\hline
7 & M & Preregistration & 11 & \\
\cellcolor[HTML]{ebe8fc}{7} & \cellcolor[HTML]{ebe8fc}{W} & \cellcolor[HTML]{ebe8fc}{Meta-analysis} & \cellcolor[HTML]{ebe8fc}{16} & \cellcolor[HTML]{ebe8fc}{}\\
7 & F & Open Science & 3 & \\
\hline
\cellcolor[HTML]{ebe8fc}{8} & \cellcolor[HTML]{ebe8fc}{M} & \cellcolor[HTML]{ebe8fc}{Visualization 1} & \cellcolor[HTML]{ebe8fc}{15} & \cellcolor[HTML]{ebe8fc}{E}\\
\cellcolor[HTML]{ebe8fc}{8} & \cellcolor[HTML]{ebe8fc}{W} & \cellcolor[HTML]{ebe8fc}{Visualization 2} & \cellcolor[HTML]{ebe8fc}{} & \cellcolor[HTML]{ebe8fc}{}\\
\cellcolor[HTML]{ebe8fc}{8} & \cellcolor[HTML]{ebe8fc}{F} & \cellcolor[HTML]{ebe8fc}{Exploratory Data Analysis Workshop} & \cellcolor[HTML]{ebe8fc}{} & \cellcolor[HTML]{ebe8fc}{}\\
\hline
9 & M & Sampling, Representativeness, and Generalizability & 4 & \\
9 & W & Data and Participants Ethics & 12 & \\
9 & F & Authorship and Research Ethics &  & \\
\hline
10 & M & Open Discussion & 17 & \\
10 & W & Final Project Presentations 1 &  & \\
10 & F & Final Project Presentations 2 &  & \\
\bottomrule
\end{longtable}

\clearpage

\subsection{10 weeks, content only}

\hypertarget{tbl-content-10}{}
\begin{longtable}{llll}
\caption{\label{tbl-content-10}A sample 10-week syllabus with only content materials. }\tabularnewline

\toprule
Week & Day & Topic & Chapter\\
\midrule
1 & M & Class Introduction & 1\\
1 & W & Theories & 2\\
1 & F & Replication and reproducibility & 3\\
\hline
2 & M & Open Science & \\
2 & W & Measurement & 8\\
2 & F & Design of experiments 1 & 9\\
\hline
3 & M & Design of experiments 2 & \\
3 & W & Sampling & 10\\
3 & F & Experimental strategy & \\
\hline
4 & M & Preregistration & 11\\
4 & W & Data collection & 12\\
\cellcolor[HTML]{ebe8fc}{4} & \cellcolor[HTML]{ebe8fc}{F} & \cellcolor[HTML]{ebe8fc}{Visualization 1} & \cellcolor[HTML]{ebe8fc}{15}\\
\hline
\cellcolor[HTML]{ebe8fc}{5} & \cellcolor[HTML]{ebe8fc}{M} & \cellcolor[HTML]{ebe8fc}{Visualization 2} & \cellcolor[HTML]{ebe8fc}{}\\
5 & W & MIDTERM EXAM & \\
5 & F & Introduction to statistics & \\
\hline
6 & M & Estimation 1 & 5\\
6 & W & Estimation 2 & \\
6 & F & Inference 1 & 6\\
\hline
7 & M & Inference 2 & \\
\cellcolor[HTML]{ebe8fc}{7} & \cellcolor[HTML]{ebe8fc}{W} & \cellcolor[HTML]{ebe8fc}{Models 1} & \cellcolor[HTML]{ebe8fc}{7}\\
\cellcolor[HTML]{ebe8fc}{7} & \cellcolor[HTML]{ebe8fc}{F} & \cellcolor[HTML]{ebe8fc}{Models 2} & \cellcolor[HTML]{ebe8fc}{}\\
\hline
\cellcolor[HTML]{ebe8fc}{8} & \cellcolor[HTML]{ebe8fc}{M} & \cellcolor[HTML]{ebe8fc}{Meta-analysis} & \cellcolor[HTML]{ebe8fc}{16}\\
8 & W & Project management & 13\\
8 & F & {}[Instructor-specific topics] & \\
\hline
9 & M & Sampling, Representativeness, and Generalizability & 4\\
9 & W & Data and Participants Ethics & 12\\
9 & F & Authorship and Research Ethics & \\
\hline
10 & M & Conclusion & 17\\
10 & W & Conclusion & \\
10 & F & FINAL EXAM & \\
\bottomrule
\end{longtable}

\clearpage

\subsection{16 weeks}

\hypertarget{tbl-skills-content-16}{}
\begin{longtable}{lllll}
\caption{\label{tbl-skills-content-16}A sample 16-week syllabus with both skills and content materials. }\tabularnewline

\toprule
Week & Day & Topic & Chapter & Appendix\\
\midrule
1 & 1 & Class Introduction & 1 & \\
1 & 2 & Theories & 2 & \\
2 & 1 & Version Control &  & B\\
\hline
2 & 2 & Reproducible reports & 14 & C\\
\cellcolor[HTML]{ebe8fc}{3} & \cellcolor[HTML]{ebe8fc}{1} & \cellcolor[HTML]{ebe8fc}{Tidyverse Tutorial} & \cellcolor[HTML]{ebe8fc}{} & \cellcolor[HTML]{ebe8fc}{D}\\
\cellcolor[HTML]{ebe8fc}{3} & \cellcolor[HTML]{ebe8fc}{2} & \cellcolor[HTML]{ebe8fc}{Tidyverse Tutorial continued (with TAs)} & \cellcolor[HTML]{ebe8fc}{} & \cellcolor[HTML]{ebe8fc}{}\\
\hline
4 & 1 & Measurement, Reliability, and Validity & 8 & \\
4 & 2 & Design of Experiments & 9 & \\
5 & 1 & Sampling & 10 & \\
\hline
5 & 2 & Project Management & 13 & \\
6 & 1 & Experiments 1: Simple survey experiments using Qualtrics &  & \\
6 & 2 & Experiments 2: Project-specific Implementation (TAs) &  & \\
\hline
7 & 1 & Estimation & 5 & \\
7 & 2 & Inference & 6 & \\
8 & 1 & Sample Size Planning &  & \\
\hline
8 & 2 & Survey Design &  & \\
9 & 1 & Midterm Presentations 1 &  & \\
9 & 2 & Midterm Presentations 2 &  & \\
\hline
10 & 1 & Preregistration & 11 & \\
\cellcolor[HTML]{ebe8fc}{10} & \cellcolor[HTML]{ebe8fc}{2} & \cellcolor[HTML]{ebe8fc}{Meta-analysis} & \cellcolor[HTML]{ebe8fc}{16} & \cellcolor[HTML]{ebe8fc}{}\\
11 & 1 & Open Science & 3 & \\
\hline
\cellcolor[HTML]{ebe8fc}{11} & \cellcolor[HTML]{ebe8fc}{2} & \cellcolor[HTML]{ebe8fc}{Visualization 1} & \cellcolor[HTML]{ebe8fc}{15} & \cellcolor[HTML]{ebe8fc}{E}\\
\cellcolor[HTML]{ebe8fc}{12} & \cellcolor[HTML]{ebe8fc}{1} & \cellcolor[HTML]{ebe8fc}{Visualization 2} & \cellcolor[HTML]{ebe8fc}{} & \cellcolor[HTML]{ebe8fc}{}\\
\cellcolor[HTML]{ebe8fc}{12} & \cellcolor[HTML]{ebe8fc}{2} & \cellcolor[HTML]{ebe8fc}{Exploratory Data Analysis Workshop} & \cellcolor[HTML]{ebe8fc}{} & \cellcolor[HTML]{ebe8fc}{}\\
\hline
13 & 1 & Sampling, Representativeness, and Generalizability & 4 & \\
13 & 2 & Data and Participants Ethics & 12 & \\
14 & 1 & Authorship and Research Ethics &  & \\
\hline
14 & 2 & {}[Instructor-specific topics] &  & \\
15 & 1 & Open Discussion & 17 & \\
15 & 2 & Open Discussion &  & \\
\hline
16 & 1 & Final Project Presentations 1 &  & \\
16 & 2 & Final Project Presentations 2 &  & \\
\bottomrule
\end{longtable}

\clearpage

\subsection{16 weeks, content only}

\hypertarget{tbl-content-16}{}
\begin{longtable}{llll}
\caption{\label{tbl-content-16}A sample 16-week syllabus with only content materials. }\tabularnewline

\toprule
Week & Day & Topic & Chapter\\
\midrule
1 & 1 & Class Introduction & 1\\
1 & 2 & Theories & 2\\
2 & 1 & Replication and reproducibility & 3\\
\hline
2 & 2 & Open Science & \\
3 & 1 & Measurement & 8\\
3 & 2 & Design of experiments 1 & 9\\
\hline
4 & 1 & Design of experiments 2 & \\
4 & 2 & Sampling & 10\\
5 & 1 & Experimental strategy & \\
\hline
5 & 2 & Preregistration & 11\\
6 & 1 & Data collection & 12\\
\cellcolor[HTML]{ebe8fc}{6} & \cellcolor[HTML]{ebe8fc}{2} & \cellcolor[HTML]{ebe8fc}{Visualization 1} & \cellcolor[HTML]{ebe8fc}{15}\\
\hline
\cellcolor[HTML]{ebe8fc}{7} & \cellcolor[HTML]{ebe8fc}{1} & \cellcolor[HTML]{ebe8fc}{Visualization 2} & \cellcolor[HTML]{ebe8fc}{}\\
7 & 2 & MIDTERM EXAM & \\
8 & 1 & Introduction to statistics & \\
\hline
8 & 2 & Estimation 1 & 5\\
9 & 1 & Estimation 2 & \\
9 & 2 & Inference 1 & 6\\
\hline
10 & 1 & Inference 2 & \\
\cellcolor[HTML]{ebe8fc}{10} & \cellcolor[HTML]{ebe8fc}{2} & \cellcolor[HTML]{ebe8fc}{Models 1} & \cellcolor[HTML]{ebe8fc}{7}\\
\cellcolor[HTML]{ebe8fc}{11} & \cellcolor[HTML]{ebe8fc}{1} & \cellcolor[HTML]{ebe8fc}{Models 2} & \cellcolor[HTML]{ebe8fc}{}\\
\hline
\cellcolor[HTML]{ebe8fc}{11} & \cellcolor[HTML]{ebe8fc}{2} & \cellcolor[HTML]{ebe8fc}{Meta-analysis} & \cellcolor[HTML]{ebe8fc}{16}\\
12 & 1 & Project management & 13\\
12 & 2 & {}[Instructor-specific topics] & \\
\hline
13 & 1 & {}[Instructor-specific topics] & \\
13 & 2 & Sampling, Representativeness, and Generalizability & 4\\
14 & 1 & Data and Participants Ethics & \\
\hline
14 & 2 & Authorship and Research Ethics & \\
15 & 1 & Ethics: Open Discussion & \\
15 & 2 & Conclusion & 17\\
\hline
16 & 1 & Conclusion & \\
16 & 2 & FINAL EXAM & \\
\bottomrule
\end{longtable}

\hypertarget{bibliography-23}{%
\section*{References}\label{bibliography-23}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-23}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-frank2012}{}}%
Frank, Michael C, and Rebecca Saxe. 2012. {``Teaching Replication.''}
\emph{Perspectives on Psychological Science} 7: 595--99.

\leavevmode\vadjust pre{\hypertarget{ref-hawkins2018}{}}%
Hawkins, Robert D, Eric N Smith, Carolyn Au, Juan Miguel Arias, Rhia
Catapano, Eric Hermann, Martin Keil, et al. 2018. {``Improving the
Replicability of Psychological Science Through Pedagogy.''}
\emph{Advances in Methods and Practices in Psychological Science} 1 (1):
7--18.

\end{CSLReferences}

\hypertarget{sec-git}{%
\chapter{GitHub}\label{sec-git}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Explain what Git and GitHub are
\item
  Set up Git and GitHub on your own computer
\item
  Learn how to make changes to a repository on GitHub
\item
  Practice undoing a change you made on GitHub
\item
  How to tell Git which files or folders to ignore in a project.
\end{itemize}

\end{tcolorbox}

\hypertarget{introduction-1}{%
\section{Introduction}\label{introduction-1}}

Have you ever sent a collaborator what you \emph{think} is a final copy
of a manuscript, perhaps titled ``Manuscript final'' only to get back an
updated version from them called ``Manuscript final - JC''? So you make
the requested changes and send it back to them as ``Manuscript final -
JC FINAL'', just as you receive an email from another collaborator with
more edits in a file called ``Manuscript final - MF''? If this sounds
familiar, read on - we are here to help!

To begin, let us introduce you to Git.

\textbf{Git} is software that you can install and run locally on your
own computer, and it allows you to track changes to your files as you
are working on them. Git helps you manage nightmare scenarios like the
one described above, because it makes it easy to have different versions
of the same document, to easily work by yourself on the same project on
multiple computers, and to collaborate with other people (whether you're
working at different times or simultaneously)! In other words, Git is
like an undo button, but with labels showing you what changes were made
when and with a history that goes back to the very first change you ever
made to the project. This makes it ideal for collaborative projects.

But Git, as a version control system, is even more powerful in
combination with GitHub:

\textbf{GitHub} provides the service of storing and sharing your git
repositories\sidenote{\footnotesize ``repo'' is short for ``repository''-- repo is
  like a folder with files in it (that are usually related as part of a
  project), with an associated history of changes over time.} online.
Anyone can get a free account on GitHub and they provide free premium
accounts to students (see \href{https://education.github.com/}{here}).

To describe some of the most useful functionalities of Github, we will
set the stage by describing Github as a place where you can store your
work in folders called repositories. These repositories live in the
``cloud,'' in that they are accessible via internet (github.com) and
therefore allows you to access them from any device.

\includegraphics{images/git/cloud_1.png}

For instance, imagine that your collaborator has a project called
``hello'' - they have stored all of their code, materials, and analyses
into a repository called ``hello'' on Github:

\includegraphics{images/git/cloud_2.png}

As a new member of the project, you need to copy this repository onto
your own local device (e.g., your laptop) so that you can inspect the
repository and make your own changes. In Github speak, this initial step
of copying the repository onto your own device is called ``cloning'' the
repository:

\includegraphics{images/git/cloud_3.png}

Once you make your changes to the ``hello'' repository (e.g., add or
delete code, add or remove files), the changes will exist in your local
repository but you will need to take steps to have them be reflected in
back in the ``cloud'' where your collaborators can see those changes.
The first step you will need to do is to ``add'' and ``commit'' your
changes. ``Adding'' your changes will take a snapshot of the changes you
made. ``Committing'' your changes records these added changes. Note that
these are preparatory steps and all your changes are still local:

\includegraphics{images/git/cloud_4.png}

Now, you are ready to actually share your changes to the cloud, where
all of your collaborators can see what you changed This step is called
``pushing'' to Github:

\includegraphics{images/git/cloud_5.png} So far, we have described one
user's interaction with Github. And while using Github as a way to track
your own changes is a good idea, Github's real potential is unlocked
once you have many users collaborating on the same project. This allows
any collaborator to ``pull'' the most recent version of the repository
at any time, keep track of who made what change at what time, and easily
revert to previous versions:

\includegraphics{images/git/cloud_7.png}

Now that we've covered the different functionalities of Github on a
conceptual level, the next section will be a more practical tutorial for
how to use Github in the ways described above.

\hypertarget{review-basic-terminal-commands}{%
\section{Review basic terminal
commands}\label{review-basic-terminal-commands}}

In this tutorial we'll be working in Terminal. Here are a few useful
commands to be aware of:

\includegraphics{images/git/git_tutorial2.png}

(more \href{https://github.com/0nn0/terminal-mac-cheatsheet}{here})

\hypertarget{install-git}{%
\section{Install git}\label{install-git}}

Go to \url{https://git-scm.com/downloads} and install git.

(Windows users, open GitBash for the rest of the tutorial; Mac users,
open Terminal.)

\hypertarget{did-you-successfully-install}{%
\subsection{Did you successfully
install?}\label{did-you-successfully-install}}

In terminal, type:

\begin{quote}
\texttt{git\ -\/-version}
\end{quote}

to see the current version of git that is installed.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{**Mac troubleshooting**}

\NormalTok{try installing: git version 1.8.4.2}
\end{Highlighting}
\end{Shaded}

\hypertarget{other-versions}{%
\subsection{Other versions}\label{other-versions}}

This tutorial will focus on how to use Github from the terminal, but if
you prefer a simple point-and-click experience, you can install the
GitHub \href{https://desktop.github.com/}{desktop app}. Here is a
screenshot of what the desktop version looks like:

\includegraphics{images/git/desktop.png}

Other options include \href{https://www.sourcetreeapp.com/}{SourceTree}
(free), \href{https://www.git-tower.com/}{Tower} (not free, but powerful
and \textasciitilde\$25 with a student discount). It's also possible to
\href{http://happygitwithr.com/}{set up a git pane in RStudio}.

\hypertarget{set-your-name-and-email-address}{%
\subsection{Set your name and email
address}\label{set-your-name-and-email-address}}

Every Git commit uses this information. Type:

\begin{quote}
\texttt{git\ config\ -\/-global\ user.name\ "John\ Doe"}
\end{quote}

\begin{quote}
\texttt{git\ config\ -\/-global\ user.email\ johndoe@example.com}
\end{quote}

Also run this once (it ensures that git pushes in a sane manner):

\begin{quote}
\texttt{git\ config\ -\/-global\ push.default\ simple}
\end{quote}

\hypertarget{make-a-repo-on-github-clone-it-to-your-computer}{%
\section{Make a repo on GitHub, clone it to your
computer}\label{make-a-repo-on-github-clone-it-to-your-computer}}

Make an account on GitHub: \url{https://github.com/}.

Create a new empty public repository at \url{https://github.com/new}

Call it `hello' and make sure the ``Add a README file'' checkbox is
checked\sidenote{\footnotesize For future reference (feel free to ignore this!): you
  can also turn any directory that's already on your computer into a git
  repo by going to that directory using \texttt{cd} and then typing
  \texttt{git\ init}. Later, when you want to put the repo on GitHub,
  you go through the steps to make a new repo \emph{without initializing
  with a readme}, then from your directory on your computer type the
  following (replacing the red text):}.

\begin{Shaded}
\begin{Highlighting}[]

\NormalTok{**git remote add origin git@\textless{}span\textgreater{}github.\textless{}/span\textgreater{}com:\textless{}span style="color: red;"\textgreater{}your\_user\_name\textless{}/span\textgreater{}/\textless{}span style="color: red;"\textgreater{}repo\_name\textless{}/span\textgreater{}.git**}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{**git push {-}u origin main**}
\end{Highlighting}
\end{Shaded}

\hypertarget{setting-up-authentication}{%
\subsection{Setting up authentication}\label{setting-up-authentication}}

GitHub needs a way of knowing whether or not your computer is authorized
to read or write to the repository. For public repositories, anyone can
clone (without authentication) but only authorized users (like you and
collaborators you've added) can push changes. For private repos, only
authorized users can see, clone, or push to the repo.

There are a few ways to autheticate with GitHub, depending on how you
are accessing GitHub (all the options are outlined in more detail
\href{https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/about-authentication-to-github}{here},
including instructions for
\href{https://docs.github.com/en/desktop/installing-and-configuring-github-desktop/installing-and-authenticating-to-github-desktop/authenticating-to-github}{GitHub
Desktop}).

We recommend setting up SSH keys since this only has to be set up once
for each computer and let's you interface with GitHub without having to
type any passwords or do anything special. To do this you'll first
\href{https://docs.github.com/en/authentication/connecting-to-github-with-ssh/checking-for-existing-ssh-keys}{check
if you already have appropriate SSH keys} and then
\href{https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent}{create
a new key if needed}. It's best if you accept the default location for
where the key is saved. If you don't want to type a password each time,
don't enter a passphrase. Finally, you'll
\href{https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account}{add
this SSH key to your github account}.

\hypertarget{cloning-the-repo}{%
\subsection{Cloning the repo}\label{cloning-the-repo}}

On your repository website, click the green `Code' button and select the
SSH option. Copy the text.

When using SSH keys, when you clone a repo, you'll want to use the SSH
option which will start with ``git@github''. If you're using a different
authentication method, you will
\href{https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories}{clone
with the HTTPS option}.

\includegraphics[width=0.5\textwidth,height=\textheight]{images/git/git_tutorial_ssh.png}

Go back to Terminal (or GitBash).

Now we'll clone the hello folder to your computer. For the purposes of
this tutorial, let's clone the folder to your desktop.

First use \texttt{cd} and \texttt{ls} to navigate to your desktop. (Mac
users can type \texttt{cd\ \textasciitilde{}/Desktop} to get there.)

Then type (replacing the URL with the one you copied above):

\begin{quote}
\texttt{git\ clone\ git@github.com:{[}username{]}/hello.git}
\end{quote}

Now you will have a folder called `hello' on your desktop, which
contains one filed called `Readme'.

\hypertarget{make-some-commits}{%
\section{Make some commits}\label{make-some-commits}}

What are ``commits''? A commit is a snapshot of your project at a
certain point in time. Each commit has an author, a time, a unique long
ID (also called a `SHA' or `hash'), and a message describing what change
it makes.

\hypertarget{update-your-readme-file}{%
\subsection{Update your README file}\label{update-your-readme-file}}

A README file contains information about other files in a directory, and
it's customary to include one in your git repo. Your README will be
rendered from markdown on the front page of your GitHub repository (see
\href{https://github.com/gabrielecirulli/2048}{here} and
\href{https://raw.githubusercontent.com/gabrielecirulli/2048/master/README.md}{here}
for an example).

When you initialized your repo on GitHub, the site created an empty
README file. Let's write something in it.

Open the README file in a text editor\sidenote{\footnotesize There will be lots of
  files that are simple text files, with \textbf{.md}, \textbf{.rmd}
  etc. as filetypes. You can open all these with a text editor. Although
  your computer comes with a default one (e.g.~Notepad, TextEdit), we
  would recommend downloading \href{https://www.sublimetext.com}{Sublime
  Text}, which is free and has a lot of powerful tools that will be
  helpful in the future.} and write a sentence or two describing your
repo, and save your changes. (Read the
\href{https://guides.github.com/features/mastering-markdown/}{basics of
markdown} and use it appropriately in formatting your README.)

\hypertarget{add-and-commit-changes-to-git}{%
\subsection{Add and commit changes to
git}\label{add-and-commit-changes-to-git}}

In Terminal, navigate to the repo by typing:

\begin{quote}
\texttt{cd\ hello}
\end{quote}

If you type:

\begin{quote}
\texttt{git\ status}
\end{quote}

You'll get a message telling you that your README has been modified.

Now we will \textbf{add this file and commit it}\sidenote{\footnotesize For future
  reference (feel free to ignore this!): you can add just part of the
  file by using \texttt{git\ add\ -p\ README.md} and following the
  instructions at the bottom of the terminal window.} so that git takes
a snapshot of the changes we made:

\begin{quote}
\texttt{git\ add\ README.md}
\end{quote}

\begin{quote}
\texttt{git\ commit\ -m\ "update\ readme"}
\end{quote}

(\texttt{-m} precedes a commit message, which allows you to describe
what you changed.)

If you now type:

\begin{quote}
\texttt{git\ status}
\end{quote}

You'll get a message telling you that everything is up to date
(``nothing to commit, working tree clean'').

\hypertarget{add-another-file-to-the-repo-commit-it}{%
\subsection{Add another file to the repo + commit
it}\label{add-another-file-to-the-repo-commit-it}}

Use RStudio\sidenote{\footnotesize We'll be using R and RStudio in the future. If you
  do not yet have those downloaded, you can open up a text editor and do
  the same thing.} to make a new R script containing one line, e.g.,
\texttt{print("hello\ world")}. Save this as \textbf{`pset0.R'} inside
the \textbf{hello} folder.

Then, back in the terminal, type:

\begin{quote}
\texttt{git\ status}
\end{quote}

This will tell you that a file called pset0.R exists, but isn't being
tracked by git.\sidenote{\footnotesize If you find a file called ``.DS\_Store'' that
  is being tracked, that is a mac file saving folder preferences. You
  can add this to a .gitignore file as described in step 7 below so that
  git will not track it.}

Now we will \textbf{add this file and commit it} (so that git starts to
track it):

\begin{quote}
\texttt{git\ add\ -A}
\end{quote}

(the \texttt{-A} specifies that we will add all of the files that have
been changed in the repo)

\begin{quote}
\texttt{git\ commit\ -m\ "initial\ commit\ of\ pset0.R"}
\end{quote}

\hypertarget{push-your-changes-to-github}{%
\section{Push your changes to
GitHub}\label{push-your-changes-to-github}}

What we've done so far -- \texttt{add} and \texttt{commit} -- only
affects your local computer. To get your changes on GitHub, use
\texttt{push}:

\begin{quote}
\texttt{git\ push}
\end{quote}

If you go to your GitHub account, you can now see the updated files.

You can only push to the remote repository if the remote and local
copies are in the same state, excepting the changes in the commits. This
may be true if you are the only person who makes changes to the repo and
you do so from only one computer, but that's missing out on a lot of the
potential to use GitHub for collaboration.

To get into a good habit, we recommend that you should pull (get changes
from the remote to your local copy) right before you start making
changes to files and right before you push your changes. To pull run the
command

\begin{quote}
\texttt{git\ pull\ -\/-rebase}
\end{quote}

This will update your local copy to match the remote, leaving any
changes you've made that are committed. (If you have uncommitted
changes, you'll get an error.) So, if your collaborator made edits or
added files and pushed those changes, when you pull, your local copy
with also have those updates.

\hypertarget{make-more-changes-to-the-repo}{%
\section{Make more changes to the
repo}\label{make-more-changes-to-the-repo}}

Now make some changes to your pset0.R file (delete and/or add another
line or two of code) and save it.

In terminal, type

\begin{quote}
\texttt{git\ status}
\end{quote}

to see that \textbf{pset0.R} has been modified since the last commit.

To see the specific changes since the last commit, type:

\begin{quote}
\texttt{git\ diff}
\end{quote}

Then commit:

\begin{quote}
\texttt{git\ add\ -A}
\end{quote}

\begin{quote}
\texttt{git\ commit\ -m\ "{[}describe\ change{]}"}
\end{quote}

Push to the repository on GitHub:

\begin{quote}
\texttt{git\ push}
\end{quote}

TIP: Commits should be focused. Try to commit little bite-sized changes
that are all related to each other together and easy to label, and make
separate commits for other changes.

Best practice for commit messages is to make sure your commit message is
not too long and would fit into the sentence: ``When you pull this
commit, it will \_\_\_\_\_\_.''

\hypertarget{rolling-back-to-previous-versions}{%
\section{Rolling back to previous
versions}\label{rolling-back-to-previous-versions}}

Sometimes you will want to go back to a previous commit. Here's how to
do it:

To view previous commits, type:

\begin{quote}
\texttt{git\ log}
\end{quote}

To change the number of displayed commits, type the number you want to
see preceded by a dash. For example, to view the three most recent
commits, type:

\begin{quote}
\texttt{git\ log\ -3}
\end{quote}

(You can also view the commit history on GitHub.)

You can use the long ID numbers attached to commits (also called hashes
or SHAs) to roll back to them if you need to see a previous version of
the repo. This can be very useful if something breaks and you don't know
how that happened. You can roll back to the last commit where your
program wasn't broken and see what files changed since then, and how.

For example, let's say we wanted to roll back to the very first commit
so we could run the code as it was back then. Let's look at the very
first commit. You can find it by typing \texttt{git\ log} and then
pressing the space bar to scroll down to the very first commit. Copy and
paste the hash for this commit, (press `q' to get back to the main
terminal window), and then type (replacing the hash with the one you
copied):

\begin{quote}
\texttt{git\ checkout\ a240f92a22cb8e9b1300bfa690e99ef07692151e}
\end{quote}

or just

\begin{quote}
\texttt{git\ checkout\ a240f92}
\end{quote}

(Git is smart enough to figure out what commit you meant to type if you
provide the first 8-10 characters of the hash.)

If you open up the hello folder on your desktop, you'll notice that it's
now in the state it was after you made your first commit.

{\textbf{IMPORTANT WARNING}}: After you've finished inspecting a
checkout, make sure you get back to where you started {[}the latest
commit on the main branch{]} by typing:

\begin{quote}
\texttt{git\ checkout\ main}
\end{quote}

To revert your files to the state they were in in an earlier commit,
type (replacing \texttt{0766c053} with the first 8-10 characters of the
hash you copied):

\begin{quote}
\texttt{git\ revert\ -\/-no-commit\ 0766c053..HEAD}
\end{quote}

\begin{quote}
\texttt{git\ commit\ -m\ "revert\ all\ changes\ since\ first\ commit"}
\end{quote}

This will essentially take all of the changes you made since this
commit, undo them, and then save this as a new commit. (Your prior
commits will still exist.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{For more info on undoing things in git, check:}
\NormalTok{https://github.com/blog/2019{-}how{-}to{-}undo{-}almost{-}anything{-}with{-}git.}
\end{Highlighting}
\end{Shaded}

If you'd like to revert one specific commit (rather than all of the
commits after a specific commit), type:

\begin{quote}
\texttt{git\ revert\ -\/-no-commit\ 0766c053}
\end{quote}

\begin{quote}
\texttt{git\ commit\ -m\ "revert\ the\ commit\ where\ I\ did\ xyz..."}
\end{quote}

These reversions are just more commits, so if you revert something (and
commit it) and don't want that change, you can revert the commit that
reverted to get back to where you were before.

\hypertarget{what-not-to-put-on-git}{%
\section{What not to put on git}\label{what-not-to-put-on-git}}

There are some things you don't want on git:

\begin{itemize}
\item
  output files (files that are deterministically generated by other
  files in the repo, e.g.~generated PDFs in a LaTeX project repo)
\item
  log files (like .RData and .Rhistory. You can't describe what
  ``changes'' were made to them and different people's .RData and
  .Rhistory files will always conflict.)
\item
  sensitive data (like human subject data and passwords)
\item
  configuration files that have configurations specific to your computer
  (Important: If you are running stuff on Mechanical Turk, make sure
  your bin/mturk.properties file is NOT on git, because that file
  contains an access key to allow you to authenticate with Amazon.)
\end{itemize}

You can put these in a special .gitignore file so git won't suggest you
add them and will even remind you not to add them if you try to. You can
create this .gitignore file in a text editor like Sublime Text and
update as needed.

Your .gitignore file might look like this (saved exactly as
``.gitignore'' without a file extension):

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# R created files}
\SpecialCharTok{*}\NormalTok{.Rproj}
\SpecialCharTok{*}\NormalTok{.Rproj.user}
\SpecialCharTok{*}\NormalTok{.Rhistory}
\SpecialCharTok{*}\NormalTok{.Ruserdata}
\SpecialCharTok{*}\NormalTok{.history}
\SpecialCharTok{*}\NormalTok{.RData}

\CommentTok{\# Image/output files unless otherwise specified}
\SpecialCharTok{*}\NormalTok{.png}
\SpecialCharTok{*}\NormalTok{.docx}
\SpecialCharTok{*}\NormalTok{.doc}
\SpecialCharTok{*}\NormalTok{.jpg}
\SpecialCharTok{*}\NormalTok{.gif}

\CommentTok{\# Misc Knit Files}
\SpecialCharTok{*}\NormalTok{.aux}
\SpecialCharTok{*}\NormalTok{.gz}
\SpecialCharTok{*}\NormalTok{.log}
\SpecialCharTok{*}\NormalTok{.rdx}
\SpecialCharTok{*}\NormalTok{.rdb}
\SpecialCharTok{*}\NormalTok{.knit.md}
\SpecialCharTok{*}\NormalTok{cache}
\SpecialCharTok{*}\NormalTok{.results}

\CommentTok{\# Other}
\SpecialCharTok{*}\NormalTok{.httr}\SpecialCharTok{{-}}\NormalTok{oauth}
\SpecialCharTok{*}\NormalTok{.DS\_Store}

\CommentTok{\# MTurk credentials and data}
\NormalTok{auth.json}
\NormalTok{my}\SpecialCharTok{{-}}\NormalTok{own}\SpecialCharTok{{-}}\NormalTok{auth.json}
\NormalTok{mturk}\SpecialCharTok{/}
\NormalTok{mturk}\SpecialCharTok{{-}}\NormalTok{and}\SpecialCharTok{{-}}\NormalTok{gmail.txt}

\CommentTok{\# Specific file keep}
\SpecialCharTok{!}\NormalTok{README.md}
\SpecialCharTok{!}\ErrorTok{/}\NormalTok{original....pdf}
\NormalTok{.Rproj.user}
\end{Highlighting}
\end{Shaded}

\hypertarget{further-resources}{%
\section{Further Resources}\label{further-resources}}

\begin{itemize}
\item
  GitHub has many useful guides for learning about branches, pull
  requests, forking and more: \url{https://guides.github.com/}
  \url{https://help.github.com/articles/good-resources-for-learning-git-and-github/}
\item
  Though the things we covered in this tutorial may seem overwhelming,
  there are really only a handful of commands that you need to know,
  which can be found (alongside some commands we didn't cover) on this
  handy
  \href{https://rogerdudler.github.io/git-guide/files/git_cheat_sheet.pdf}{cheatsheet}.
\item
  Request a premium account at \url{https://education.github.com/} for
  free private repos.
\item
  For students seeking deeper Git knowledge, ProGit is a thorough
  \href{https://git-scm.com/book}{open source book} from Scott Chacon.
  It can be viewed online or downloaded in ePub, Mobi, or PDF formats.
\end{itemize}

Acknowledgments: Thank you to Cayce Hook, Erin Bennett and Daniel Watson
for creating the first version of this tutorial for Psych 251!

\hypertarget{sec-rmarkdown}{%
\chapter{R Markdown}\label{sec-rmarkdown}}

\begin{tcolorbox}[colframe=.red, title=\faAppleWhole \enspace Learning goals]

\begin{itemize}
\tightlist
\item
  Explain what Markdown is and how the syntax works,
\item
  Practice how to integrate code and data in R Markdown,
\item
  Understand the different output formats from R Markdown and how to
  generate them
\item
  Know about generating APA format files with \texttt{papaja} and bibtex
\end{itemize}

\end{tcolorbox}

This is a short tutorial on using R Markdown to mix prose and code
together for creating reproducible scientific documents.\sidenote{\footnotesize This
  appendix is adapted from a tutorial that Mike Frank and Chris
  Hartgerink taught together at SIPS 2017.}

In short: R Markdown allows you to create documents that are compiled
with code, producing your next scientific paper. This tutorial will help
you learn the nuts and bolts of how to do this. This appendix --
actually this whole book -- is written in R Markdown. It's a very
flexible platform for writing nice looking documents.\sidenote{\footnotesize If you're
  interested in the source code for this tutorial, it's available
  \href{https://github.com/langcog/experimentology/blob/master/101-rmarkdown.Rmd}{here}.
  We use the amazing \texttt{bookdown} package to format multi-chapter
  manuscripts, and we use the \texttt{tufte} package (lightly
  customized) for style.}

\hypertarget{getting-started}{%
\section{Getting Started}\label{getting-started}}

Fire up Rstudio and create a new R Markdown file. Don't worry about the
settings, we'll get to that later.

If you click on ``Knit'' (or hit \texttt{CTRL+SHIFT+K}) the R Markdown
file will run and generate all results and present you with a PDF file,
HTML file, or a Word file. If RStudio requests you to install packages,
click yes and see whether everything works to begin with.

We need that before we teach you more about R Markdown. But you should
feel good if you get here already, because honestly, you're about 80\%
of the way to being able to write basic R Markdown files. It's
\emph{that} easy.

\begin{tcolorbox}[colframe=.yellow, title=\faPenRuler \enspace Exercises]

Knit the R Markdown template to Word and PDF to ensure that you can get
this to work. Isn't it gratifying?

\end{tcolorbox}

\hypertarget{structure-of-an-r-markdown-file}{%
\section{Structure of an R Markdown
file}\label{structure-of-an-r-markdown-file}}

An R Markdown file contains several parts. Most essential are the
header, the body text, and code chunks. When you knit the resulting
document, you will get the output -- text combined with the results of
running the core -- in one of a number of output formats.

\hypertarget{header}{%
\subsection{Header}\label{header}}

Headers in R Markdown files contain some metadata about your document,
which you can customize to your liking. Below is a simple example that
purely states the title, author name(s), date, and output
format.\sidenote{\footnotesize The header is written in ``YAML'', which means ``yet
  another markup language.'' You don't need to know that, and don't
  worry about it. Just make sure you are careful with indenting, as YAML
  does care about that.}

\begin{Shaded}
\begin{Highlighting}[]
\PreprocessorTok{{-}{-}{-}}
\FunctionTok{title}\KeywordTok{:}\AttributeTok{ }\StringTok{"Untitled"}
\FunctionTok{author}\KeywordTok{:}\AttributeTok{ }\StringTok{"NAME"}
\FunctionTok{date}\KeywordTok{:}\AttributeTok{ }\StringTok{"July 28, 2017"}
\FunctionTok{output}\KeywordTok{:}\AttributeTok{ html\_document}
\PreprocessorTok{{-}{-}{-}}
\end{Highlighting}
\end{Shaded}

\hypertarget{body-text}{%
\subsection{Body text}\label{body-text}}

The body of the document is where you actually write your reports. This
is primarily written in the Markdown format, which is explained in the
\protect\hyperlink{markdown-syntax}{Markdown syntax} section.

The beauty of R Markdown is, however, that you can evaluate \texttt{R}
code right in the text. To do this, you start inline code with `r, type
the code you want to run, and close it again with a `. Usually, this key
is below the escape (\texttt{ESC}) key or next to the left SHIFT button.

For example, if you want to have the result of 48 times 35 in your text,
you type ` r 48-35`, which returns 13. Please note that if you return a
value with many decimals, it will also print these depending on your
settings (for example, 3.1415927).

\hypertarget{code-chunks}{%
\subsection{Code chunks}\label{code-chunks}}

In the section above we introduced you to running code inside text, but
often you need to take several steps in order to get to the result you
need. And you don't want to do data cleaning in the text! This is why
there are code chunks. A simple example is a code chunk loading
packages.

First, insert a code chunk by going to
\texttt{Code-\textgreater{}Insert\ code\ chunk} or by pressing
\texttt{CTRL+ALT+I}. Inside this code chunk you can then type for
example, \texttt{library(ggplot2)} and create an object \texttt{x}.

If you do not want to have the contents of the code chunk to be put into
your document, you include \texttt{echo=FALSE} at the start of the code
chunk. We can now use the contents from the above code chunk to print
results (e.g., \(x=2\)).

These code chunks can contain whatever you need, including tables, and
figures (which we will go into more later). Note that all code chunks
regard the location of the R Markdown as the working directory, so when
you try to read in data use the relative path in.

\hypertarget{output-formats}{%
\subsection{Output formats}\label{output-formats}}

By default, R Markdown renders to HTML format, the standard format of
web pages. These output files are visible in the RStudio viewer and in
any web-browser. These files can be shared on the web and are a great
way to provide the outputs of your research to collaborators (e.g.,
sharing intermediate analytic results).

Through a program called pandoc, R Markdown can also render to Microsoft
Word's DOCX format. This functionality can be very useful for sharing
editable writeups with collaborators (see below).

Finally, rendering to PDF is useful If you want to create PDFs from R
Markdown you need a \LaTeX installation on your computer. (Latex, or tex
for short, is a powerful typesetting package). Many tex installations
are available. One recent possibility is
\href{https://yihui.org/tinytex/}{TinyTEX}, a minimal tex installaction
made for working with R Markdown. Or if you want a full install, try
\href{https://miktex.org/}{MikTeX} for Windows,
\href{https://tug.org/mactex/}{MacTeX} for Mac, or
\href{https://www.tug.org/texlive/}{TeX Live} for Linux.

\hypertarget{markdown-syntax}{%
\section{Markdown syntax}\label{markdown-syntax}}

Markdown is one of the simplest document languages around, that is an
open standard and can be converted into \texttt{.tex}, \texttt{.docx},
\texttt{.html}, \texttt{.pdf}, etc. This is the main workhorse of R
Markdown and is very powerful. You can
\href{https://learnxinyminutes.com/docs/markdown/}{learn Markdown in
five minutes}. Other resources include
\href{https://rmarkdown.rstudio.com/authoring_basics.html}{this
tutorial}, and
\href{https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf}{this
cheat sheet}.

These are the basics:

\begin{itemize}
\tightlist
\item
  It's easy to get \texttt{*italic*} or \texttt{**bold**}.
\item
  You can get headings using \texttt{\#\ heading1} for first level,
  \texttt{\#\#\ heading2} for second-level, and
  \texttt{\#\#\#\ heading3} for third level. Make sure you leave a space
  after the \texttt{\#}!
\item
  Lists are delimited with \texttt{*} for each entry.
\item
  You can write links by writing
  \texttt{{[}here\textquotesingle{}s\ my\ link{]}(http://foo.com)}.
\end{itemize}

The great thing about Markdown is that it works almost everywhere!
Github, OSF, slack, many wikis, and even in text documents it looks
pretty good.

\hypertarget{headers-graphs-and-tables}{%
\section{Headers, graphs, and tables}\label{headers-graphs-and-tables}}

\hypertarget{headers}{%
\subsection{Headers}\label{headers}}

We're going to want more libraries loaded (for now we're loading them
inline).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(knitr)}
\FunctionTok{library}\NormalTok{(ggplot2)}
\FunctionTok{library}\NormalTok{(broom)}
\end{Highlighting}
\end{Shaded}

We often also add \texttt{chunk\ options} to each code chunk so that,
for example:

\begin{itemize}
\tightlist
\item
  code does or doesn't display inline (\texttt{echo} setting)
\item
  figures are shown at various sizes (\texttt{fig.width} and
  \texttt{fig.height} settings)
\item
  warnings and messages are suppressed (\texttt{warning} and
  \texttt{message} settings)
\item
  computations are cached (\texttt{cache} setting)
\end{itemize}

There are many others available as well. Caching can be very helpful for
large files, but can also cause problems when there are external
dependencies that change. An example that is useful for manuscripts is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{opts\_chunk}\SpecialCharTok{$}\FunctionTok{set}\NormalTok{(}\AttributeTok{fig.width=}\DecValTok{8}\NormalTok{, }\AttributeTok{fig.height=}\DecValTok{5}\NormalTok{, }
               \AttributeTok{echo=}\ConstantTok{TRUE}\NormalTok{, }
               \AttributeTok{warning=}\ConstantTok{FALSE}\NormalTok{, }\AttributeTok{message=}\ConstantTok{FALSE}\NormalTok{, }
               \AttributeTok{cache=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{tcolorbox}[colframe=.yellow, title=\faPenRuler \enspace Exercises]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Outlining using headings is a really great way to keep things
  organized! Try making a bunch of headings, and then recompiling your
  document.
\item
  To show off your headings from the previous exercise, add a table of
  contents. Go to the header of the document (the \texttt{YAML}), and
  add some options to the \texttt{html\ document} bit. You want it to
  look like this (indentation must to be correct):
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{output}\KeywordTok{:}\AttributeTok{ }
\AttributeTok{  }\FunctionTok{html\_document}\KeywordTok{:}
\AttributeTok{    }\FunctionTok{toc}\KeywordTok{:}\AttributeTok{ }\CharTok{true}
\end{Highlighting}
\end{Shaded}

\end{tcolorbox}

\hypertarget{graphs}{%
\subsection{Graphs}\label{graphs}}

It's really easy to include graphs, like this one. (Using the
\texttt{mtcars} dataset that comes with \texttt{ggplot2}).

\includegraphics{102-rmarkdown_files/figure-pdf/rmarkdown-ex-1.png}

All you have to do is make the plot and it will render straight into the
text.

External graphics can also be included, as follows:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{include\_graphics}\NormalTok{(}\StringTok{"path/to/file"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{tables}{%
\subsection{Tables}\label{tables}}

There are many ways to make good-looking tables using R Markdown,
depending on your display purpose.

\begin{itemize}
\item
  The \texttt{knitr} package (which powers R Markdown) comes with the
  \texttt{kable} function. It's versatile and makes perfectly reasonable
  tables. It also has a \texttt{digits} argument for controlling
  rounding.
\item
  For HTML tables, there is the \texttt{DT} package, which provides
  \texttt{datatable} -- these are pretty and interactive
  javascript-based tables that you can click on and search in. Not great
  for static documents though.
\item
  For APA manuscripts, it can also be helpful to use the \texttt{xtable}
  package, which creates very flexible LaTeX tables. These can be tricky
  to get right but they are completely customizable provided you want to
  google around and learn a bit about tex.
\end{itemize}

We recommend starting with \texttt{kable}. An expression like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(mtcars), }\AttributeTok{digits =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Produces tabular output like this:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.2727}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0758}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0606}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0758}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0606}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0758}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0606}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0758}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0455}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0455}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0758}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0758}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mpg
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
cyl
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
disp
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
hp
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
drat
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
wt
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
qsec
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
vs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
am
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
gear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
carb
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Mazda RX4 & 21.0 & 6 & 160 & 110 & 3.9 & 2.6 & 16.5 & 0 & 1 & 4 & 4 \\
Mazda RX4 Wag & 21.0 & 6 & 160 & 110 & 3.9 & 2.9 & 17.0 & 0 & 1 & 4 &
4 \\
Datsun 710 & 22.8 & 4 & 108 & 93 & 3.9 & 2.3 & 18.6 & 1 & 1 & 4 & 1 \\
Hornet 4 Drive & 21.4 & 6 & 258 & 110 & 3.1 & 3.2 & 19.4 & 1 & 0 & 3 &
1 \\
Hornet Sportabout & 18.7 & 8 & 360 & 175 & 3.1 & 3.4 & 17.0 & 0 & 0 & 3
& 2 \\
Valiant & 18.1 & 6 & 225 & 105 & 2.8 & 3.5 & 20.2 & 1 & 0 & 3 & 1 \\
\end{longtable}

\begin{tcolorbox}[colframe=.yellow, title=\faPenRuler \enspace Exercises]

Using the \texttt{mtcars} dataset, insert a table and a graph of your
choice into your R Markdown template document. If you're feeling
uninspired, try \texttt{hist(mtcars\$mpg)}.

\end{tcolorbox}

\hypertarget{statistics-1}{%
\subsection{Statistics}\label{statistics-1}}

It's also really easy to include statistical tests of various types. One
option is to use the \texttt{broom} package, which formats the outputs
of various tests really nicely. Paired with knitr's \texttt{kable} you
can make very simple tables in just a few lines of code. This
expression:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mod }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(mpg }\SpecialCharTok{\textasciitilde{}}\NormalTok{ hp }\SpecialCharTok{+}\NormalTok{ cyl, }\AttributeTok{data =}\NormalTok{ mtcars)}
\FunctionTok{kable}\NormalTok{(}\FunctionTok{tidy}\NormalTok{(mod), }\AttributeTok{digits =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

produces this output:

\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
term & estimate & std.error & statistic & p.value \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
(Intercept) & 36.908 & 2.191 & 16.847 & 0.000 \\
hp & -0.019 & 0.015 & -1.275 & 0.213 \\
cyl & -2.265 & 0.576 & -3.933 & 0.000 \\
\end{longtable}

Cleaning these tables up for publication can take some work. For
example, we'd need to rename a bunch of fields to make this table have
the labels we wanted (e.g., to turn \texttt{hp} into
\texttt{Horsepower}).

We often need APA-formatted statistics to be printed in text, though. A
good approach is to compute them first, and then print them inline.
First, we'd run something like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ts }\OtherTok{\textless{}{-}} \FunctionTok{with}\NormalTok{(mtcars,}\FunctionTok{t.test}\NormalTok{(hp[cyl}\SpecialCharTok{==}\DecValTok{4}\NormalTok{], hp[cyl}\SpecialCharTok{==}\DecValTok{6}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

Then we'd print this:

\begin{quote}
There's a statistically-significant difference in horsepower for 4- and
6-cylinder cars (\(t(11.49) = -3.56\), \(p = 0.004\)).
\end{quote}

We did this via an inline code block:
\texttt{round(ts\$parameter,\ 2)}.\sidenote{\footnotesize APA would require omission
  of the leading zero. \texttt{papaja::printp()} will let you do that,
  see below.}

Rounding \(p\) values can occasionally get you in trouble. It's very
easy to have an output of \(p = 0\) when in fact \(p\) can never be
exactly equal to 0. Nonetheless, this can help you prevent the kinds of
rounding errors that would get picked up by software like
\texttt{statcheck}.

\hypertarget{writing-apa-format-papers}{%
\section{Writing APA-format papers}\label{writing-apa-format-papers}}

The end-game of reproducible research is to knit your entire paper into
a submittable APA-style writeup. Managing APA format is a pain in the
best of times. The \texttt{papaja} package allows you to circumvent this
task by rendering your manuscript directly from R Markdown.\sidenote{\footnotesize Thanks
  to \href{https://github.com/crsh}{Frederick Aust} for contributing
  much of the code in this section! For a bit more on \texttt{papaja},
  check out \href{https://rpubs.com/YaRrr/papaja_guide}{this guide}.}

\texttt{papaja} has not yet been released on CRAN but you can install it
from GitHub.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Install devtools package if necessary}
\ControlFlowTok{if}\NormalTok{(}\SpecialCharTok{!}\StringTok{"devtools"} \SpecialCharTok{\%in\%} \FunctionTok{rownames}\NormalTok{(}\FunctionTok{installed.packages}\NormalTok{())) }\FunctionTok{install.packages}\NormalTok{(}\StringTok{"devtools"}\NormalTok{)}

\CommentTok{\# Install papaja}
\NormalTok{devtools}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"crsh/papaja"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The APA manuscript template should now be available through the RStudio
menus when creating a new R Markdown file.

When you click RStudio's \emph{Knit} button \texttt{papaja},
\texttt{rmarkdown,} and \texttt{knitr} work together to create an APA
conform manuscript that includes both your manuscript text and the
results of any embedded R code.

\begin{tcolorbox}[colframe=.yellow, title=\faPenRuler \enspace Exercises]

Make sure you've got \texttt{papaja}, then open a new APA template file.
Compile this document, and look at how awesome it is. Try pasting in
your figure and table from your other R Markdown (don't forget any
libraries you need to make it compile).

\end{tcolorbox}

\hypertarget{bibiographic-management}{%
\section{Bibiographic management}\label{bibiographic-management}}

Managing a bibliography by hand is a lot of work. Letting software do
this for you is much easier. In R Markdown it's possible to include
references using \texttt{bibtex}, by using \texttt{@ref} syntax. You can
do this in \texttt{papaja} but it's also possible to o it in other
packages that have some kind of bibliographic handling.

It's simple. You put together a set of paper citations in a bibtex file
-- then when you refer to them in text, the citations pop up formatted
correctly, and they are also put in your bibliography. As an example,
\texttt{@nuijten2016} results in the in text citation ``Nuijten et al.
(\protect\hyperlink{ref-nuijten2016}{2016})'', or cite them
parenthetically with \texttt{{[}@nuijten2016{]}}
(\protect\hyperlink{ref-nuijten2016}{Nuijten et al. 2016}). Take a look
at the \texttt{papaja} APA example to see how this works.

How do you make your bibtex file? You can do it by hand but this is a
pain. One option for managing references is
\href{https://bibdesk.sourceforge.net/}{bibdesk}, which integrates with
google scholar.\sidenote{\footnotesize Many other options are possible. For example,
  some of us use Zotero frequently as well.} \texttt{citr} is an R
package that provides an easy-to-use
\href{https://rstudio.github.io/rstudioaddins/}{RStudio addin} that
facilitates inserting citations. The addin will automatically look up
the Bib(La)TeX-file(s) specified in the YAML front matter. The
references for the inserted citations are automatically added to the
documents reference section. Once \texttt{citr} is installed
(\texttt{install.packages("citr")}) and you have restarted your R
session, the addin appears in the menus and you can define a
\href{https://rstudio.github.io/rstudioaddins/\#keyboard-shorcuts}{keyboard
shortcut} to call the addin.

\hypertarget{collaboration}{%
\section{Collaboration}\label{collaboration}}

How do we collaborate using R Markdown? There are lots of different
workflows that people use. Here are a few:

\begin{itemize}
\tightlist
\item
  The lead author makes a github repository with the markdown-formatted
  document in it. Others read the PDF and send text comments or PDF
  annotations and the lead makes modifications accordingly.\sidenote{\footnotesize Dropbox
    has good PDF annotation tools for writing comments on specific lines
    of text.} This workflow works well when the lead author is
  relatively experienced and wants to keep control of the manuscript
  without too much line-by-line rewriting.
\item
  The lead author makes a repository as above, but co-authors
  collaborate either by pushing changes to master or by creating pull
  requests. This workflow works well when the authors are all fairly
  git-savvy, and can be great for quickly writing different parts in
  parallel because of git's automatic merging.\sidenote{\footnotesize We wrote this
    book using the all-github workflow, and it was pretty good, modulo
    some merge conflicts.}
\item
  The authors work collaboratively together in an editor like Google
  Docs, Word, or Overleaf. (We favor cloud platforms rather than
  emailing back and forth, for all the reasons discussed in
  \textbf{?@sec-management}). Once the substantive text sections have
  converged, the lead author puts that text back into the markdown
  document and adds references. This workflow is good for very
  collaborative introduction writing when co-authors don't use git or
  markdown. This workflow is a little clunky, but not too bad. And
  critically, all the figures and numbers get rendered fresh when you
  re-knit, so nothing can get accidentally altered during the editing
  process.
\item
  The lead author renders the results section from markdown, then writes
  text in the resulting Word document (or uploads it to Google Docs).
  This workflow is closest to the ``old way'' that many people are used
  to, but runs the biggest risk of errors getting introduced and
  propagated forward, since it's not possible to rerender the whole
  document from scratch. If someone makes changes to the results
  section, it's critical to propagate these back to the markdown and
  keep the two in sync.
\end{itemize}

In sum, there are lots of ways to collaborate -- the best thing is to
talk with your co-authors to select one that works for the group.

\hypertarget{r-markdown-chapter-summary}{%
\section{R Markdown: Chapter summary}\label{r-markdown-chapter-summary}}

R Markdown is a great way to write reproducible papers. It is not too
tricky to learn, and once you master it you can save time by
reformatting quickly and automatically, managing your bibliography
automatically, and even creating nice web-compatible documents.

\refs

\hypertarget{bibliography-25}{%
\section*{References}\label{bibliography-25}}
\addcontentsline{toc}{section}{References}

\markright{References}

\hypertarget{refs-25}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-nuijten2016}{}}%
Nuijten, Michle B, Chris H J Hartgerink, Marcel A L M van Assen, Sacha
Epskamp, and Jelte M Wicherts. 2016. {``The Prevalence of Statistical
Reporting Errors in Psychology (1985--2013).''} \emph{Behav. Res.
Methods} 48 (4): 1205--26.

\end{CSLReferences}

\hypertarget{sec-tidyverse}{%
\chapter{Tidyverse}\label{sec-tidyverse}}

\hypertarget{sec-ggplot}{%
\chapter{ggplot}\label{sec-ggplot}}



\end{document}
